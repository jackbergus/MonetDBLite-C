@' The contents of this file are subject to the MonetDB Public License
@' Version 1.1 (the "License"); you may not use this file except in
@' compliance with the License. You may obtain a copy of the License at
@' http://monetdb.cwi.nl/Legal/MonetDBLicense-1.1.html
@'
@' Software distributed under the License is distributed on an "AS IS"
@' basis, WITHOUT WARRANTY OF ANY KIND, either express or implied. See the
@' License for the specific language governing rights and limitations
@' under the License.
@'
@' The Original Code is the MonetDB Database System.
@'
@' The Initial Developer of the Original Code is CWI.
@' Portions created by CWI are Copyright (C) 1997-July 2008 CWI.
@' Copyright August 2008-2009 MonetDB B.V.
@' All Rights Reserved.

@f run_octopus
@a M. Kersten
@+ Octopus scheduling
The octopus modules enable compute cloud based processing of SQL queries.
The optimizer splits a plan managed by the mitosis and mergetable into
independent functions by backtracking the instruction flow starting
at the aggregation points.
This leads to a series of MAL functions with possibly quite some
instruction overlap when run at a single node, 
which will not be a problem when the recycler is active.

The octopus scheduler takes over control of a MAL execution by
re-directing requests to multiple sites. If there are no sites known,
then the code is executed linearly as is.

The scheduler runs all tentacles asynchronously.
To make our live easier, we assume that all tentacles are
grouped together in a guarded block as follows:

@verbatim
barrier (parallel,a):= scheduler.octopus(timeout);
a:= octopus.tentacle_1();
...
b:= octopus.tentacle_n();
a:= mat.pack(a,...,b);
exit (parallel,a);
@end verbatim

This way the MAL flow of control simplifies skipping to the end
when parallel execution has been enacted and serial execution
simply requires opening the guarded block.

Allowing MAL instructions inbetween complicates our work,
because it would mean that we have to do a flow analysis.

To make this work the scheduler needs a list of database worker.
For the time being, this is an explicitly administered list here. 
When the octopus scheduling is called, we check the connection with
the remote site. If it is down, it is re-activated using Merovingian.

@{
@mal
pattern scheduler.octopus(timeout:int)(:bit, :bat[:any_1,:any_2])
address OCTOPUSrun
comment "Run the program block in parallel, but don't wait longer then t seconds";

pattern scheduler.worker(dbnme:str, usr:str, pw:str)
address OCTOPUSworker
comment "Add a new worker to the known list ";
pattern scheduler.worker(dbnme:str, usr:str, pw:str, host:str, port:int)
address OCTOPUSworker
comment "Add a worker site to the known list ";

pattern scheduler.drop(dbnme:str)
address OCTOPUSdrop
comment "Remove a worker from the list";
@h
#ifndef _RUN_OCTOPUS
#define _RUN_OCTOPUS
#include "mal.h"
#include "mal_instruction.h"
#include "mal_client.h"

/*#define DEBUG_RUN_OCTOPUS 		to trace processing */

#ifdef WIN32
#ifndef LIBRUN_OCTOPUS
#define octopus_export extern __declspec(dllimport)
#else
#define octopus_export extern __declspec(dllexport)
#endif
#else
#define octopus_export extern
#endif

octopus_export str OCTOPUSrun(Client cntxt, MalBlkPtr mb, MalStkPtr stk, InstrPtr p);
octopus_export str OCTOPUSworker(Client cntxt, MalBlkPtr mb, MalStkPtr stk, InstrPtr p);
octopus_export str OCTOPUSdrop(Client cntxt, MalBlkPtr mb, MalStkPtr stk, InstrPtr p);
#endif /* MAL_RUN_OCTOPUS */

@+ Octopus scheduling implementation
The discovery phase consists of establishing connections with
the (remote) database servers.
@c
#include "mal_config.h"
#include "mal_interpreter.h"
#include "mat.h"
#include "run_octopus.h"
#include "optimizer.h"
#include <mapilib/Mapi.h>
#include "remote.h"
#include "alarm.h"

#define SITEasleep	0
#define SITElocal	1
#define SITEremote	2

typedef struct {
	str alias;
	str db;	/* connection parameters */
	str usr;
	str pw;
	str host;	/* used when merovigian is not running */
	int port;
	int status;	/* asleep, local, remote */
} Site;

#define MAXSITES 2048	/* should become dynamic at some point */
static Site *sites;
static int nrsites = 0;

static str
OCTOPUSdiscover(Client cntxt){
	bat b1 = 0, b2 = 0;
	BAT *b;
	str msg = MAL_SUCCEED;

	(void) cntxt;
	(void) b2;
	/* determine if sites are reachable */
	msg = RMTgetList(&b1,&b2);
	if ( msg != MAL_SUCCEED)
		return msg;
	b = BATdescriptor(b1);
	if ( b == NULL)
		throw(MAL,"octopus.discover","No database list available");
	BBPunfix(b1);
	return MAL_SUCCEED;
}

@-
The replica is identified by database name. The host and port
should address a merovingian to ensure the database instance is
started. The default is to contact the local merovingian at 
default port 50000.
@c
str
OCTOPUSworker(Client cntxt, MalBlkPtr mb, MalStkPtr stk, InstrPtr pci)
{
	int idx;

	(void) mb;
	if (nrsites == MAXSITES)
		throw(MAL,"scheduler.worker","Too many worker");
	mal_set_lock(mal_contextLock,"scheduler.worker");
	if (nrsites == 0)
		sites = (Site *) GDKzalloc(sizeof(Site) * MAXSITES);
	idx = nrsites++;
	sites[idx].alias = NULL;
	sites[idx].db = GDKstrdup(*(str*) getArgReference(stk,pci,1));
	sites[idx].usr = GDKstrdup(*(str*) getArgReference(stk,pci,2));
	sites[idx].pw = GDKstrdup(*(str*) getArgReference(stk,pci,3));
	if (pci->argc > 4){
		sites[idx].host = GDKstrdup(*(str*) getArgReference(stk,pci,4));
		sites[idx].port = *(int*) getArgReference(stk,pci,5);
	} else {
		sites[idx].host = GDKstrdup("localhost");
		sites[idx].port = 50000;
	}
	mal_unset_lock(mal_contextLock,"scheduler.worker");
#ifdef DEBUG_RUN_OCTOPUS
	stream_printf(cntxt->fdout,"# added worker %s %s %s %s\n", 
		sites[idx].alias, sites[idx].usr, sites[idx].pw);
		sites[idx].db, sites[idx].usr, sites[idx].pw);
#else
	(void) cntxt;
#endif
	return MAL_SUCCEED;
}
str
OCTOPUSdrop(Client cntxt, MalBlkPtr mb, MalStkPtr stk, InstrPtr pci)
{
	int i,j;
	str alias = *(str*) getArgReference(stk,pci,1);

	(void) cntxt;
	(void) mb;
	mal_set_lock(mal_contextLock,"scheduler.drop");
	for (i=j=0; i<nrsites; i++){
		if( strcmp(sites[i].alias, alias) ==0) {
			GDKfree(sites[i].alias);
			GDKfree(sites[i].db);
			GDKfree(sites[i].usr);
			GDKfree(sites[i].pw);
			GDKfree(sites[i].host);
			continue;
		}
		sites[j++] = sites[i];
	}
	nrsites = j;
	mal_unset_lock(mal_contextLock,"scheduler.drop");
	if ( i == j )
		throw(MAL,"scheduler.drop","Site not found");
	return MAL_SUCCEED;
}
@-
The policy to check for sites is a multiphase phase process.
First, we try to re-use a site where the operation was ran before.
If not available, we select a non-used worker.
If all this fails, we pick a random site to execute the plan.
@c
static str
OCTOPUSexec(Client cntxt, MalBlkPtr mb, MalStkPtr stk, InstrPtr pci)
{
	int i=0, tries= nrsites * 2;
	str msg = MAL_SUCCEED;

redo:
#ifdef DEBUG_RUN_OCTOPUS
	stream_printf(cntxt->fdout,"octopus.exec site selected %d\n",i);
#else
	(void) cntxt;
#endif

	/* register the plan remotely */
	msg = RMTregisterInternal(cntxt, sites[i].alias, 
		getModuleId(pci), getFunctionId(pci));

	/* ignore a duplicate definition */
	if (msg != MAL_SUCCEED && !strstr(msg,"Function already defined")){
#ifdef DEBUG_RUN_OCTOPUS
		stream_printf(cntxt->fdout,"octopus.exec failed to register plan %s.%s at site %s\n",getModuleId(pci),getFunctionId(pci),sites[i].alias);
		stream_printf(cntxt->fdout,"reply: %s\n",msg);
#endif
		if (--tries <= 0)
			return msg;
		goto redo;
	}

	/* execute the plan as an independent process thread if it is local*/
	/* otherwise activate it on the remote site passing parameters as well */
	msg =runMALprocess(cntxt,mb,stk, getPC(mb,pci), getPC(mb,pci)+1);
	if ( msg != MAL_SUCCEED){
#ifdef DEBUG_RUN_OCTOPUS
		stream_printf(cntxt->fdout,"octopus.exec failed to run remote plan\n");
#endif
		if (--tries <= 0)
			return msg;
		goto redo;
	}

	/* if it fails, we need to find another site */
	return msg;
}
@-
The scheduler runs all tentacles asynchronously.
The rigid structure makes it possible to wait for the
threads to end their work. We simple set them to nil
and wait for the assignments to have been completed.
The the sole remainder is the timeout, which could be
part of the scheduler arguments.

We should be careful in accessing a site that runs out
of clients or any failure. It may cause the scheduler to
wait forever.
@c
str
OCTOPUSrun(Client cntxt, MalBlkPtr mb, MalStkPtr stk, InstrPtr p)
{
	int *res = (int*) getArgReference(stk,p,0);
	int timeout = *(int*) getArgReference(stk,p,2);
	int j,fnd, i = getPC(mb,p);
	str msg = MAL_SUCCEED;
	*res = 0;	/* skip the block */

	if ( OCTOPUSdiscover(cntxt) == 0 ){
#ifdef DEBUG_RUN_OCTOPUS
		stream_printf(cntxt->fdout,"#Run in local serial mode\n");
#endif
		*res = 1;
		return MAL_SUCCEED; 
	}
	/* do the actual parallel work */
	for (i++; i<mb->stop && msg == MAL_SUCCEED; i++){
		p= getInstrPtr(mb,i);
		/* don't do it remote if we need arguments */
		if ( p->retc != p->argc){
#ifdef DEBUG_RUN_OCTOPUS
		stream_printf(cntxt->fdout,"#Run in local serial mode due to arguments\n");
#endif
			*res = 1;
			return MAL_SUCCEED; 
		}
		if ( p->barrier == EXITsymbol )
			break;
		if ( getModuleId(p) == matRef && getFunctionId(p) == packRef){
			/* collect the results */
			do{
				fnd = 0;
				for( j=3; j<p->argc; j++)
					fnd += stk->stk[getArg(p,j)].val.bval > 0;
#ifdef DEBUG_RUN_OCTOPUS
				stream_printf(cntxt->fdout,"await answer, seen %d\n",fnd);
#endif
				MT_sleep_ms(1000);
				timeout--;
			} while ( fnd < p->argc-3 && timeout > 0 );
			if (timeout <= 0)
				throw(MAL,"scheduler.pack","Execution time out");
			return MATpackInternal(stk,p,1);
		}
		if ( getModuleId(p) != octopusRef)
			throw(MAL,"scheduler.octopus","tentacle expected");
#ifdef DEBUG_RUN_OCTOPUS
		stream_printf(cntxt->fdout,"#Start a new thread\n");
#else
	(void) cntxt;
#endif
		msg = OCTOPUSexec(cntxt,mb,stk,p);
	}
	return msg; 
}
@}
