@/
The contents of this file are subject to the MonetDB Public License
Version 1.1 (the "License"); you may not use this file except in
compliance with the License. You may obtain a copy of the License at
http://www.monetdb.org/Legal/MonetDBLicense

Software distributed under the License is distributed on an "AS IS"
basis, WITHOUT WARRANTY OF ANY KIND, either express or implied. See the
License for the specific language governing rights and limitations
under the License.

The Original Code is the MonetDB Database System.

The Initial Developer of the Original Code is CWI.
Portions created by CWI are Copyright (C) 1997-July 2008 CWI.
Copyright August 2008-2011 MonetDB B.V.
All Rights Reserved.
@

@c
/*
 * @f radix
 * @a Peter Boncz
 * @v 1.0
 * @t Radix Algorithms
 *
 * @* Introduction
 *
 * This module introduces algorithms that enhance performance of
 * generic join processing in Monet, by optimizing both memory and CPU
 * resources in modern hardware. We now shortly discuss the problems at
 * hand, and in the next section go into detail on how these algorithms
 * are applied in Monet's join processing strategy.
 *
 * @+ The Memory Story
 * Computer RAM carries the acronym Random Access Memory, indicating that
 * the memory access speed is independent of memory location. While this
 * is still (mostly) true, the imbalance in speed improvements between CPU
 * speed and access speed of the most common memory type DRAM has caused the
 * situation that reading a byte of memory takes 100 CPU cycles (e.g. given a
 * 1GHz processor and 100ns memory latency). Load instructions are frequent in
 * most programs (sometimes one in four instructions references memory), and in
 * order not to let the CPU stall for 100 cycles too often, the memory subsystem
 * of a computer therefore does not solely consist of DRAM chips anymore, but also
 * has various levels of "cache memory" built from more speedy SRAM chips. A typical modern
 * computer has at least an L1 (level-one) cache (typical size 16-32KB, typical latency 5-10ns)
 * and a L2 (level-two) cache (typical size 256-2MB, typical latency 10-30ns). The L1
 * and sometimes even the L2 are now located on the CPU chip itself in order to reduce latency.
 *
 * Cache memories are organized in @emph{cache lines} of a fixed width. A typical width for
 * an L1 line is 32 bytes, whereas a line L2 can be 32-128 bytes long. A cache line
 * is the smallest unit of transfer, which means that on a miss, the memory system fetches
 * all bytes of the line from the lower levels of the memory hierarchy in one go.
 * This simultaneous transfer often happens through a wide bus where each line in the
 * bus gets one bit from a DRAM chip in parallel. In reality, transfer is not fully simultaneous,
 * as the different bytes of a cache line arrive in a sequential burst, but the total difference
 * in arrival-time tends to be no more than 4 cycles apart from first to last byte.  This design of
 * the memory caches (on all levels, but with varying size and line width parameters) has consequences
 * for application performance, whose severity depend on what kind of @emph{memory access patterns}
 * the application exhibits. Reading (or writing) a memory location that is not in the cache, causes
 * a miss, and the CPU is forced for waiting the latency period. It is obvious that subsequent
 * reading of this exact same data will not cause sub-sequent cache misses, and be fast -- because
 * this data is already in cache.  But what happens with a sequential access pattern to data that
 * is not in the cache? Again, the first read causes a cache miss. However, subsequent reads
 * to adjacent bytes access the same cache line that is already loaded and therefore
 * do @emph{not} cause any cache misses. This is the reason why sequential memory access
 * is cheaper than random access, as the latter pattern may cause a cache miss on every
 * read (considering the access is to an uncached memory region).
 *
 * In all, costs of memory access can be high (up to 100 cycles, and rising),
 * may occur frequently (up to one in four CPU instructions), and are strongly
 * dependent on the access pattern of the application (is the access repetitive,
 * sequential or random, and to how large a total region). This is not just a
 * theoretical exercise for DBMS designers, since measurements on the
 * memory performance of data-intensive operations like query processing in
 * relational DBMS products has indeed shown that CPUs are stalled for most of
 * their time on memory cache misses.
 *
 * @- Optimizing Memory Access during Join
 * We focus here on improving the memory access performance of the join
 * operator in order to gain performance. This is relevant, because the
 * most popular main-memory join algorithm is hash-join, which exhibits a
 * random access pattern to a hash-table that is used to find values in an
 * "inner" relation, while an "outer" relation is scanned sequentially.
 *
 * Concerning memory access, this algorithm works fine only as long as
 * the inner relation plus its hash-table can be cached at all levels
 * of the memory hierarchy. As soon as it does not fit the smallest cache
 * anymore, (multiple) cache misses will appear for each random access to
 * this inner relation and hash table (at least one miss to the hash-table and
 * one to the relation itself). The CPU stalls caused by these misses can soon
 * start to dominate overall join costs, which can be in Monet -- without cache
 * misses -- as low as 20 cycles per tuple (hence two 100 cycle misses every
 * tuple seriously damage performance).
 *
 * The idea of the radix-algorithms implemented in this module is to change
 * the access pattern of the join operation, in such a way that the overall
 * number of cache misses is reduced. This is typically achieved by doing
 * extra CPU work and/or by making additional sequential passes over the
 * memory. In particular, the @emph{partitioned-join} strategy first partitions
 * (or clusters) both inner and outer relations into small clusters, such that
 * each cluster fits the smallest memory cache. The partitioned-join then only
 * has to combine tuples from corresponding clusters. This module provides two
 * partitioned-join algorithms:
 * @itemize
 * @item
 * the @emph{phash join} algorithm that performs hash-join on the matching clusters.
 * @item
 * as an alternative, we provide @emph{radix join} that performs nested loop
 * join on the matching clusters.
 * @end itemize
 * Phash join needs clusters where the clusters of the inner relation plus hash
 * table fit the smallest cache. If we assume that tuples in the inner relation
 * (plus hash table) occupy 16 bytes and the L1 cache is 16KB, we need clusters
 * of (less than) 1000 tuples.  Radix join needs even smaller clusters, that consist
 * of just 8 tuple to perform well.
 *
 * Partitioning/clustering into such small cluster sizes can become a memory access
 * problem in itself; due to the high number of clusters required for a large relation.
 * A straightforward clustering algorithm that creates H clusters, would allocate
 * H output buffers and scan the input relation once, inserting each tuple into the
 * cluster where it belongs. Hence there are H different "output cursors" that should
 * reside in the cache to perform well. But, a 16KB L1 cache with 32 byte lines only
 * holds 500 lines, hence when H exceeds 500, the "output cursors" cannot be cached and
 * the cluster operation itself will start to generate a huge number of cache misses.
 * This reduces the efficiency of partitioned join.
 *
 * The @emph{radix-cluster} algorithm proposed
 * here solves this problem by making multiple passes; in each pass the relation is
 * subclustered on a number of radix-bits.  Radix-bits are a subsequence of bits taken
 * from a @emph{radix-number}, which usually is the integer result of hashing the value on which
 * we want to cluster. The first pass of the radix-cluster algorithm puts all tuples with an
 * equal bit-pattern in the higher H1 radix-bits together in a cluster. The second pass starts
 * where the previous left off, and subdivides each cluster on the second-highest H2 bits.
 * This process repeats for p passes such that H1*..*HP=H. By keeping Hi lower than the
 * total amount of cache lines, high cache miss ratios can be avoided.
 *
 * On platforms that implement "software TLB" management (Transition Lookaside Buffer;
 * the "cache" of most-recent translations of virtual memory addresses from logical to
 * physical form), TLB misses are also an expensive, and heavily influence performance.
 * As the number of TLB entries is typically limited to 64, on such platforms the TLB
 * poses an even lower bound on the size of Hi than the number of cache lines.
 * This makes radix-cluster even more beneficial on those platforms.
 *
 * @+ The CPU Story
 * Modern CPUs are called @emph{super-scalar}, by which is meant that the CPU
 * has two mechanisms for parallel processing:
 * @enumerate
 * @item
 * CPU instruction execution is chopped into as many as 10-25 different
 * stages, which can be executed one after the other by different pieces of hardware.
 * These pieces of hardware form a pipeline, so each cycle a new instruction can enter
 * the pipeline, while at the other end one leaves (or "graduates").
 * The more stages the pipeline has, less tasks have to be performed per stage,
 * hence the quicker the hardware can execute a stage, hence the higher the overall
 * clock speed of the CPU can be. The search of ever higher CPU clock speeds hence
 * explains the trend of ever longer pipelines found in modern CPUs.
 * @item
 * multiple independent pipelines may be implemented, meaning that
 * two CPU instructions that are independent can be pushed each cycle into two
 * different hardware pipelines for execution. Modern CPUs have at least
 * 2 and possibly up to 9 replicated pipelines (often separated in integer and
 * floating-point pipelines).
 * This second trend is driven by the ever smaller process technology, which gives
 * CPU designers the possibility to use ever more circuits on a single CPU. As
 * a consequence these circuits are used to create replicated execution units
 * organized in pipelines whose parallel activity is supposed to increase the
 * performance of the CPU.
 * @end enumerate
 *
 * All this complexity comes at a price though, which is performance vulnerability.
 * Application code must at all times have three totally independent instructions
 * ready for execution to keep three replicated pipelines busy. This is probably
 * only true for specific scientific computation code, other applications will
 * leave the replicated pipelines mostly without use. Additionally, pipelined execution
 * itself poses the challenge that before the previous execution is finished executing,
 * the CPU has to guess correctly what the next instruction will be.
 * That is, when one instruction enters the pipeline, at the next CPU cycle,
 * it is only past stage one of typically 10-20 stages that have to pass for
 * it to be fully executed, we have to push a next instruction into the pipeline.
 *
 * This turns nasty on if-then-else code like:
 * @verbatim
 * if (A)
 * then B
 * else C
 * @end verbatim
 * The basic problem is that just after entering "if A" in the pipeline at stage 1,
 * the CPU does not yet know whether this instruction will evaluate to true or false,
 * hence it does not know whether the next instruction will be B or C. Modern CPUs
 * resort in this situation to @emph{speculative execution}, by e.g. putting B in the
 * pipeline just because that taking the then-branch is default (a poor estimator)
 * or because in a high percentage of the previous cases this piece of code was executed,
 * "if A" turned out to evaluate to true (which is a better estimator).
 *
 * Clearly, the CPU will turn out to guess wrong in a certain percentage of cases
 * (called the @emph{mis-prediction rate}). Mis-predicting execution has performance
 * consequences, as the real outcome of "if A" only comes to light when the instruction
 * is already deep into the pipeline, and many instructions have already been inserted
 * after it. That work has to be thrown away. Suppose now C would have been the correct
 * next instruction instead of B, then the whole pipeline up to the stage where "if A"
 * is then, needs to be flushed. Also, corrective action needs also to be taken in order
 * to e.g. undo all effect of executing B and all other flushed instructions (e.g. by
 * restoring CPU flags and registers) and we need to start over with C in stage 1. Notice
 * that a mis-prediction rate as low as 5% on a 20-stage pipeline will typically cause 50%
 * of the pipeline to be thrown away, which already decreases performance below the level
 * where a 20-stage pipeline at that speed is actually useful (i.e. some code would do better
 * at a lower speed with a shorter pipeline).
 *
 * The mis-prediction rate is obviously dependent on the type of code being executed.
 * Code that contains many if-statements typically suffers a high mis-prediction
 * rate (as explained above, correctly predicting 95% of the if-branches can still give
 * awful performance), whereas scientific code that does millions of independent
 * scalar operations in a matrix multiplication is highly predictable and will suffer
 * almost none. In addition, such scientific code contains sufficient independent
 * instructions to keep a whole array of independent pipelines busy (assuming, for a
 * minute, that we solved our first problem, memory access).
 *
 * @- The CPU optimization problem
 * Many independent studies show that CPU resource usage during most DBMS loads is awful,
 * plagued by low prediction rates (and high number of cache misses). This indicates that
 * typical DBMS software has a nature of being full of if-statements and branches,
 * much different from scientific code used for matrix processing.
 *
 * We think that that is not necessary. DBMS tasks typically process millions of
 * independent tuples that could well profit from the parallel capabilities of
 * modern CPUs, just like scientific matrix code does. The question is: what needs to
 * be done in DBMS code to make it CPU-wise efficient?
 *
 * In Monet, we apply two techniques:
 * @itemize
 * @item
 * macro-driven (explicit) loop unrolling. This is often dubbed code-expansion.
 * Loop unrolling is a well-known technique to improve the pipelined performance
 * of code that processes a bulk data structure. Regrettably, compilers can only
 * detect opportunity for loop unrolling when the bounds of the bulk structure (array)
 * are known. The sizes of arrays that store database tables are not known
 * at compile time, hence the compiler needs to be helped a bit.
 * @item
 * factoring-out function calls. Function calls are an important source of
 * dependence among subsequent instructions. In a language like C, a function call
 * may modify any reachable memory location, hence the compiler must generate code to
 * reload many values that are cached in registers. On top of that, executing a function
 * call carries substantial stack management overhead (e.g. 20 cycles) and decreases
 * the prediction-rate of the CPU.
 * @end itemize
 *
 * We provide our radix-algorithms in such versions that they can be experimented with
 * with and without these optimization techniques enabled in order to monitor
 * their effectiveness.
 *
 * @* Join Processing Optimized for Memory/CPU cost
 * We now address the issue of optimizing generic join processing for optimal usage of
 * CPU resources and memory hardware on super-scalar CPUs featuring long pipelines and
 * out-of-order speculative execution and memory subsystems that consist of deep hierarchies
 * with various levels of memory cache.
 *
 * We specifically want to compare the effectiveness of the 'Monet-approach' with a standard
 * 'relational approach'. We consider the generic join query:
 *
 * @verbatim
 * SELECT larger.a1, .., larger.aY, smaller.b1, .., smaller.bZ
 * FROM   larger, smaller
 * WHERE  larger.key = smaller.key
 * @end verbatim
 * Without loss of generality we assume that the "larger" table has the same amount or more
 * tuples than the "smaller" table.
 *
 * @+ The Monet Approach
 * In the standard approach this query would be executed in Monet with the following MIL statements:
 * @verbatim
 * 01
 * 02  # join is either positional-, merge- or hash-join.
 * 03  res_join := join(larger_key, smaller_key.reverse);
 * 04  res_larger := res_join.mark(0@0).reverse;
 * 05  res_smaller := res_join.reverse.mark(0@0).reverse;
 *
 *     # positional-join projected columns from smaller table into result
 * A1  res_a1 := join(res_smaller, smaller_a1);
 * AX  ....
 * AY  res_aY := join(res_smaller, smaller_aY);
 *
 *     # positional-join projected columns from larger table into result
 * B1  res_b1 := join(res_larger, larger_b1);
 * BX  ....
 * BZ  res_bZ := join(res_larger, larger_bZ);
 * @end verbatim
 *
 * A positional join is a highly efficient kind of join found in the Monet system, that occurs
 * when an OID-column is joined with a VOID column. A VOID column is a column that contains
 * a sequence of densely ascending OIDs: 1@@0, 2@@0, 3@@0, ..., N@@0. In its implementation,
 * Monet does not materialize suchOID sequences, hence the type-name "void". It is easy to lookup
 * a value in a VOID column, as the value you look up (e.g. 3@@0) already tells its position (=3).
 * The positional join algorithms joins an outer BAT[any,oid] with an inner BAT[void,any]
 * by scanning over the inner-BAT and performing positional lookup into the outer BAT.
 *
 * In a typical data warehouse, the join at line 03 would be positional if the "key"
 * columns are foreign keys between tables with a 1-1, 1-N or N-1 relationship (in those cases,
 * one of the key columns would be of type VOID). However, if the columns are a N-M relationship,
 * or if they do not form a foreign key at all, the join
 * would become a merge-join or a hash-join. Merge-join is only taken if both
 * smaller_key and larger_key are already be sorted on key (tail column). As this cannot
 * generally be assumed, normally a hash-join would be the implementation chosen by Monet.
 * A hash-join performs well as long as the smaller_key BAT plus its associated hash-table
 * (which adds about 8 bytes per tuple), is smaller than the memory cache.
 *
 * The latter phase of the query (lines A0-AY,B0-BZ) fetches column values from the projected columns
 * using positional join.  This performs fine up until table sizes of the larger table when one
 * larger_bX column BAT starts to exceed the size of the memory cache.
 *
 * We now turn our attention to what happens if these sizes exceed. First, we discuss what happens
 * if the larger_bX BATs (which for simplicity we assume all have approximately the same size)
 * do not fit the memory cache. Then, we discuss what happens if even the smaller_key BAT plus
 * its hash-table does not fit anymore.
 *
 * @- The Role of Sorting in improving Memory Access
 * If the BATs storing the columns of the "larger" table do not fit the memory cache anymore,
 * the positional joins in the last Y statements of the MIL script will start to generate cache
 * misses. This is caused by the fact that the OIDs in the tail of the res_larger BATs are
 * not sorted; hence the access to the larger_bX column BATs is random.
 *
 * This problem can be solved, by sorting the result of the join first on the OIDs that point
 * to the "larger" table (the head column of res_join):
 *
 * @verbatim
 * ..
 * 03  res_join := join(larger_key, smaller_key.reverse).sort;
 * 04  res_larger_sorted := res_join.mark(0@0).reverse;
 * 05  res_smaller := res_join.reverse.mark(0@0).reverse;
 * ..
 *     # positional-join projected columns from larger table into result
 * B1  res_b1 := join(res_larger_sorted, larger_b1);
 * BX  ....
 * BZ  res_bZ := join(res_larger_sorted, larger_bZ);
 * @end verbatim
 *
 * As a result, the res_larger BAT will be ordered on tail, hence the positional joins on the larger_bX
 * columns will cause a nice sequential access to both res_larger (as it is scanned in its role as
 * "outer" join operand) and larger_bX (due to the fact that the lookup values from res_larger are
 * now sorted). We must, however, take into account that res_join
 * may be a BAT that itself is larger than the memory cache, in which case the sorting operation
 * itself could cause a great many cache misses itself, and therefore perform badly.  Let us therefore
 * shortly discuss the memory access properties of Monet's sorting algorithms.
 *
 * Monet uses a CPU-optimized quicksort algorithm for sorting large relations. The CPU-optimizations
 * reduce the amount of function calls, by doing all value-comparison and data movement in line,
 * using C macros. In this sense it differs from the standard Unix library call qsort(), as that
 * routine compares values with a user-provided function, and (often) moves values with memcpy().
 *
 * The memory access pattern of the Monet quicksort consists of one sequential scan per recursion
 * level (walking two cursors simultaneously, one from the start of the BAT forward, as well as another
 * from the end of the BAT backward, until both meet in the middle). Quicksort is binary recursive and
 * therefore takes log2(ntuples) recursion levels to sort a BAT, hence its total memory access consists of log2(ntuples)
 * sequential scans.  However, since quicksort zooms into ever smaller sub-chunks of the BAT, there will
 * be cache re-use in the deeper recursion levels as soon as such a chunk fits the memory cache, which
 * happens when @math{sizeof(chunk) = sizeof(BAT)/(2^level) <= sizeof(cache)}.  Hence, the total memory cost of
 * quicksort is @math{log2(ntuples)-log2(sizeof(cache)/sizeof(tuple))} sequential scans.
 *
 * In all, the Monet quicksort implementation behaves quite good both concerning CPU efficiency and
 * memory access pattern. Still, for some simple data types (in particular columns containing OIDs) one
 * can further improve the memory access performance by using @emph{radix-sort} instead of quicksort.
 *
 * Radix-sort is essentially a radix-cluster on all bits, hence we do:
 *
 * @verbatim
 * ..
 * 03 res_join := join(larger_key, smaller_key.reverse).reverse.radix_cluster(R1,..,Rp).reverse;
 * ..
 * @end verbatim
 *
 * Where p is a suitable number of passes and R=R1+..+Rp is the total number of "significant bits"
 * (the most significant bit in a collection of integer values is the highest bit set in all values).
 * The head column of the join(larger_key, smaller_key.reverse) is of type OID, and contains
 * the OIDs from the matching tuples in the larger table. Table-OID are automatically generated by
 * the VOID columns of Monet, and therefore these integer values are from the range [0,...,N], where N
 * is the number of tuples in the "larger" table. We call such an integer
 * sub-domain a "dense" domain. As a result, the number of significant bits is minimal (i.e. R=log2(N),
 * there are no "spoiled" values), and we do not expect skew in such a column. This motivates our choice
 * to implement radix-cluster for the OID type by getting radix bits @emph{without} hashing (for all other
 * types, we hash first). Hashing is not necessary due to absence of value-skew on OID columns, and
 * absence of hashing allows us to use radix-cluster as radix-sort.
 *
 * @- Partitioned Hash Join
 * We now discuss the case that even the smaller_key BAT with its hash structure does not fit the
 * smallest cache. What happens then in the join-phase? Since the hash-join algorithm exhibits a random
 * access pattern, compulsory cache misses will start to appear up to the point that each access to
 * the hash-table will be a miss. Even in the direct hashing employed by Monet, this amounts to at least
 * 3 misses per tuple in the "larger" table (one in the hash-mask, at least one in the BAT, and at least
 * one in the hash-link list to conclude that this is the last match).  To these memory cost, we should
 * also add the cost of constructing the hash table, which amount to (at least) one miss per tuple in
 * the "smaller" table. As cache misses can be as expensive as 100 CPU cycles, and pure CPU costs for
 * direct hashing in Monet can be as low as 20 cycles per tuple, such a high cache miss ratio
 * tremendously slows down the join (e.g. by a factor 10).
 *
 * In the situation mentioned above, performance can be improved by using partitioned hash-join,
 * as presented earlier in this module, instead of simple hash-join. The partitioned hash-join uses
 * radix-cluster to quickly cluster both the smaller_key and larger_key BATs into clusters that fit
 * the memory cache, and then repeatedly performs hash-join on the corresponding clusters. In this way,
 * the random access is restricted to areas that fit the memory cache, hence the expensive cache misses
 * disappear (mostly).
 *
 * This is realized in Monet by using radix-clustering both relations on H bits, e.g. larger_key
 * in l passes, and smaller_key in s passes, such that H = L1+..+Ll = S1+..+Ss:
 * @verbatim
 * 00  # first radix cluster both key columns on H bits (maybe different number of passes and bit settings)
 * 01  cluster_larger := radix_cluster(larger_key, L1,..,Ll);
 * 02  cluster_smaller := radix_cluster(smaller_key, S1,..,Ss);
 *
 *     # partitioned hash join on clusters of H radix-bits.
 * 03  res_join := phash_join(cluster_larger, cluster_smaller.reverse, H);
 * ..
 * @end verbatim
 * Line 03 above uses phash_join, but could alternatively use radix-join:
 * @verbatim
 * ..
 * 03  res_join := radix_join(cluster_larger, cluster_smaller.reverse, H);
 * ..
 * @end verbatim
 *
 * From this point on, the same code as in the previous MIL script could be applied to fetch the column
 * values for columns a1..aY from "smaller" and b1..bZ from "larger". The remaining problem here is that both
 * the larger_bX *and* the smaller_aX BATs will tend to be bigger than the smallest memory cache (though this
 * also depends on the join hit-ratio, but let use suppose it is >= 1).
 *
 * To improve this, we can sort on the OIDs from the "larger" table, like described in the previous section.
 * This will enhance the access pattern of the subsequent positional joins to the larger_bX column BATs:
 *
 * @verbatim
 * ..
 *     # partitioned hash join on clusters of H radix-bits, followed by radix-sort on head column.
 * 03  res_join := phash_join(cluster_larger, cluster_smaller.reverse, H).reverse.radix_cluster(R1,..,Rp).reverse;
 * 04  res_larger_sorted := res_join.mark(0@0).reverse;
 * 05  res_smaller := res_join.reverse.mark(0@0).reverse;
 * ..
 * @end verbatim
 *
 * Now, as the smaller_aX column BATs probably are also larger than the memory cache, we would like to do
 * the same for the "smaller" table. But, we then cannot sort res_join twice. We could sort res_smaller on tail
 * after line 05:
 * @verbatim
 * ..
 * 05  res_smaller := res_join.reverse.mark(0@0).reverse.radix_cluster(R1,..,Rp);
 * ..
 * @end verbatim
 * However, this approach of the problem only transfers the problem to later phases of query processing.
 * The positional joins would run fine, but as a tail-sorted res_smaller would be a BAT[oid,oid] (i.e.
 * it would no longer have a VOID head column), the result of the positional joins with the smaller_aX
 * BAT[void,T]s would be of the form BAT[oid,T]. These results would not only take more space than the
 * desired form BAT[void,T], but would also create a problem in further use of the query result, as these
 * res_aX BATs will not be sorted on head. Join access to them would go to the hash-join rather than the
 * positional join, and due to the random access this would pose a memory caching problem as these
 * res_aX BATs tend to be larger than the memory cache.
 *
 * Therefore, for these projection joins, we propose the use of a new memory-conscious join algorithm that is
 * called @emph{clustered positional join} which implements a join(BAT[void,oid] L, BAT[void,T] R) : BAT[void,T]
 *
 * This algorithm consists of three phases, of which we already know the first two:
 * @table @code
 * @item [partial radix-cluster]
 * First, the res_smaller is @emph{partially} radix-clustered on tail-OID. That is, the relation L (= res_smaller in
 * the positional joins to the column BATs smaller_aX) is clustered on some number of highest significant
 * radix-bits, but not on all radix-bits. Because the radix-cluster on OIDs does not use a hash-function,
 * clustering an OID column on all significant bits radix-sorts it, or - as in this case - on a subset of the
 * highest significant bits, partially orders it. The partial ordering of OIDs in chunks is done in such a way that the
 * size of the corresponding chunk in R (remember R is a BAT[void,T] and has all OIDs in a densely ascending sequence) fits
 * the memory cache.
 * @item [positional join]
 * The purpose of the radix-clustering in the previous phase L is to accelerate the positional join between L and R
 * (i.e. res_smaller and smaller_aX).  Because the OIDs in the tail of L are now partially sorted, each chunk in L
 * will only randomly access data from one chunk in R.  Therefore, during positional join, these chunks in R stay
 * memory resident, accelerating performance with respect to to doing the same with a non-clustered L (where each access to
 * R would be a cache miss).
 * @item [radix decluster]
 * The result of the positional join is a BAT[oid,T]. Still, we know that the head column, when sorted, would
 * form an void column. What is now the fastest way to sort it and convert it back to a void BAT? One special property
 * of the radix-cluster algorithm is that when we cluster on tail column, each result chunk will have the head-values
 * in order. In this case, the clustered version of L (res_smaller_clustered, see below) has the head OIDs in order @emph{within the
 * chunk}. This sub-ordering is also carried over by the positional join to result of the positional join: in each
 * 'virtual chunk' in the BAT[oid,T], the OIDs appear in order. Therefore, we can perform an merge operation to merge
 * all BAT[oid,T] chunks into a BAT[void,T] result. Normally, the cost of a merge is at least O(log(P)*N), where N
 * is the total number of tuples, and P is the number of chunks. By using the special property that eventually,
 * the merged OIDs form a densely ascending sequence (0@@0, 1@@0,..,N@@0), we can bring this cost back to O(N)! This @emph{radix decluster}
 * algorithm keeps a windows open of OIDs [windowStart, windowStart+1, ..., windowStart+windowSize-1] during the
 * merge. Each iteration of the algorithm finds all the next windowSize OIDs in the chunks and inserts them
 * in the result BAT[void,T]. This is done by going to through all (not yet empty) chunks and inserting from the top of
 * each chunk all elements whose OID fits into the window. Due to the fact that all chunks are sorted, we are
 * sure to have found all OIDs after having processed all chunks. Then, the window is shifted windowSize positions
 * and the process repeats. The windowSize is typically a multiple of the number of chunks, such that per iteration
 * in each chunk multiple tuples fall into the window. Because these multiple tuples are accessed sequentially
 * in the chunk, the chunk cache lines will be re-used and performance will be good. The only restriction on the windowSize
 * is that the insertion window on the output BAT must fit the memory cache. This will only start to exceed on very
 * large table sizes (a possible remedy is then to perform the merge in multiple passes).
 * @end table
 *
 * In all, in Monet this join strategy is expressed in the following MIL:
 * @verbatim
 * ..
 *     # subcluster on Rs significant radix-bits, ignoring lowest Ri bits
 * 05  res_smaller_clustered := res_join.reverse.mark(0@0).reverse.radix_cluster(-Ri, Rs);
 *
 *     # positional-join and decluster projected columns from smaller table into result
 * A0  borders_smaller := res_smaller_clustered.radix_count(Ri, Rs);
 * A1  res_a1 := join(res_smaller_clustered, smaller_a1).radix_decluster(borders_smaller);
 * AX   ....
 * AY  res_aY := join(res_smaller_clustered, smaller_aY).radix_decluster(borders_smaller);
 * ..
 * @end verbatim
 *
 * Possibly, one could consider to use this approach as well for the projections on the larger table
 * (instead of the sort). However, sorting once (with radix-sort by radix-clustering on all significant
 * bits) is probably faster than one-time partial radix-cluster, followed by multiple times positional decluster
 * operations (one for each projected column). Therefore, it is best to do one of the set of projections
 * with the sort/join strategy and the other with the clustered positional join strategy.
 * The decision to do which (sort on the smaller or on the larger table?), could also be made in function of the
 * number of projection attributes needed from the "smaller" and "larger" tables. That is, one could choose to do
 * the clustered positional join on the table with least projections, and sort/join on the other.
 *
 * The full MIL script when the individual BATs of both the "smaller" and "larger" tables as well as
 * the (N-M) join result, exceed the memory cache becomes:
 * @verbatim
 *     # first radix cluster both key columns on H bits (maybe different number of passes and bit settings)
 * 01  cluster_larger := radix_cluster(larger_key, L1,..,Ll);
 * 02  cluster_smaller := radix_cluster(smaller_key, S1,..,Ss);
 *
 *     # partitioned hash join on clusters of H radix-bits, followed by radix-sort on head column.
 * 03  res_join := phash_join(cluster_larger, cluster_smaller.reverse, H).reverse.radix_cluster(R1,..,Rp).reverse;
 * 04  res_larger_sorted := res_join.mark(0@0).reverse;
 *
 *     # subcluster on Rs significant radix-bits, ignoring lowest Ri bits
 * 05  res_smaller_clustered := res_join.reverse.mark(0@0).reverse.radix_cluster(-Ri, Rs);
 *
 *     # positional-join and decluster projected columns from smaller table into result
 * A0  borders_smaller := res_smaller_clustered.radix_count(Ri, Rs);
 * A1  res_a1 := join(res_smaller_clustered, smaller_a1).radix_decluster(borders_smaller);
 * AX   ....
 * AY  res_aY := join(res_smaller_clustered, smaller_aY).radix_decluster(borders_smaller);
 *
 *     # positional-join projected columns from larger table into result
 * B1  res_b1 := join(res_larger_sorted, larger_b1);
 * BX  ....
 * BZ  res_bZ := join(res_larger_sorted, larger_bZ);
 * @end verbatim
 *
 * @+ The Relational Approach
 * A cache-conscious join in a relational DBMS would first radix-cluster both the smaller and larger table,
 * where in the process it would project on just the selected columns. As the relational model does not
 * separate its algebraic actions by column, as Monet in MIL does, it cannot use the technique of type-expansion
 * and have primitives that are optimized for a specific type. In other words, the join operator in a relational
 * DBMS must be able to handle tables of variable number of columns with variable constellations of column types.
 * This is either implemented in the kernel of a relational DBMS by using ADT interfaces with functions dereferenced
 * from a type table, or through late-binding style C++ overloaded methods to handle data values of variable types.
 * That means that, e.g. during the radix-cluster, for each projected column, at least one function call is executed
 * to move a data value from the source record to the destination record.
 *
 * To model this extra function calling overhead, we use the integerX Monet types, that were created specifically
 * for these experiments.  An integerX value models a relational tuple that stores X simple integer column values.
 * Through some tricks, copying one integer8 value in Monet is actually done by copying 8 integers width memcpy(),
 * which closely mimics what happens in a typical relational DBMS.
 *
 * In order to give a concrete example, we suppose "smaller" and "larger" each have 128 columns, and
 * the projection widths are Y=Z=8; we emulate relational storage of both tables in Monet by storing
 * them as BAT[integer128,integer], where the tail columns contain the "key" value and the head contains
 * all other columns.
 *
 * The entire MIL sequence are then the following 5 statements:
 * @verbatim
 * 01  smaller_all := [integer]([integer](smaller_key,1).reverse, 128).reverse;
 * 02  larger_all := [integer]([integer](larger_key,1).reverse, 128).reverse;
 *
 * 03  smaller_view := [integer](smaller_all.reverse, 8).reverse;
 * 04  larger_view := [integer](larger_all.reverse, 8).reverse;
 *
 * 05  cluster_smaller := radix_cluster(smaller_view, S1,..,Ss);
 * 06  cluster_larger := radix_cluster(larger_view, L1,..,Ll);
 * 07  res_join := phash_join(cluster_larger, cluster_smaller.reverse, H);
 * @end verbatim
 *
 * Notice that the fact that smaller_view and larger_view are MIL views on the base BATs
 * smaller_all and larger_all, means that the projection is never materialized. Projecting
 * is done on the fly during the first pass of radix cluster, just like what would happen in
 * a relational system. What is more, the copying of each integer8 (view) value from its
 * storage as integer128 is done with 8 memcpy() calls that fetch values at regular intervals
 * (i.e. at positions 0, 16, 32, 48, 64,  80, 96 and 112). In practice, this means that
 * each memcpy() causes a memory cache miss.
 *
 * We expect that the relational strategy will have a different performance characteristic than
 * Monet. First, there will be many more cache misses due to the reasons described above.
 * Even radix-cluster cannot avoid those cache misses, as it is inherent to the relational
 * storage format of base data. Second, there will be much more CPU cost, due to the fact that
 * function-call overhead cannot be eliminated. In Monet algorithms, CPU optimizations cause
 * a 5-fold performance improvement.
 *
 * Therefore, it might even be that the increased cost of radix-cluster will make simple
 * hash-join faster than partitioned hash-join:
 * @verbatim
 * 05  res_join := join(larger_view, smaller_view.reverse);
 * @end verbatim
 * In either way, we expect a relational performance to be a factor 10 slower than Monet
 * on big sizes of both the "smaller" and "larger" tables with hit rates such that the result table is big.
 *
 * @* Module Definition
 */
@mal
module radix;

pattern radix_cluster( b:bat[:any_1,:any_2], limits:str, perc:flt, radix1:int ... ):bat[:any_1,:any_2]
address M5_RDX_radix_cluster
comment
"do N radix-cluster steps creating (radix1 * radix2 * ... * radixN) clusters. First pass uses
 the last radix parameter, and so on backwards. Partial radix cluster (i.e. skipping lower
 significant bits) can be indicated by passing a negative number of bits as first parameter.

 If you pass an appendable, empty, limits bat, a radix_count2 result with the resulting cluster
 boundaries is returned in it. Note that you pass a batname of b, as returned by b.bbpname().

 If you pass a non-empty limits bat, it is used for resuming the clustering. The operation
 assumes that all significant highermost bits are already clustered, and limits contains
 the cluster sizes.

 If you pass a non-empty but writable limits bat, it will be used as above for resume,
 and will also be overwritten with the new cluster boundaries"
;

pattern radix_cluster( b:bat[:any_1,:any_2], perc:flt, radix1:int ... ):bat[:any_1,:any_2]
address M5_RDX_radix_cluster_l
comment
"shortcut for:  radix_cluster( b, limits=\"tmp_0\", perc, radix1, ... );"
;

pattern radix_cluster( b:bat[:any_1,:any_2], limits:str, radix1:int ... ):bat[:any_1,:any_2]
address M5_RDX_radix_cluster_p
comment
"shortcut for:  radix_cluster( b, limits, perc=1.0, radix1, ... );"
;

pattern radix_cluster( b:bat[:any_1,:any_2], radix1:int ... ):bat[:any_1,:any_2]
address M5_RDX_radix_cluster_lp
comment
"shortcut for:  radix_cluster( b, limits=\"tmp_0\", perc=1.0, radix1, ... );"
;

command radix_cluster2( b:bat[:any_1,:any_2], p:int, r:int, i:int ):bat[:any_1,:any_2]
address M5_RDX_radix_cluster2
comment
"(Partially) radix-cluster b in p (<=4) passes on r in total (r/p bits per pass),
 ignoring the lower i bits."
;


command radix_bits( b:bat[:any_1,:any_2] ):int
address M5_RDX_radix_bits
comment
"return the number of bits on which the head column is radix clustered."
;


command radix_count( b:bat[:oid,:oid], shift:int, radix:int ):bat[:int,:int]
address M5_RDX_radix_count
comment
"return a histogram [radix-pattern,bucket-size] for the tail values of some BAT b.
 the first int holds the number of radix bits, the second how many lower bits to ignore.
 notice that the bat b maybe partially radix-clustered (or not at all). the radix_count
 command just detects consecutive chunks where (1) the tail values have the same specified
 radix-bits and (2) the head values are ascending"
;

command radix_count2( b:bat[:any_1,:oid], shift:int, radix:int ):bat[:int,:int]
address M5_RDX_radix_count2
comment
"generates a bat with in head the dense ascending sequence 0..((1 << radix) - 1), and in tail
the count of all tail value in b with these radix bits, ignoring the lowmost ones passed in shift"
;


command radix_decluster( b:bat[:oid,:any_1], radix_cnt:bat[:int,:int], multiplier:int ):bat[:void,:any_1]
address M5_RDX_radix_decluster
comment
"merge a (partially) radix-clustered dense collection of oids back to their original void
 position.  second bat *must* be result of b.radix_count(x,y). Third parameter is a multiplier
 that tells how many times the number of chunks the matching-window-size should be."
;

command radix_decluster( b:bat[:oid,:any_1], radix_cnt:bat[:int,:int] ):bat[:void,:any_1]
address M5_RDX_radix_decluster_m
comment
"shortcut for:  radix_decluster( b, radix_cnt, multiplier=4 );"
;


command radix_decluster2( b:bat[:any_2,:oid], a:bat[:any_2,:any_1], radix_cnt:bat[:int,:int], multiplier:int ):bat[:void,:any_1]
address M5_RDX_radix_decluster2
comment
""
;

command radix_decluster3( b:bat[:any_2,:oid], a:bat[:any_2,:any_1], radix_cnt:bat[:int,:int], multiplier:int ):bat[:void,:any_1]
address M5_RDX_radix_decluster3
comment
""
;

command radix_decluster4( b:bat[:any_2,:oid], a:bat[:any_2,:any_1], radix_cnt:bat[:int,:int], multiplier:int ):bat[:void,:any_1]
address M5_RDX_radix_decluster4
comment
""
;


command radix_join( l:bat[:any_1,:any_2], r:bat[:any_2,:any_3], radix:int, hitrate:int ):bat[:any_1,:any_3]
address M5_RDX_radix_join
comment
"nested-loop join on radix clustered inputs"
;

command radix_join( l:bat[:any_1,:any_2], r:bat[:any_2,:any_3], radix:int ):bat[:any_1,:any_3]
address M5_RDX_radix_join_h
comment
"shortcut for:  radix_join( l, r, radix, hitrate=1 );"
;

command radix_join( l:bat[:any_1,:any_2], r:bat[:any_2,:any_3] ):bat[:any_1,:any_3]
address M5_RDX_radix_join_xh
comment
"shortcut for:  radix_join( l, r, radix=min(radix_bits(reverse(l)), radix_bits(r)), hitrate=1 );"
;


command phash_join( l:bat[:any_1,:any_2], r:bat[:any_2,:any_3], radix:int, hitrate:int, cutoff:bit ):bat[:any_1,:any_3]
address M5_RDX_phash_join
comment
"partitioned hash-join on radix clustered inputs"
;

command phash_join( l:bat[:any_1,:any_2], r:bat[:any_2,:any_3], radix:int, hitrate:int ):bat[:any_1,:any_3]
address M5_RDX_phash_join_c
comment
"shortcut for:  phash_join( l, r, radix, hitrate, cutoff=key(r) );"
;

command phash_join( l:bat[:any_1,:any_2], r:bat[:any_2,:any_3], radix:int, cutoff:bit ):bat[:any_1,:any_3]
address M5_RDX_phash_join_h
comment
"shortcut for:  phash_join( l, r, radix, hitrate=1, cutoff );"
;

command phash_join( l:bat[:any_1,:any_2], r:bat[:any_2,:any_3], radix:int ):bat[:any_1,:any_3]
address M5_RDX_phash_join_hc
comment
"shortcut for:  phash_join( l, r, radix, hitrate=1, cutoff=key(r) );"
;

command phash_join( l:bat[:any_1,:any_2], r:bat[:any_2,:any_3], cutoff:bit ):bat[:any_1,:any_3]
address M5_RDX_phash_join_xh
comment
"shortcut for:  phash_join( l, r, radix=min(radix_bits(reverse(l)), radix_bits(r)), hitrate=1, cutoff );"
;

command phash_join( l:bat[:any_1,:any_2], r:bat[:any_2,:any_3] ):bat[:any_1,:any_3]
address M5_RDX_phash_join_xhc
comment
"shortcut for:  phash_join( l, r, radix=min(radix_bits(reverse(l)), radix_bits(r)), hitrate=1, cutoff=key(r) );"
;


command jivejoin0( proj:bat[:oid,:oid], attr:bat[:void,:any_1], cnt:bat[:int,:int], shift:int, radix:int ):bat[:void,:any_1]
address M5_RDX_jivejoin0
comment
"positional join with built-in NSM projection, and implicit 1-pass perfect knowledge radix-cluster on output according to proj head"
;

command jivejoin1( preallocated:bat[:void,:oid], proj:bat[:oid,:oid], attr:bat[:void,:any_1], cnt:bat[:int,:int], shift:int, radix:int ):bat[:void,:any_1]
address M5_RDX_jivejoin1
comment
"positional join with built-in NSM projection, and implicit 1-pass perfect knowledge radix-cluster on output according to proj head"
;

command jivejoin2( proj:bat[:oid,:oid], attr:bat[:void,:any_1] ):bat[:void,:any_1]
address M5_RDX_jivejoin2
comment
"positional join that creates a void head by inserting the join result positionally"
;


command posjoin( c:bat[:void,:oid], v:bat[:void,:any_1], radix_bits:int, stride:int, vector_size:int ):bat[:void,:any_1]
address M5_RDX_posjoin_clustered
comment
"perform a positional join that exploits radix-clusteredness for prefetching"
;

command posjoin( c:bat[:void,:oid], v:bat[:void,:any_1], radix_bits:int, stride:int ):bat[:void,:any_1]
address M5_RDX_posjoin_clustered_v
comment
"shortcut for:  posjoin( c, v, radix_bits, stride, vector_size=512 );"
;

command posjoin( c:bat[:void,:oid], v:bat[:void,:any_1], radix_bits:int ):bat[:void,:any_1]
address M5_RDX_posjoin_clustered_sv
comment
"shortcut for:  posjoin( c, v, radix_bits, stride=128, vector_size=512 );"
;


command cache_join( l:bat[:void,:oid], r:bat[:void,:any_1], cache_size:int, cache_line_size:int ):bat[:oid,:any_1]
address M5_RDX_cache_join
comment
"cache-conscious positional (\"fetch-\") join with given cache_size & cache_line_size"
;

command cache_join( l:bat[:void,:oid], r:bat[:void,:any_1] ):bat[:oid,:any_1]
address M5_RDX_cache_join_256k_32
comment
"shortcut for:  cache_join( l, r, cache_size=256*1024, cache_line_size=32 );"
;


command uniform( base:oid, size:int, domain:int ):bat[:oid,:int]
address M5_RDX_BATuniform
comment
"create a random bat of certain size, head values starting from base, unique, dense and ordered,
 tail values perfect uniform from a certain domain ((domain >= size)  =>  unique tails)."
;

command uniform( size:int, domain:int ):bat[:oid,:int]
address M5_RDX_BATuniform_b
comment
"shortcut for:  uniform( base=0@0, size, domain);"
;

command uniform( size:int ):bat[:oid,:int]
address M5_RDX_BATuniform_bd
comment
"shortcut for:  uniform( base=0@0, size, domain=size);"
;


command normal( base:oid, size:int, domain:int, stddev:int, mean:int ):bat[:oid,:int]
address M5_RDX_BATnormal
comment
"create a random bat of certain size, head values starting from base, unique, dense and ordered,
 tail values from a normal distribution between [0..domain] with given stddev & mean."
;

command normal( size:int, domain:int, stddev:int, mean:int ):bat[:oid,:int]
address M5_RDX_BATnormal_b
comment
"shortcut for:  normal( base=0@0, size, domain, stddev, mean);"
;

command normal( base:oid, size:int, domain:int, stddev:int ):bat[:oid,:int]
address M5_RDX_BATnormal_m
comment
"shortcut for:  normal( base, size, domain, stddev, mean=domain/2);"
;

command normal( size:int, domain:int, stddev:int ):bat[:oid,:int]
address M5_RDX_BATnormal_bm
comment
"shortcut for:  normal( base=0@0, size, domain, stddev, mean=domain/2);"
;

command normal( base:oid, size:int, domain:int ):bat[:oid,:int]
address M5_RDX_BATnormal_vm
comment
"shortcut for:  normal( base, size, domain, stddev=domain/10, mean=domain/2);"
;

command normal( size:int, domain:int ):bat[:oid,:int]
address M5_RDX_BATnormal_bvm
comment
"shortcut for:  normal( base=0@0, size, domain, stddev=domain/10, mean=domain/2);"
;

command normal( base:oid, size:int ):bat[:oid,:int]
address M5_RDX_BATnormal_dvm
comment
"shortcut for:  normal( base, size, domain=size, stddev=domain/10, mean=domain/2);"
;

command normal( size:int ):bat[:oid,:int]
address M5_RDX_BATnormal_bdvm
comment
"shortcut for:  normal( base=0@0, size, domain=size, stddev=domain/10, mean=domain/2);"
;

@(
 @= integer
.ATOM integer@1[@2,4];
      .COMP    = integer@1Cmp;
      .TOSTR   = integer@1ToStr;
      .FROMSTR = integer@1FromStr;
      .HASH    = integer@1Hash;
      .NULL    = integer@1Null;
.END;

.ATOM pax@1 = integer@1;
.END;

command [pax]( b:bat[:any_1,:integer@1] ):bat[:any_1,:pax@1]
address M5_RDX_BATpax
comment
"convert an integer@1 to pax@1"
;

command [integer]( b:bat[:any_1,:integer@1], width:int ):bat[:any_1,:any]
address M5_RDX_BATinteger
comment
"create a view that makes tail column appear as an integerX column for some width=X"
;

 @= int
.ATOM Int@1 = integer@1;
.END;

command [IntX]( b:bat[:any_1,:integer@1] ):bat[:any_1,:Int@1]
address M5_RDX_BATintX
comment
"convert an integer@1 to Int@1"
;

command jivejoin1( preallocated:bat[:void,:oid], proj:bat[:oid,:oid], attr:bat[:void,:integer@1], cnt:bat[:int,:int], shift:int, radix:int ):bat[:void,:Int@1]
address M5_RDX_jivejoin1
comment
"positional join with built-in NSM projection, and implicit 1-pass perfect knowledge radix-cluster on output according to proj head"
;

command jivejoin1( preallocated:bat[:void,:oid], proj:bat[:oid,:oid], attr:bat[:void,:pax@1], cnt:bat[:int,:int], shift:int, radix:int ):bat[:void,:Int@1]
address M5_RDX_jivejoin1
comment
"positional join with built-in NSM projection, and implicit 1-pass perfect knowledge radix-cluster on output according to proj head"
;

command jivejoin2( proj:bat[:oid,:oid], attr:bat[:void,:integer@1] ):bat[:void,:Int@1]
address M5_RDX_jivejoin2
comment
"positional join that creates a void head by inserting the join result positionally"
;

command jivejoin2( proj:bat[:oid,:oid], attr:bat[:void,:pax@1] ):bat[:void,:Int@1]
address M5_RDX_jivejoin2
comment
"positional join that creates a void head by inserting the join result positionally"
;

command posjoin( proj:bat[:void,:oid], attr:bat[:void,:integer@1] ):bat[:void,:Int@1]
address M5_RDX_posjoin_tuple
comment
"positional join with built-in NSM projection"
;

command posjoin( proj:bat[:void,:oid], attr:bat[:void,:pax@1] ):bat[:void,:Int@1]
address M5_RDX_posjoin_tuple
comment
"positional join with built-in PAX projection"
;

pattern radix_cluster( b:bat[:integer@1,:any_2], limits:str, perc:flt, radix1:int ... ):bat[:Int@1,:any_2]
address M5_RDX_radix_cluster
comment
""
;
pattern radix_cluster( b:bat[:pax@1,:any_2], limits:str, perc:flt, radix1:int ... ):bat[:Int@1,:any_2]
address M5_RDX_radix_cluster
comment
""
;

 @= tpe
 @:@1(1,4)@
 @:@1(2,8)@
 @:@1(4,16)@
 @:@1(8,32)@
 @:@1(16,64)@
 @:@1(32,128)@
 @:@1(64,256)@
 @:@1(128,512)@
 @:@1(256,1024)@
 @mal
 @:tpe(integer)@
 @:tpe(int)@

command [integer]( b:bat[:any_1,:int], width:int ):bat[:any_1,:any]
address M5_RDX_BATinteger
comment
"create a view that makes tail column appear as an integerX column for some width=X"
;

command [integer]( b:bat[:any_1,:int] ):bat[:any_1,:integer1]
address M5_RDX_BATinteger1
comment
"shortcut for:  [integer]( b, width=1 );"
;


command pax_blocksize( int ):int
address M5_RDX_pax_blocksize
comment
"set the pax blocksize"
;

command pax_blocksize(  ):int
address M5_RDX_pax_blocksize_get
comment
"get the pax blocksize"
;
@)

@c
/*
 * @-
 * @f radix
 * @* Implementation
 */
#include "monetdb_config.h"
#include "mal.h"
#include "mal_exception.h"
#include "mal_interpreter.h"  /* for getArgReference() */
#include <math.h>

#include "radix.h"

@(
#define integer1 int
#define integer2 int
#define integer4 int
#define integer8 int
#define integer16 int
#define integer32 int
#define integer64 int
#define integer128 int
#define integer256 int
@)

#ifdef HAVE_XMMINTRIN_H 
/* constants for use with _mm_prefetch */
#include <xmmintrin.h>
#endif

/* math.h files do not have M_PI/M_E defined */
#ifndef M_PI
# define M_PI		3.14159265358979323846	/* pi */
#endif
#ifndef M_E
# define M_E		2.7182818284590452354	/* e */
#endif

#define int_HUSSLE(x)	(((x)>>7)^((x)>>13)^((x)>>21)^(x))

/*
 * @+ Experimentation Gimmicks
 * @- Data Generation
 */

int
M4_RDX_BATuniform(BAT **bn, oid *base, int *size, int *domain)
{
	size_t n = (size_t) * size, i, r;
	BAT *b = NULL;
	char *firstbun;
	int bunsize;		/* initialized in BATloopFast */
	oid bs = *base;
	int j = 0;
	BUN p, q;

	if (*size < 0) {
		GDKerror("BATuniform: size must not be negative");
		return GDK_FAIL;
	}

	b = BATnew(TYPE_oid, TYPE_int, n);
	if (b == NULL)
		return GDK_FAIL;
	if (n == 0) {
		b->tsorted = GDK_SORTED;
		b->hsorted = GDK_SORTED;
		b->hdense = TRUE;
		BATseqbase(b, *base);
		BATkey(b, TRUE);
		BATkey(BATmirror(b), TRUE);
		*bn = b;
		return GDK_SUCCEED;
	}

	firstbun = (char *) BUNfirst(b);
	/* preset b->batBuns->free to make BATloopFast work */
	b->batBuns->free = n * BUNsize(b);
	BATsetcount(b, n);
	/* create BUNs with uniform distribution */
	BATloopFast(b, p, q, bunsize) {
		*(oid *) BUNhloc(b, p) = bs ++;

		*(int *) BUNtloc(b, p) = j;
		if (++j >= *domain)
			j = 0;
	}
	/* mix BUNs randomly */
	for (r = i = 0; i < n; i++) {
		size_t idx = i + ((r += rand()) % (n - i));
		int val;

		p = (BUN) (firstbun + i * bunsize);	/* fast version of BUNptr */
		q = (BUN) (firstbun + idx * bunsize);
		val = *(int *) BUNtloc(b, p);
		*(int *) BUNtloc(b, p) = *(int *) BUNtloc(b, q);
		*(int *) BUNtloc(b, q) = val;
	}
	b->tsorted = FALSE;
	b->T->nonil = FALSE;
	b->hdense = TRUE;
	BATseqbase(b, *base);
	BATkey(b, TRUE);
	*bn = b;
	return GDK_SUCCEED;
}

int
M4_RDX_BATuniform_b(BAT **bn, int *size, int *domain)
{
	oid base = 0;
	return M4_RDX_BATuniform(bn, &base, size, domain);
}

int
M4_RDX_BATuniform_bd(BAT **bn, int *size)
{
	oid base = 0;
	return M4_RDX_BATuniform(bn, &base, size, size);
}

int
M4_RDX_BATnormal(BAT **bn, oid *base, int *size, int *domain, int *stddev, int *mean)
{
	size_t n = (size_t) * size, i;
	unsigned int r = (unsigned int) n;
	size_t d = (size_t) * domain;
	BAT *b = NULL;
	char *firstbun;
	int bunsize;
	BUN p, q;
	int m = *mean, s = *stddev;
	oid bs = *base;
	int *itab;
	flt *ftab, tot = 0.0;

	if (*size < 0) {
		GDKerror("BATnormal: size must not be negative");
		return GDK_FAIL;
	}

        b = BATnew(TYPE_oid, TYPE_int, n);
	if (b == NULL)
		return GDK_FAIL;
	if (n == 0) {
		b->tsorted = GDK_SORTED;
		b->hsorted = GDK_SORTED;
		b->hdense = TRUE;
		BATseqbase(b, *base);
		BATkey(b, TRUE);
		BATkey(BATmirror(b), TRUE);
		*bn = b;
		return GDK_SUCCEED;
	}

	firstbun = (char *) BUNfirst(b);
	itab = (int *) GDKmalloc(d * sizeof(int));
	ftab = (flt *) itab;

	/* assert(0 <= *mean && *mean < *size); */

	/* created inverted table */
	for (i = 0; i < d; i++) {
		dbl tmp = (dbl) ((i - m) * (i - m));

		tmp = pow(M_E, -tmp / (2 * s * s)) / sqrt(2 * M_PI * s * s);
		ftab[i] = (flt) tmp;
		tot += ftab[i];
	}
	for (tot = (flt) (1.0 / tot), i = 0; i < d; i++) {
		itab[i] = (int) ((flt) n * ftab[i] * tot);
		r -= itab[i];
	}
	itab[m] += r;

	/* preset b->batBuns->free to make BATloopFast work */
	b->batBuns->free = n * BUNsize(b);
	BATsetcount(b, n);
	/* create BUNs with normal distribution */
	BATloopFast(b, p, q, bunsize) {
		*(oid *) BUNhloc(b, p) = bs ++;

		while (itab[r] == 0)
			r++;
		itab[r]--;
		*(int *) BUNtloc(b, p) = (int) r;
	}
	GDKfree(itab);

	/* mix BUNs randomly */
	for (r = 0, i = 0; i < n; i++) {
		size_t idx = i + ((r += rand()) % (n - i));
		int val;

		p = (BUN) (firstbun + i * bunsize);	/* fast version of BUNptr */
		q = (BUN) (firstbun + idx * bunsize);
		val = *(int *) BUNtloc(b, p);
		*(int *) BUNtloc(b, p) = *(int *) BUNtloc(b, q);
		*(int *) BUNtloc(b, q) = val;
	}
	b->tsorted = FALSE;
	b->T->nonil = FALSE;
	b->hdense = TRUE;
	BATseqbase(b, *base);
	BATkey(b, TRUE);
	*bn = b;
	return GDK_SUCCEED;
}

int
M4_RDX_BATnormal_b(BAT **bn, int *size, int *domain, int *stddev, int *mean)
{
	oid base = 0;
	return M4_RDX_BATnormal(bn, &base, size, domain, stddev, mean);
}

int
M4_RDX_BATnormal_m(BAT **bn, oid *base, int *size, int *domain, int *stddev)
{
	int mean = *domain / 2;
	return M4_RDX_BATnormal(bn, base, size, domain, stddev, &mean);
}

int
M4_RDX_BATnormal_bm(BAT **bn, int *size, int *domain, int *stddev)
{
	oid base = 0;
	int mean = *domain / 2;
	return M4_RDX_BATnormal(bn, &base, size, domain, stddev, &mean);
}

int
M4_RDX_BATnormal_vm(BAT **bn, oid *base, int *size, int *domain)
{
	int stddev = *domain / 10;
	int mean = *domain / 2;
	return M4_RDX_BATnormal(bn, base, size, domain, &stddev, &mean);
}

int
M4_RDX_BATnormal_bvm(BAT **bn, int *size, int *domain)
{
	oid base = 0;
	int stddev = *domain / 10;
	int mean = *domain / 2;
	return M4_RDX_BATnormal(bn, &base, size, domain, &stddev, &mean);
}

int
M4_RDX_BATnormal_dvm(BAT **bn, oid *base, int *size)
{
	int domain = *size;
	int stddev = domain / 10;
	int mean = domain / 2;
	return M4_RDX_BATnormal(bn, base, size, &domain, &stddev, &mean);
}

int
M4_RDX_BATnormal_bdvm(BAT **bn, int *size)
{
	oid base = 0;
	int domain = *size;
	int stddev = domain / 10;
	int mean = domain / 2;
	return M4_RDX_BATnormal(bn, &base, size, &domain, &stddev, &mean);
}


@(
 @- data types for relational simulation

 @= integerdef
 @:integerdef_extern(@1)@
 @:integerdef_static(@1)@
 @
 @= integerdef_extern
#define integer@1 int
int  integer@1ToStr(str* dst, int* len, int* src) { return intToStr(dst,len,src); }
int  integer@1FromStr(str src, int* len, int** dst) { return intFromStr(src,len,dst); }
int  integer@1Cmp(int* l, int* r) { return *l - *r; }
int* integer@1Null(void) { integer_nil[0] = int_nil; return integer_nil; }
hash_t integer@1Hash(int*i) { return int_HUSSLE(*i); }
 @
 @= integerdef_static
static size_t
integer@1Copy(void *dst, void *src, size_t size)
{
	(void) size;
	*(int*) dst = *(int*) src;
	return sizeof(int);
}
static
int  integer@1Dummy(int i, int p) { S256; return i; } /* create some distance in the binary between fcns */
 @
 @c
#define S004 i=(i&255)*p; i=(i&255)*p; i=(i&255)*p; i=(i&255)*p;
#define S016 S004 S004 S004 S004
#define S256 S016 S016 S016 S016

int integer_nil[256];

 @:integerdef(1)@
 @:integerdef(2)@
 @:integerdef(4)@
 @:integerdef(8)@
 @:integerdef(16)@
 @:integerdef(32)@
 @:integerdef(64)@
 @:integerdef(128)@
/* otherwise, icc (correctly) complains about
 * integer256Copy() & integer256Dummy()
 * being "declared but never referenced"   */
 @:integerdef_extern(256)@

typedef struct {
	size_t(*cpy) (void *dst, void *src, size_t size);
	int (*fcn) (int i, int p);
	size_t size;
	int tpe;
} atom_t;

atom_t atomtbl[8] = {		/* a simulated ADT lookup table */
	{integer1Copy, integer1Dummy, sizeof(int), 0,},
	{integer2Copy, integer2Dummy, sizeof(int), 4,},
	{integer4Copy, integer4Dummy, sizeof(int), 1,},
	{integer8Copy, integer8Dummy, sizeof(int), 5,},
	{integer16Copy, integer16Dummy, sizeof(int), 2,},
	{integer32Copy, integer32Dummy, sizeof(int), 6,},
	{integer64Copy, integer64Dummy, sizeof(int), 3,},
	{integer128Copy, integer128Dummy, sizeof(int), 7},
};

int PAX_INT_PER_BLOCK = 1024;

static int
int_type(char *src)
{
	char dst[32];

	while (*src && (*src < '0' || *src > '9'))
		src++;
	sprintf(dst, "int%s", src);
	return ATOMindex(dst);
}

static void
integerCopy(int *dst, BAT *b, BUN p, int width, int distance)
{
	int *src = (int *) BUNhloc(b, p);
	char *hatom = ATOMname(b->htype);

	if (hatom[0] == 'p' && hatom[1] == 'a' && hatom[2] == 'x') {
		/* PAX */
		size_t idx = BUNindex(b, p) - BUNindex(b, BUNfirst(b));
		int block_ntuples = PAX_INT_PER_BLOCK / width;
		size_t block_idx = idx / block_ntuples;
		size_t tuple_idx = idx % block_ntuples;

		src += block_idx * PAX_INT_PER_BLOCK + tuple_idx;

		while (width-- > 0) {
			int tpe = atomtbl[width & 7].tpe;

			if ((BUN) src >= BUNlast(b))
				src = (int *) BUNfirst(b);
			(void) (*atomtbl[tpe].cpy) (dst, src, atomtbl[tpe].size);
			src += distance * block_ntuples;
			dst++;
		}
	} else {
		/* NSM */
		while (width-- > 0) {
			int tpe = atomtbl[width & 7].tpe;

			(void) (*atomtbl[tpe].cpy) (dst, src, atomtbl[tpe].size);
			src += distance;
			dst++;
		}
	}
}

int
M4_RDX_BATinteger(BAT **ret, BAT *b, int *width)
{
	BAT *bn, *m;
	int tpe, lg = 0;
	int val = *width;

	while (val) {
		int stop = (val & 1);

		val >>= 1;
		if (stop)
			break;
		lg++;
	}
	if (*width <= 0 || lg > 8 || val) {
		GDKerror("BATinteger: unknown width %d\n", *width);
		return GDK_FAIL;
	}
	tpe = TYPE_integer1 + lg * 2;
	if (ATOMsize(tpe) <= ATOMsize(b->ttype)) {
		/* view implementation */
		bn = VIEWcreate(b);
		m = BATmirror(bn);
		HASHdestroy(bn);
		m->htype = bn->ttype = tpe;
		bn->batDirty = 1;
	} else {
		int buf[256], xx, yy, zz;
		BUN p, q, r;

		bn = BATnew(b->htype, tpe, BATcount(b));
		if (bn == NULL)
			return GDK_FAIL;
		r = BUNfirst(bn);
		zz = BUNsize(bn);
		BATloopFast(b, p, q, xx) {
			for (yy = 0; yy < *width; yy++) {
				buf[yy] = *(int *) BUNtail(b, p);
			}
			bunfastins_nocheck(bn, r, BUNhead(b, p), buf, zz);
			r += zz;
		}
		ALIGNsetH(bn, b);
		bn->tsorted = b->tsorted;
		bn->T->nonil = b->T->nonil;
		BATkey(BATmirror(bn), BATtkey(b));
		bn->T->nosorted = b->T->nosorted;
		bn->T->nosorted_rev = b->T->nosorted_rev;
		bn->T->nokey[0] = b->T->nokey[0];
		bn->T->nokey[1] = b->T->nokey[1];
	}
	*ret = bn;
	return GDK_SUCCEED;
      bunins_failed:
	BBPreclaim(bn);
	return GDK_FAIL;
}

int
M4_RDX_BATinteger1(BAT **ret, BAT *b)
{
	int width = 1;
	return M4_RDX_BATinteger(ret, b, &width);
}

int
M4_RDX_BATpax(BAT **ret, BAT *b)
{
	BAT *bn = *ret = VIEWcreate(b);

	HASHdestroy(bn);
	bn->ttype = BATmirror(bn)->htype = bn->ttype + 1;
	bn->batDirty = 1;
	return GDK_SUCCEED;
}

int
M4_RDX_BATintX(BAT **ret, BAT *b)
{
	BAT *bn = *ret = VIEWcreate(b);
	int tpe = int_type(ATOMname(b->ttype));

	HASHdestroy(bn);
	bn->ttype = BATmirror(bn)->htype = tpe;
	bn->batDirty = 1;
	return GDK_SUCCEED;
}

int
M4_RDX_pax_blocksize(int *ret, int *size)
{
	if (*size != int_nil)
		PAX_INT_PER_BLOCK = *size;
	*ret = PAX_INT_PER_BLOCK;
	return GDK_SUCCEED;
}

int
M4_RDX_pax_blocksize_get(int *ret)
{
	int size = int_nil;
	return M4_RDX_pax_blocksize(ret, &size);
}
@)


/*
 * @+ Radix Cluster
 * In radix cluster we want to deliver one new BAT that consists
 * of a consecutive memory area (like all BATs do) with the tuples
 * clustered on a certain radix. To do this correctly in one scan
 * we would need perfect information on how large each cluster is.
 * Only then we can initialize the correct buffer boundaries.
 *
 * Such perfect information could be obtained by a 'histogram' scan; that
 * would prelude the real clustering scan. On uniform data (and as the radix is
 * taken from a hashed number -- that is what we hope to encounter) two
 * scans is a waste, though.
 *
 * In this approach we start assuming perfect uniformity and continue
 * clustering till one of the cluster buffers overflows. If this happens
 * when N (with N near 100) percent of data is clustered; we just have to
 * do the histogram on 100-N percent of the data; and subsequently
 * shift the buffers to make correct room for each cluster. If data
 * is near to uniform, very little data will need to be moved.
 *
 * TODO (easy): make N tunable
 *
 * functions:
 * @itemize
 * @item radix_buns()
 * does the basic clustering stuff (95% of effort)
 * @item cnt_buns()
 * is the histogram scan that is triggered when we need
 * to move data
 * @item move_buns()
 * does the moving work. This is tricky; as copy
 * dependencies bind you to a certain schedule.
 * @item radix_chunk()
 * is the main routine that does one radix scan and
 * produces a new (chunk of a) bat. Does so by first going
 * for uniform distributions and executing radix_buns().
 * If a buffer overflows, this stops and cnt_buns() is
 * done. The buffers are then moved into correct position
 * by move_buns(). Clustering is then finished by again
 * radix_buns().
 * @item radix_cluster()
 * is the multilevel radix cluster routine. On the first
 * level; it processes 1 chunk and produced N1 new ones.
 * On the second level, N1 chunks are processed and divided
 * each in N2 new ones (making for N1*N2 clusters), etc.
 * For clustering each chunk, it calls radix_chunk().
 * @end itemize
 */
#define any_RADIX(p,rd) ((hash_t)((*BATatoms[any].atomHash)(p) & rd))
#define oid_RADIX(p,rd) ((hash_t) *(oid*) (p) & rd)
#define int_RADIX(p,rd) ((hash_t)int_HUSSLE(*(unsigned int*) (p)) & rd)
#define lng_RADIX(p,rd) ((hash_t)int_HUSSLE(((unsigned int*)(p))[0]^((unsigned int*)(p))[1]) & rd)

typedef struct {
	size_t src;		/* offset of chunk where it is now */
	size_t dst;		/* offset of chunk where it should go */
	size_t size;		/* size of chunk (in bytes) */
} move_t;

@(
#define HTYPE(b) ((b->htype >= TYPE_integer1 && b->htype <= TYPE_pax256)?int_type(ATOMname(b->htype)):BAThtype(b))
@)
#define HTYPE(b) BAThtype(b)
#define TTYPE(b) HTYPE(BATmirror(b))

static BAT *
quickbat(BAT *b, size_t cap)
{
	BAT *bn = BATnew(HTYPE(b), BATttype(b), cap);

	if (bn == NULL)
		return NULL;
	bn->hsorted = bn->tsorted = 0;
	if (b->hkey)
		BATkey(bn, TRUE);
	if (b->tkey)
		BATkey(BATmirror(bn), TRUE);
	return bn;
}

/* take care: block sequences that are copied to the right should be done from
 *	      right to left; and vice versa! Otherwise you overwrite blocks.
 */
static void
move_chunks(move_t *mov, BUN base, size_t from, size_t end)
{
	assert(((ssize_t) end) >= 0);	/* make sure that cop=(ssize_t)from does not overflow */
	while (from < end) {
		ssize_t cur, cop = (ssize_t) from;

		while (++from < end) {
			if (mov[from].src >= mov[from].dst)
				break;
		}
		for (cur = from - 1; cur >= cop; cur--) {
			if (mov[cur].size) {
				memcpy(base + mov[cur].dst, base + mov[cur].src, mov[cur].size);
			}
		}
	}
}

static void
rearrange_chunks(BAT *b, BAT *limits, move_t *mov, BUN base, size_t * dst, size_t * lim, size_t * cnt, size_t n)
{
	size_t i, cur = 0, start = 0;
	ssize_t id = -1;

	if (limits && BATcount(limits)) {
		id = *(ssize_t *) BUNhloc(limits, BUNlast(limits) - BUNsize(limits));
	}
	for (i = 0; i < n; i++) {
		size_t end = dst[i];
		size_t siz = end - start;
		size_t nxt = cur + siz + cnt[i];

		if (start <= cur && end >= cur) {
			/* overlap at start of cur */
			mov[i].src = start;
			mov[i].dst = end;
			mov[i].size = cur - start;
			if (limits) {
				ssize_t i0 = ++id;
				size_t i1 = BUNindex(b, ((start == cur) ? nxt : end));
				size_t i2 = i1 - BUNindex(b, cur);
				size_t i3 = BUNindex(b, nxt) - i1;

				BUNins(limits, &i0, &i2, FALSE);
				if (i3)
					BUNins(limits, &i0, &i3, FALSE);
			}
		} else if (start >= cur && start <= nxt) {
			/* overlap just before nxt */
			size_t hole = start - cur;

			if (hole > siz)
				hole = siz;
			mov[i].src = end - hole;
			mov[i].dst = cur;
			mov[i].size = hole;
			if (limits) {
				ssize_t i0 = ++id;
				size_t i1 = BUNindex(b, cur + hole);
				size_t i2 = i1 - BUNindex(b, cur);
				size_t i3 = BUNindex(b, nxt) - i1;

				if (i2)
					BUNins(limits, &i0, &i2, FALSE);
				BUNins(limits, &i0, &i3, FALSE);
			}
		} else {
			/* no overlap: copy all */
			mov[i].src = start;
			mov[i].dst = cur;
			mov[i].size = siz;
			if (limits) {
				ssize_t i0 = ++id;
				size_t i1 = BUNindex(b, cur);
				size_t i2 = BUNindex(b, nxt) - i1;

				BUNins(limits, &i0, &i2, FALSE);
			}
		}
		start = lim[i];
		lim[i] = cur = nxt;
		dst[i] = nxt - cnt[i];
	}
	move_chunks(mov, base, 0, n);
}

static void
sample_buns(BAT *bn, BAT *b, BUN src, BUN cur, BUN end, size_t * dst, size_t * lim, size_t * cnt, size_t n)
{
	size_t ntuples = (end - src) / BUNsize(b);
	size_t done = (cur - src) / BUNsize(b);
	int bunsize = BUNsize(bn);
	size_t total = (ntuples - done);
	size_t i, oldlim;

	/* extrapolate all processed buns as if they were a sample */
	for (oldlim = i = 0; i < n; oldlim = lim[i], i++) {
		cnt[i] = (((dst[i] - oldlim) / bunsize) * (ntuples - done)) / done;
		total -= cnt[i];
		cnt[i] *= bunsize;
	}

	/* add left-overs */
	for (i = 0; total > 0; total--) {
		cnt[i] += bunsize;
		if (++i >= n)
			i = 0;
	}
}


@= radix_buns
static BUN
radix_buns_@1(BAT *bn, BAT *b, BUN start, BUN end, BUN base,
	      size_t* cur, size_t* lim, size_t mask, int shift, oid *maxbits)
{
	/* this accounts for 95% of processing cost; so it is optimized */
	int dst_bunsize = BUNsize(bn);
	int src_bunsize = BUNsize(b);
	int htpe = ATOMstorage(bn->htype);
	BUN src = start;

	if (b->htype == TYPE_void) {
		oid o = *(oid*) BUNhead(b,src);	/* void => oid materialization */
		while(src < end) {
			ptr p = BUNtloc(b,src);
			hash_t x = @1_RADIX(p,mask) >> shift;
			BUN dst = base + cur[x];

			if (cur[x] == lim[x])
				break;
			*(oid*) BUNhloc(bn,dst) = o;
			if (o != oid_nil) o++;
			*(@1*) BUNtloc(bn,dst) = *(@1*) p;
			cur[x] += dst_bunsize; src += src_bunsize;
			@1_CHECK(*maxbits, p);
		}
	} else if (htpe == TYPE_int) {	     /* most common case */
		while(src < end) {
			ptr p = BUNtloc(b,src);
			hash_t x = @1_RADIX(p,mask) >> shift;
			BUN dst = base + cur[x];

			if (cur[x] == lim[x])
				break;
			*(int*) BUNhloc(bn,dst) = *(int*) BUNhloc(b,src);
			*(@1*) BUNtloc(bn,dst) = *(@1*) p;
			cur[x] += dst_bunsize; src += src_bunsize;
			@1_CHECK(*maxbits, p);
		}
	} else
		while(src < end) {
			ptr p = BUNtloc(b,src);
			hash_t x = @1_RADIX(p,mask) >> shift;
			BUN dst = base + cur[x];

			if (cur[x] == lim[x])
				break;
			ATOMput(htpe, bn->H->vheap, BUNhloc(bn,dst), BUNhead(b,src));
			*(@1*) BUNtloc(bn,dst) = *(@1*) p;
			cur[x] += dst_bunsize; src += src_bunsize;
			@1_CHECK(*maxbits, p);
		}
	return src;
  bunins_failed:
	return NULL;
}
@
@c
#define lng_CHECK(dst,v) dst = 0	/* dummy */
#define int_CHECK(dst,v) dst = 0	/* dummy */
#define oid_CHECK(dst,v) dst |= *(oid*) (v)

@:radix_buns(oid)@
@:radix_buns(int)@
@:radix_buns(lng)@

static BUN
radix_buns_any(BAT *bn, BAT *b, BUN src, BUN end, BUN base, size_t * dst, size_t * lim, size_t mask, int shift, oid *maxbits)
{
	int dst_bunsize = BUNsize(bn);
	int src_bunsize = BUNsize(b);
	int htpe = BAThtype(b);
	int ttpe = BATttype(b);
	int any = ttpe;

@(
/* PETER start experimentation hack; must emulate relational projection cost  */
	if (htpe >= TYPE_integer1 && htpe <= TYPE_pax256) {
		BAT *p = VIEWparent(b) ? BBP_cache(VIEWparent(b)) : b;
		int width = ATOMsize(b->htype) >> 2;
		int distance = 1 << ((p->htype - htpe) >> 1);

		while (src < end) {
			hash_t x = any_RADIX(BUNtail(b, src), mask) >> shift;
			BUN bun = base + dst[x];

			if (dst[x] == lim[x])
				break;
			integerCopy((int *) BUNhloc(bn, bun), b, src, width, distance);
			ATOMput(ttpe, bn->T->vheap, BUNtloc(bn, bun), BUNtail(b, src));
			dst[x] += dst_bunsize;
			src += src_bunsize;
		}
	} else
/* PETER end experimentation hack */
@)

		while (src < end) {
			hash_t x = any_RADIX(BUNtail(b, src), mask) >> shift;
			BUN bun = base + dst[x];

			if (dst[x] == lim[x])
				break;
			ATOMput(htpe, bn->H->vheap, BUNhloc(bn, bun), BUNhead(b, src));
			ATOMput(ttpe, bn->T->vheap, BUNtloc(bn, bun), BUNtail(b, src));
			dst[x] += dst_bunsize;
			src += src_bunsize;
		}
	*maxbits = 0;
	return src;
      bunins_failed:
	return NULL;
}

@= cnt_buns
static void
cnt_buns_@1(BAT *bn, BAT *b, BUN src, BUN end, size_t *cnt,
	    size_t mask, int shift)
{
	int dst_bunsize = BUNsize(bn);
	int src_bunsize = BUNsize(b);

	/* count what's left for each chunk */
	for(src += b->tloc, end += b->tloc; src < end; src += src_bunsize) {
		hash_t x = @1_RADIX(src,mask) >> shift;
		cnt[x] += dst_bunsize;
	}
}
@
@c
@:cnt_buns(oid)@
@:cnt_buns(int)@
@:cnt_buns(lng)@

static void
cnt_buns_any(BAT *bn, BAT *b, BUN src, BUN end, size_t * cnt, size_t mask, int shift)
{
	int dst_bunsize = BUNsize(bn);
	int src_bunsize = BUNsize(b);
	int any = b->ttype;

	/* count what's left for each chunk */
	for (; src < end; src += src_bunsize) {
		hash_t x = any_RADIX(BUNtail(b, src), mask) >> shift;

		cnt[x] += dst_bunsize;
	}
}

@= radix_chunk
static int
radix_chunk_@1(BAT *bn, BAT *b, BAT *limits, BUN start, BUN init, BUN end, BUN out,
	       size_t *lim, int nbits, int shift, size_t *dst, size_t *cnt, oid *maxbits)
{
	BUN lo_limit, hi_limit, cur;
	size_t ntuples = BUNindex(b,end)-BUNindex(b,start);
	size_t nchunks = (size_t) 1 << nbits;
	size_t chunksize = ntuples/nchunks;
	size_t i;
	size_t mask = (nchunks-1) << shift;
	move_t buf[512], *mov = (nchunks>512)?(move_t*)GDKmalloc(nchunks*sizeof(move_t)):buf;

	/* kick off with uniform boundaries */
	for (i=0; i<nchunks; i++) {
		lim[i] = chunksize*BUNsize(bn);
		ntuples -= chunksize;
	}
	for (i=0; ntuples > 0; ntuples--) {
		lim[i] += BUNsize(bn);
		if (++i >= nchunks)
			i = 0;
	}
	for (dst[0]=0,i=1; i<nchunks; i++) {
		dst[i] = lim[i-1];
		lim[i] += dst[i];
	}
	lo_limit = start + MAX((size_t) (0.1*(double)(init-start)), 20*nchunks*BUNsize(bn));
	hi_limit = MIN(start + (size_t) (0.9*(double)lim[i-1]), end - 32768);

	cur = radix_buns_@1(bn, b, start, init, out, dst, lim, mask, shift, maxbits);

	/* out of memory in some bucket: sample and continue */
	if (cur && cur < hi_limit && cur > lo_limit && limits == NULL) {
		sample_buns(bn, b, start, cur, end, dst, lim, cnt, nchunks);
		rearrange_chunks(bn, NULL, mov, out, dst, lim, cnt, nchunks);
		cur = radix_buns_@1(bn, b, cur, end, out, dst, lim, mask, shift, maxbits);
	}

	/* out of memory in some bucket: count and finish */
	if (cur) {
		if (cur < end) {
			memset(cnt, 0, nchunks*sizeof(size_t));
			cnt_buns_@1(bn, b, cur, end, cnt, mask, shift);
			rearrange_chunks(bn, limits, mov, out, dst, lim, cnt, nchunks);
			radix_buns_@1(bn, b, cur, end, out, dst, lim, mask, shift, maxbits);
		} else if (limits) {
			memset(cnt, 0, nchunks*sizeof(size_t));
			rearrange_chunks(bn, limits, mov, out, dst, lim, cnt, nchunks);
		}
	}
	if (nchunks>512)
		GDKfree(mov);
	return cur?0:-1;
}
@
@c
@:radix_chunk(oid)@
@:radix_chunk(int)@
@:radix_chunk(lng)@
@:radix_chunk(any)@

static int
radix_cluster_va(BAT **ret, BAT *b, str nme, flt *perc, int *nbits, va_list ap)
{
	int shift = ABS(*nbits), total_shift, res = 0;
	size_t n;
	size_t cap = BATcount(b);
	int radix[MAXPARAMS], argc = 1, *arg;
	size_t *lim;
	oid maxbits = 0;
	bat limitid = BBPindex(nme);
	BAT *bn = b, *limits = NULL;
	flt prc = *perc;
	/*va_list ap;*/

	if (limitid) {
		limits = BATdescriptor(limitid);
	}
	if (limits &&
#if SIZEOF_OID == SIZEOF_INT
	    (limits->htype != TYPE_int || limits->ttype != TYPE_int)
#else
	    (limits->htype != TYPE_lng || limits->ttype != TYPE_lng)
#endif
	    ) {
		GDKerror("radix_cluster: limits %s is not of right type.\n", BATgetId(limits));
		limits = NULL;
	}
	if (limits && (BATcount(limits) == 0 && limits->batRestricted == BAT_READ)) {
		GDKerror("radix_cluster: limits %s is not empty and appendable.\n", BATgetId(limits));
		limits = NULL;
	}

	ALGODEBUG THRprintf(GDKout, "#radix_cluster(%s(%d,%d), %s,%g, %d", BATgetId(b), (int) b->batCacheid, BATtordered(b), nme, *perc, *nbits);

	radix[0] = *nbits;
	/*va_start(ap, nbits);*/
	while ((arg = va_arg(ap, int *)) != NULL) {
		ALGODEBUG THRprintf(GDKout, ",%d", *arg);

		radix[argc] = *arg;
		shift += ABS(radix[argc]);
		argc++;
	}
	/*va_end(ap);*/
	total_shift = shift;
	ALGODEBUG THRprintf(GDKout, ") -> ");

	if (limits && BATcount(limits)) {
		/* we can now resume partial clustering by passing in of a radix_count2 bat */
		size_t tot = 0;
		ssize_t prev = -1;
		int xx;
		BUN r, q;

		lim = (size_t *) GDKmalloc(BATcount(limits) * sizeof(size_t));

		/* find the destination byte-offsets using the radix-count bat */
		n = 0;
		BATloopFast(limits, r, q, xx) {
			size_t cnt = *(size_t *) BUNtloc(limits, r);
			ssize_t cur = *(ssize_t *) BUNhloc(limits, r);

			if (cur <= prev) {
				GDKerror("radix_cluster: non ascending bits %d,%d in limits bat %s!\n", prev, cur, BBPname(limitid));
				BBPunfix(limitid);
				GDKfree(lim);
				return GDK_FAIL;
			}
			tot += cnt;
			lim[n++] = BUNsize(b) * tot;
			prev = cur;
		}
		if (n == 0 || tot != BATcount(b)) {
			GDKerror("radix_cluster: total number size " SZFMT " of all limits %s does not end up to size " BUNFMT " of %s!\n", tot, nme, BATcount(b), BATgetId(b));
			BBPunfix(limitid);
			GDKfree(lim);
			return GDK_FAIL;
		}
		if (limits->batRestricted == BAT_WRITE) {
			BATclear(limits);	/* overwrite with resulting bounds */
		} else {
			limits = NULL;
		}
		total_shift = 30 + MIN(0, radix[0]);	/* just assume the new cluster bits are consecutive.. */
	} else {
		/* just start with one cluster: the entire bat */
		lim = (size_t *) GDKmalloc(sizeof(size_t));
		lim[0] = BUNlast(b) - BUNfirst(b);
		n = 1;
	}

	while (--argc >= 0 && radix[argc] > 0) {
		size_t i, h = (size_t) 1 << radix[argc], j = h * sizeof(size_t), *k, *l = lim;
		size_t *dst = (size_t *) GDKmalloc(sizeof(size_t) * h);
		size_t *cnt = (size_t *) GDKmalloc(sizeof(size_t) * h);
		BAT *prev = bn, *lims = NULL;
		BUN p, q;

		if (limits && (argc == 0 || radix[argc - 1] <= 0)) {
			lims = limits;	/* only pass limits on last iteration */
		}
		bn = quickbat(prev, cap);
		bn->batBuns->free = cap * BUNsize(bn);
		BATsetcount(bn, cap);
		bn->hdense = b->hdense;
		bn->tdense = b->tdense;
		p = BUNfirst(prev);
		q = BUNfirst(bn);

		if (argc == 0) {
			h = 0;	/* last radix: we can use the same lim for each i */
		} else {
			j *= n;	/* get n lims; next radix reuses them as l[] boundary */
		}
		k = lim = (size_t *) GDKmalloc(j);
		shift -= radix[argc];

		for (j = i = 0; i < n; j = l[i], i++) {
			BUN r = p + j + (size_t) (prc * (l[i] - j));
			size_t *kk = k + h;

			if (b->ttype == TYPE_oid) {
				res = radix_chunk_oid(bn, prev, lims, p + j, r, p + l[i], q + j, k, radix[argc], shift, dst, cnt, &maxbits);
			} else if (ATOMstorage(b->ttype) == TYPE_int || ATOMstorage(b->ttype) == TYPE_flt) {
				res = radix_chunk_int(bn, prev, lims, p + j, r, p + l[i], q + j, k, radix[argc], shift, dst, cnt, &maxbits);
			} else if (ATOMstorage(b->ttype) == TYPE_dbl || ATOMstorage(b->ttype) == TYPE_lng) {
				res = radix_chunk_lng(bn, prev, lims, p + j, r, p + l[i], q + j, k, radix[argc], shift, dst, cnt, &maxbits);
			} else {
				res = radix_chunk_any(bn, prev, lims, p + j, r, p + l[i], q + j, k, radix[argc], shift, dst, cnt, &maxbits);
			}
			if (res < 0)
				break;
			while (k < kk)
				*(k++) += j;	/* make relative limits absolute ones */
		}
		if (prev != b)
			BBPreclaim(prev);
		prc = 1.0;
		n *= h;
		GDKfree(l);

		GDKfree(dst);
		GDKfree(cnt);
		if (res < 0)
			break;
	}
	GDKfree(lim);
	ALGODEBUG THRprintf(GDKout, "#(argc=%d,total_shift=%d) ", argc, total_shift);

	if (argc >= 0) {
		bn->tsorted = 0;	/* partial radix cluster */
	} else {
		if (bn->ttype == TYPE_oid) {
			/* now check whether we saw any higher bits; if not, its sorted! */
			bn->tsorted = (total_shift >= 30) || (maxbits & 0x3FFFFFFF) < ((oid) 1 << total_shift);
		}
		bn->tsorted |= total_shift << 1;	/* set radix bits */
	}

	if (bn == b) {
		BBPkeepref(b->batCacheid);
		if (limits) {
			ssize_t zero = 0;
			size_t cnt = BATcount(b);

			BUNins(limits, &zero, &cnt, FALSE);
		}
	}
	if (limitid) {
		BBPunfix(limitid);
	}
	ALGODEBUG THRprintf(GDKout, "#%s(%d,%d) = %d;\n", BATgetId(bn), (int) bn->batCacheid, BATtordered(bn), res);

	if (res < 0) {
		BBPreclaim(bn);
		return GDK_FAIL;
	}
	*ret = bn;
	return GDK_SUCCEED;
}

int
M4_RDX_radix_cluster(BAT **ret, BAT *b, str nme, flt *perc, int *nbits, ...)
{
	int rtrn = 0;
	va_list ap;

	va_start(ap, nbits);
	rtrn = radix_cluster_va(ret, b, nme, perc, nbits, ap);
	va_end(ap);

	return rtrn;
}

int
M4_RDX_radix_cluster_l(BAT **ret, BAT *b, flt *perc, int *nbits, ...)
{
	str nme = "tmp_0";
	int rtrn = 0;
	va_list ap;

	va_start(ap, nbits);
	rtrn = radix_cluster_va(ret, b, nme, perc, nbits, ap);
	va_end(ap);

	return rtrn;
}

int
M4_RDX_radix_cluster_p(BAT **ret, BAT *b, str nme, int *nbits, ...)
{
	flt perc = 1.0;
	int rtrn = 0;
	va_list ap;

	va_start(ap, nbits);
	rtrn = radix_cluster_va(ret, b, nme, &perc, nbits, ap);
	va_end(ap);

	return rtrn;
}

int
M4_RDX_radix_cluster_lp(BAT **ret, BAT *b, int *nbits, ...)
{
	str nme = "tmp_0";
	flt perc = 1.0;
	int rtrn = 0;
	va_list ap;

	va_start(ap, nbits);
	rtrn = radix_cluster_va(ret, b, nme, &perc, nbits, ap);
	va_end(ap);

	return rtrn;
}

int
M4_RDX_radix_cluster2(BAT **ret, BAT *b, int *p, int *r, int *i)
{
	int npasses = *p, nbits = *r, nignore = abs(*i);
	int maxpasses = MIN(4, nbits), quota, rest;
	
	if (npasses < 1) {
		GDKerror("radix_cluster2: number of passes must be > 0");
		return GDK_FAIL;
	}
	if (nbits < 1) {
		GDKerror("radix_cluster2: number of radix bits must be > 0");
		return GDK_FAIL;
	}

	npasses = MIN(maxpasses, *p);
	quota = nbits / npasses;
	rest = nbits - (quota * (npasses - 1));

	if (nignore == 0) {
		nignore = rest;
		rest = quota;
	} else {
		nignore = -1 * nignore;
		npasses += 1;
	}
	switch (npasses) {
	case 5:	return M4_RDX_radix_cluster_lp(ret, b, &nignore, &rest, &quota, &quota, &quota, NULL);
	case 4:	return M4_RDX_radix_cluster_lp(ret, b, &nignore, &rest, &quota, &quota, NULL);
	case 3:	return M4_RDX_radix_cluster_lp(ret, b, &nignore, &rest, &quota, NULL);
	case 2: return M4_RDX_radix_cluster_lp(ret, b, &nignore, &rest, NULL);
	case 1: return M4_RDX_radix_cluster_lp(ret, b, &nignore, NULL);
	default:
		assert(1 <= npasses && npasses <= 5);
	}
	return GDK_FAIL;
}

/*
 * @+ Radix Statistics
 *
 * @- radix bits
 */
int
M4_RDX_radix_bits(int *nbits, BAT *b)
{
	int radix_bits = (BAThordered(b) >> 1);

	if (BAThordered(b) & 1)
		if (b->htype > TYPE_void && ATOMstorage(b->htype) <= TYPE_int) {
			*nbits = ATOMsize(b->htype) << 3;
			if (*nbits != radix_bits) {
				b->hsorted = (*nbits << 1) | 1;
				b->batDirtydesc = TRUE;
			}
			return GDK_SUCCEED;
		}
	*nbits = radix_bits;
	return GDK_SUCCEED;
}

/*
 * @- radix-count
 * simple statistics function that tells us how large the radix chunks are on a certain
 * radix setting for a certain BAT dataset (tail column).
 */
#define RADIX_COUNT_MASK 65535

int
M4_RDX_radix_count(BAT **res, BAT *b, int *shift, int *radix)
{
	size_t mask = (size_t) 1 << *radix;
	int off = *shift;
	BAT *bn = BATnew(TYPE_int, TYPE_int, mask);
	int bunsize;

	if (bn == NULL)
		return GDK_FAIL;
	*res = bn;
	bunsize = BUNsize(b);

	/* assert(0 <= *radix && *radix < 32); */
	bn->tsorted = 0;
	mask = (mask - 1) << off;

	if (BATcount(b)) {
		BUN p = BUNfirst(b);
		BUN q = BUNlast(b);
		int tcur, tbak = (int) (oid_RADIX(BUNtail(b, p), mask) >> off);
		oid hcur, hbak = *(oid *) BUNhead(b, p);
		int cnt = 1;

		while ((p += bunsize) < q) {
			tcur = (int) (oid_RADIX(BUNtail(b, p), mask) >> off);
			hcur = *(oid *) BUNhead(b, p);

			if (tcur != tbak || hcur < hbak) {
				BUNfastins(bn, &tbak, &cnt);
				if (tcur < tbak)
					bn->hsorted = 0;
				tbak = tcur;
				cnt = 1;
			} else {
				cnt++;
			}
			hbak = hcur;
		}
		BUNfastins(bn, &tbak, &cnt);
	}
	if (b->halign == 0) {
		b->halign = OIDnew(1);
		b->batDirtydesc = TRUE;
	}
	/* sign the tail column of the radix_count so we can check later */
	bn->talign = (*radix << 24) ^ (*shift << 16) ^ b->halign;
	return GDK_SUCCEED;
}

int
M4_RDX_radix_count2(BAT **res, BAT *b, int *shift, int *radix)
{
	size_t mask = (size_t) 1 << *radix;
	int off = *shift;
	BAT *bn = *res = BATnew(TYPE_int, TYPE_int, mask);
	size_t xx;
	int *cnt;
	BUN p, q;

	if (bn == NULL)
		return GDK_FAIL;
	cnt = (int *) BUNfirst(bn);

	/* 64bit: assert(0 <= *radix && *radix < 32); */
	/* initialize the result BAT with ascending head and zero counts */
	for (xx = 0; xx < mask; xx++) {
		cnt[xx + xx] = (int) xx;
		cnt[xx + xx + 1] = 0;
	}
	bn->batBuns->free = BUNsize(bn) * mask;
	BATsetcount(bn, mask);
	mask = (mask - 1) << off;

	/* compute the result: a histogram of bit patterns */
	BATloopFast(b, p, q, xx) {
		oid idx = oid_RADIX(BUNtail(b, p), mask) >> off;

		cnt[idx + idx + 1]++;
	}
	BATkey(bn, TRUE);
	bn->hsorted = GDK_SORTED;
	bn->tsorted = 0;
	return GDK_SUCCEED;
}

/*
 * @+ Phash Join
 * @T
 * Joins two clustered inputs with partitioned hash-join. The main innovation of this
 * implementation is to avoid the (int % mask) and replace it with shifts and masks.
 * The % takes 40 cycles in modern CPUs while the >>, and, xor take one. Moreover;
 * there is independence of various operators, so the below hash function may well
 * take less than 7 cycles on a RISC cpu featuring speculative execution.
 *
 * One consequence is that the number of buckets is always a power of two.
 *
 * This new implementation also ensures that the same memory block is used
 * as the mask and link list for all phash invocations.
 */
#define hash_atom(x)  ((hash_t)((x)%h->mask))
#define hash_simple(x)((hash_t)(x))

#define bunfastINSERT(b,h,t,dummy1,dummy2,dummy3,dummy4) bunfastins(b,h,t)
#define intfastINSERT(b,h,t,dummy1,dummy2,dummy3,dummy4) {\
	    if (_dst >= _end) {\
		ALGODEBUG THRprintf(GDKout, "#phash_join: intfastINSERT: BATextend!\n");\
		b->batBuns->free = ((BUN) _dst) - b->batBuns->base;\
		if (BATextend((b), BATgrows(b)) == NULL) goto bunins_failed;\
		_dst = (int*) (b->batBuns->base + b->batBuns->free);\
		_end = (int*) (b->batBuns->base + b->batBuns->size);\
	    }\
	    _dst[0] = *(int*) h;\
	    _dst[1] = *(int*) t;\
	    _dst += 2;\
}

#define NOTSOfastINSERT(b, dummy1, dummy2, hb, hp, tb, tp) { \
	    register BUN _p = BUNlast(b);\
	    register int _bunsize = BUNsize(b);\
	    if ((b)->batBuns->free + _bunsize > (b)->batBuns->size) {\
		if (BATextend((b), BATgrows(b)) == NULL) goto bunins_failed;\
		_p = BUNlast(b);\
	    }\
	    (b)->batBuns->free += _bunsize;\
	    (b)->batCount ++;\
	    integerCopy((int*) BUNhloc(b,_p), hb, hp, width1, distance1);\
	    integerCopy((int*) BUNtloc(b,_p), BATmirror(tb), tp, width2, distance2);\
	}

static int
hash_join(Hash *h, BAT *bn, BAT *l, BUN llo, BUN lhi, BAT *r, BUN rlo, BUN rhi, int rd, int cutoff)
{
	int xx;
	size_t zz = BUNindex(r, rlo);
	hash_t yy = 0;
	ptr nil = ATOMnilptr(l->ttype);

	if ((size_t) (rhi - rlo) > h->lim) {
		/* simplistic skew handling by holding on to initial hash mask size */
		h->lim = rhi - rlo;
		h->hash = (hash_t *) GDKrealloc(h->hash, (h->mask + 1) * sizeof(hash_t) + h->lim);
		h->link = h->hash + h->mask + 1;
	}
	memset(h->hash, ~0, (h->mask + 1) * sizeof(hash_t));

@(
/* PETER start experimentation hack */
	if (l->htype >= TYPE_integer1 && l->htype <= TYPE_pax256 && r->ttype >= TYPE_integer1 && r->ttype <= TYPE_pax256) {
		BAT *p1 = VIEWparent(l) ? BBP_cache(VIEWparent(l)) : l;
		BAT *p2 = VIEWparent(r) ? BBP_cache(VIEWparent(r)) : r;
		int width1 = ATOMsize(l->htype) >> 2;
		int distance1 = 1 << ((p1->htype - l->htype) >> 1);
		int width2 = ATOMsize(r->ttype) >> 2;
		int distance2 = 1 << ((p2->ttype - r->ttype) >> 1);
		int any = ATOMstorage(r->htype);
		size_t mask = ~(size_t) 0;

		if (cutoff) {
			@:hash_join(any,head,tail,atom,NOTSO,hloc,tloc,break;)@
		} else {
			@:hash_join(any,head,tail,atom,NOTSO,hloc,tloc)@
		}
	} else
/* PETER end experimentation hack */
@)

	if (ATOMstorage(r->htype) < TYPE_int || ATOMstorage(r->htype) > TYPE_dbl || ATOMstorage(l->htype) != TYPE_int || ATOMstorage(r->ttype) != TYPE_int) {
		int any = ATOMstorage(r->htype);
		size_t mask = ~(size_t) 0;

		if (cutoff) {
			@:hash_join(any,head,tail,atom,bun,head,tail,break;)@
		} else {
			@:hash_join(any,head,tail,atom,bun,head,tail)@
		}
	} else {
		/* ATOMstorage(l->htype) == TYPE_int && ATOMstorage(r->ttype) == TYPE_int
		   hence the result BAT is [int,int], so this works */
		int *_dst = (int *) (bn->batBuns->base + bn->batBuns->free);
		int *_end = (int *) (bn->batBuns->base + bn->batBuns->size);
		size_t mask = h->mask << rd;

		if (r->htype == TYPE_oid) {
			if (cutoff) {
				@:hash_join(oid,hloc,tloc,simple,int,hloc,tloc,break;)@
			} else {
				@:hash_join(oid,hloc,tloc,simple,int,hloc,tloc)@
			}
		} else if (ATOMstorage(r->htype) == TYPE_int || ATOMstorage(r->htype) == TYPE_flt) {
			if (cutoff) {
				@:hash_join(int,hloc,tloc,simple,int,hloc,tloc,break;)@
			} else {
				@:hash_join(int,hloc,tloc,simple,int,hloc,tloc)@
			}
		} else {	/* if (ATOMstorage(r->htype) == TYPE_lng || ATOMstorage(r->htype) == TYPE_dbl) */

			if (cutoff) {
				@:hash_join(lng,hloc,tloc,simple,int,hloc,tloc,break;)@
			} else {
				@:hash_join(lng,hloc,tloc,simple,int,hloc,tloc)@
			}
		}
		bn->batBuns->free = ((BUN) _dst) - bn->batBuns->base;
		BATsetcount(bn, bn->batBuns->free/BUNsize(bn));
	}
	return 0;
      bunins_failed:
	return -1;
}

@= hash_join
	ALGODEBUG THRprintf(GDKout, "#phash_join: hash_join[@1,@2,@3,@4,@5,@6,@7,@8]\n");
	/* build phase */
	for (xx=BUNsize(r); rlo<rhi; rlo+=xx) {
		ptr p = BUN@2(r,rlo);
		if (!@4_EQ(p, nil, @1)) {
			hash_t v = hash_@4(@1_RADIX(p,mask)>>rd);
			h->link[yy] = h->hash[v];
			h->hash[v] = yy++;
		}
	}
	/* probe phase */
	for (xx=BUNsize(l); llo<lhi; llo+=xx) {
		ptr v = BUN@3(l,llo);
		for(yy=h->hash[hash_@4(@1_RADIX(v,mask)>>rd)]; yy!=~(hash_t)0; yy=h->link[yy]) {
			rlo = BUNptr(r,yy+zz);
			if (@4_EQ(BUN@2(r,rlo),v,@1)) {
				@5fastINSERT(bn, BUN@6(l,llo), BUN@7(r,rlo), l, llo, r, rlo); @8
												      }
		}
	}
@
@c
int
M4_RDX_phash_join(BAT **res, BAT *l, BAT *r, int *radix, int *hitrate, bit *cutoff)
{
	ALGODEBUG THRprintf(GDKout, "#phash_join(%s(%d,%d),%s(%d,%d),%d,%d,%d) -> ",
		BATgetId(l), (int) l->batCacheid, BATtordered(l), 
		BATgetId(r), (int) r->batCacheid, BAThordered(r), *radix, *hitrate, *cutoff);

	if (*radix != (BATtordered(l) >> 1)) {
		GDKerror("phash_join: tail of %s is radix clustered on %d bits.\n", 
			BATgetId(l), BATtordered(l) >> 1);
		return GDK_FAIL;
	}
	if (*radix != (BAThordered(r) >> 1)) {
		GDKerror("phash_join: head of %s is radix clustered on %d bits.\n", 
			BATgetId(r), BAThordered(r) >> 1);
		return GDK_FAIL;
	}
	{
		size_t k, m, rd = (((size_t)1) << *radix) - 1;
		size_t estimate = MIN(BATcount(r), BATcount(l)) * (*hitrate);
		BAT *bn = BATnew(HTYPE(l), TTYPE(r), estimate);
		BUN r_end, l_cur, l_last;
		BUN l_end, r_cur, r_last;
		int xx, yy;
		int rcut = r->hkey || (*cutoff && (*hitrate == 1));
		Hash h;

		if (bn == NULL)
			return GDK_FAIL;
		l_cur = BUNfirst(l);
		l_last = BUNlast(l);
		r_cur = BUNfirst(r);
		r_last = BUNlast(r);
		xx = BUNsize(l);
		yy = BUNsize(r);

		/* alloc hash table */
		h.lim = BATcount(r) >> *radix;	/* mean cluster size */
		k = h.lim / (*hitrate);	/* mean number of different elements per cluster */
		for (m = 1; m < k; m <<= 1) ;	/* perfect hashing */
		h.lim <<= 2;	/* make lim four times as big for handling some skew */
		h.hash = (hash_t *) GDKmalloc((m + h.lim) * sizeof(hash_t));
		h.link = h.hash + m;
		h.type = ATOMtype(r->htype);
		h.mask = m - 1;
		h.lim *= BUNsize(r);	/* lim is used as a byte offset */

		/* set properties on result bat */
		bn->hsorted = BAThordered(l);
		bn->tsorted = 0;
		if (l->hkey && r->hkey)
			BATkey(bn, TRUE);
		if (r->tkey && l->tkey)
			BATkey(BATmirror(bn), TRUE);

		if (BATcount(r)) {
			if (r->htype == TYPE_oid) {
				@:hash_merge(oid,hloc,tloc)@
			} else if (ATOMstorage(r->htype) == TYPE_int || ATOMstorage(r->htype) == TYPE_flt) {
				@:hash_merge(int,hloc,tloc)@
			} else if (ATOMstorage(r->htype) == TYPE_lng || ATOMstorage(r->htype) == TYPE_dbl) {
				@:hash_merge(lng,hloc,tloc)@
			} else {
				int any = r->htype;
				@:hash_merge(any,head,tail)@
			}
		}
	      xit:
		GDKfree(h.hash);
		ALGODEBUG THRprintf(GDKout, "%s(%d);\n", BATgetId(bn), (int) bn->batCacheid);

		*res = bn;
		return bn ? GDK_SUCCEED : GDK_FAIL;
	}
}

int
M4_RDX_phash_join_c(BAT **res, BAT *l, BAT *r, int *radix, int *hitrate)
{
	if (l && r) {
		bit cutoff = r->hkey;
		return M4_RDX_phash_join(res, l, r, radix, hitrate, &cutoff);
	} else {
		return GDK_FAIL;
	}
}

int
M4_RDX_phash_join_h(BAT **res, BAT *l, BAT *r, int *radix, bit *cutoff)
{
	if (l && r) {
		int hitrate = 1;
		return M4_RDX_phash_join(res, l, r, radix, &hitrate, cutoff);
	} else {
		return GDK_FAIL;
	}
}

int
M4_RDX_phash_join_hc(BAT **res, BAT *l, BAT *r, int *radix)
{
	if (l && r) {
		int hitrate = 1;
		bit cutoff = r->hkey;
		return M4_RDX_phash_join(res, l, r, radix, &hitrate, &cutoff);
	} else {
		return GDK_FAIL;
	}
}

int
M4_RDX_phash_join_xh(BAT **res, BAT *l, BAT *r, bit *cutoff)
{
	int rblt, rbrh;
	if (l && r &&
	    M4_RDX_radix_bits(&rblt, BATmirror(l)) == GDK_SUCCEED &&
	    M4_RDX_radix_bits(&rbrh, r) == GDK_SUCCEED) {
		int radix = MIN(rblt, rbrh);
		int hitrate = 1;
		return M4_RDX_phash_join(res, l, r, &radix, &hitrate, cutoff);
	} else {
		return GDK_FAIL;
	}
}

int
M4_RDX_phash_join_xhc(BAT **res, BAT *l, BAT *r)
{
	int rblt, rbrh;
	if (l && r &&
	    M4_RDX_radix_bits(&rblt, BATmirror(l)) == GDK_SUCCEED &&
	    M4_RDX_radix_bits(&rbrh, r) == GDK_SUCCEED) {
		int radix = MIN(rblt, rbrh);
		int hitrate = 1;
		bit cutoff = r->hkey;
		return M4_RDX_phash_join(res, l, r, &radix, &hitrate, &cutoff);
	} else {
		return GDK_FAIL;
	}
}


@= hash_merge
	if (rd == 0) {
		/* with zero bits, it is fairer not to perform a radix merge */
		hash_join(&h, bn, l, l_cur, l_last, r, r_cur, r_last, *radix, rcut);
	} else {
		hash_t y = @1_RADIX(BUN@2(r,r_cur),rd);

		/* hack alert */
		if (!h.mask)
			h.mask = 1;
		/* merge join on phash number */
		while(l_cur < l_last) {
			/* find l range */
			hash_t x = @1_RADIX(BUN@3(l,l_cur),rd);
			for(l_end=l_cur+xx; l_end < l_last; l_end += xx) {
				ptr v = BUN@3(l,l_end);
				if (@1_RADIX(v,rd) != x)
					break;
			}

			/* find matching r */
			while (y < x) {
				ptr v;
				if ((r_cur += yy) >= r_last)
					goto xit;
				v = BUN@2(r,r_cur);
				y = @1_RADIX(v,rd);
			}

			if (x == y) {  /* phash hits found */
				/* find r range */
				for (r_end=r_cur+yy; r_end < r_last; r_end += yy) {
					ptr v = BUN@2(r,r_end);
					y = @1_RADIX(v,rd);
					if (y != x)
						break;
				}
				if (hash_join(&h, bn, l, l_cur, l_end, r, r_cur, r_end, *radix, rcut) < 0) {
					BBPreclaim(bn);
					bn = NULL;
					goto xit;
				}
				r_cur = r_end;
			}
			l_cur = l_end;
		}
	}

@
@c
/*
 * @+ Radix Join
 * this is a nested-loop join on the matching clusters (inputs should be radix clustered)
 */
int
M4_RDX_radix_join(BAT **res, BAT *l, BAT *r, int *radix, int *hitrate)
{
	size_t estimate = MIN(BATcount(r), BATcount(l)) * (*hitrate);
	BUN r_end, l_cur = BUNfirst(l), l_last = BUNlast(l);
	BUN l_end, r_cur = BUNfirst(r), r_last = BUNlast(r);
	int xx = BUNsize(l), yy = BUNsize(r);
	size_t rd = (((size_t)1) << *radix) - 1;
	int any = ATOMstorage(l->ttype);
	ptr nil = ATOMnilptr(any);
	BAT *bn;

	if (*radix != (BATtordered(l) >> 1)) {
		GDKerror("radix_join: tail of %s is radix clustered on %d bits.\n", 
			BATgetId(l), BATtordered(l) >> 1);
		return GDK_FAIL;
	}
	if (*radix != (BAThordered(r) >> 1)) {
		GDKerror("radix_join: head of %s is radix clustered on %d bits.\n", 
			BATgetId(r), BAThordered(r) >> 1);
		return GDK_FAIL;
	}

	/* set properties on result bat; so we can return in any moment */
	bn = *res = BATnew(HTYPE(l), TTYPE(r), estimate);
	if (bn == NULL)
		return GDK_FAIL;
	bn->hsorted = BAThordered(l);
	bn->tsorted = 0;
	if (l->hkey && r->hkey)
		BATkey(bn, TRUE);
	if (r->tkey && l->tkey)
		BATkey(BATmirror(bn), TRUE);

	if (BATcount(r) == 0)
		return GDK_SUCCEED;

@(
/* PETER start experimentation hack */
	if (l->htype >= TYPE_integer1 && l->htype <= TYPE_pax256 && r->ttype >= TYPE_integer1 && r->ttype <= TYPE_pax256) {
		BAT *p1 = VIEWparent(l) ? BBP_cache(VIEWparent(l)) : l;
		BAT *p2 = VIEWparent(r) ? BBP_cache(VIEWparent(r)) : r;
		int width1 = ATOMsize(l->htype) >> 2;
		int distance1 = 1 << ((p1->htype - l->htype) >> 1);
		int width2 = ATOMsize(r->ttype) >> 2;
		int distance2 = 1 << ((p2->ttype - r->ttype) >> 1);

		@:radix_join(any,tail,head,atom,NOTSO,hloc,tloc)@
	} else
/* PETER end experimentation hack */
@)

	if (ATOMstorage(l->htype) == TYPE_int && ATOMstorage(r->ttype) == TYPE_int) {
		int *_dst = (int *) (bn->batBuns->base + bn->batBuns->free);
		int *_end = (int *) (bn->batBuns->base + bn->batBuns->size);

		if (l->ttype == TYPE_oid) {
			@:radix_join(oid,tloc,hloc,simple,int,hloc,tloc)@
		} else if (any == TYPE_int || any == TYPE_flt) {
			@:radix_join(int,tloc,hloc,simple,int,hloc,tloc)@
		} else if (any == TYPE_lng || any == TYPE_dbl) {
			@:radix_join(lng,tloc,hloc,simple,int,hloc,tloc)@
		} else {
			@:radix_join(any,tail,head,atom,int,hloc,tloc)@
		}
		bn->batBuns->free = ((BUN) _dst) - bn->batBuns->base;
		BATsetcount(bn, bn->batBuns->free/BUNsize(bn));
	} else {
		if (l->ttype == TYPE_oid) {
			@:radix_join(oid,tloc,hloc,simple,bun,head,tail)@
		} else if (any == TYPE_int || any == TYPE_flt) {
			@:radix_join(int,tloc,hloc,simple,bun,head,tail)@
		} else if (any == TYPE_lng || any == TYPE_dbl) {
			@:radix_join(lng,tloc,hloc,simple,bun,head,tail)@
		} else {
			@:radix_join(any,tail,head,atom,bun,head,tail)@
		}
	}
	return GDK_SUCCEED;
      bunins_failed:
	BBPreclaim(bn);
	return GDK_FAIL;
}

int
M4_RDX_radix_join_h(BAT **res, BAT *l, BAT *r, int *radix)
{
	int hitrate = 1;
	return M4_RDX_radix_join(res, l, r, radix, &hitrate);
}

int
M4_RDX_radix_join_xh(BAT **res, BAT *l, BAT *r)
{
	int rblt, rbrh;
	if (l && r &&
	    M4_RDX_radix_bits(&rblt, BATmirror(l)) == GDK_SUCCEED &&
	    M4_RDX_radix_bits(&rbrh, r) == GDK_SUCCEED) {
		int radix = MIN(rblt, rbrh);
		int hitrate = 1;
		return M4_RDX_radix_join(res, l, r, &radix, &hitrate);
	} else {
		return GDK_FAIL;
	}
}

@= radix_join
	hash_t y = @1_RADIX(BUN@3(r,r_cur),rd);

	/* merge join on radix number */
	while(l_cur < l_last) {
		/* find l range */
		hash_t x = @1_RADIX(BUN@2(l,l_cur),rd);
		for(l_end=l_cur+xx; l_end < l_last; l_end += xx) {
			if (@1_RADIX(BUN@2(l,l_end),rd) != x)
				break;
		}

		/* find matching r */
		while (y < x) {
			if ((r_cur += yy) >= r_last)
				return GDK_SUCCEED;
			y = @1_RADIX(BUN@3(r,r_cur),rd);
		}

		if (x == y) {		 /* radix hits found */
			/* find r range */
			for (r_end=r_cur+yy; r_end < r_last; r_end += yy) {
				y = @1_RADIX(BUN@3(r,r_end),rd);
				if (y != x)
					break;
			}

			/* filter the radix hits; L1 gushes with oil */
			while (l_cur < l_end) {
				BUN r_var = r_cur;
				ptr p = BUN@2(l,l_cur);
				if (!@4_EQ(p, nil, @1))
					while (r_var < r_end) {
						if (@4_EQ(p, BUN@3(r,r_var), @1)) {
							@5fastINSERT(bn, BUN@6(l,l_cur), BUN@7(r,r_var), l, l_cur, r, r_var);
						}
						r_var += yy;
					}
				l_cur += xx;
			}
			r_cur = r_end;
		}
		l_cur = l_end;
	}


@
 * @+ Radix Decluster
 * @T
 * Radix decluster reclusters a BAT that is partially radix-clustered on tail by converting
 * its non-sorted OID head column into a sorted and densely ascending void column. The tail
 * itself does not need to contain the partially sorted values; in fact we do not even look
 * at those values but directly take the cluster boundaries as an input parameter. This input
 * parameter is a radix_count BAT that must have been created with the radix_count command.
@= radix_decluster
{
	@1 *dst = (@1*) BUNfirst(bn);
	while (lim < lst) {
		lim += mult;
		yy = 0;
		while ((size_t) yy < cnt) {
			p = src + cnk[yy].cur;
			q = src + cnk[yy].end;
			for (;;) {
				if (p >= q) {
					ssize_t nxt = cnk[yy].nxt;
					if (nxt < 0)
						nxt = --cnt;
					cnk[yy] = cnk[nxt];
					break; /* cluster exhausted; overwrite with last; start over */
				}
				cur = *(oid*) BUNhloc(b,p);
				if (cur >= lim) {
					cnk[yy++].cur = p - src;
					break; /* value exceeds current limit, go to next cluster */
				}
				@2_put(dst[cur], @1, BUN@3(b,p));
				p += xx;
			}
		}
	}
	break;
}
@
@c
#define simple_put(var,tpe,val) var = *(tpe*) val
#define atom_put(var,tpe,val) ATOMput(any, bn->T->vheap, &var, val)

typedef struct {
	size_t cur, end;
	ssize_t nxt;
} cnk_t;

int
M4_RDX_radix_decluster(BAT **res, BAT *b, BAT *cnts, int *multiplier)
{
	size_t cnt = BATcount(cnts);
	int tpe = b->ttype;
	int any = ATOMstorage(tpe);
	int width = ATOMsize(ATOMtype(any));
	ssize_t mult = *multiplier;
	int xx, zz;
	ssize_t yy = 0;
	char *src = b->batBuns->base;
	size_t offset;
	oid cur, lim, lst;
	BUN p, q;
	BAT *bn;
	cnk_t *cnk;

	if ((cnts->talign & RADIX_COUNT_MASK) != (b->halign & RADIX_COUNT_MASK)) {
		GDKerror("radix_decluster: %s is not a radix_count(%s)\n", 
			BATgetId(cnts), BATgetId(b));
		return GDK_FAIL;
	}
/* TODO: propagation of tdense even when not sorted
    if (!b->hdense) {
	GDKerror("radix_decluster: %s may not contain a dense collection of oids.\n", BATgetId(b));
	return GDK_FAIL;
    }
*/
	if (BAThdense(b)) {
		BATseqbase(*res = VIEWhead(b), b->hseqbase);
		return GDK_SUCCEED;
	}

	/* init cluster offset arrays (byte distances) */
	cnk = (cnk_t *) GDKmalloc(cnt * sizeof(cnk_t));
	offset = BUNindex(b, BUNfirst(b)) * BUNsize(b);
	yy = -1;
	zz = -1;
	BATloopFast(cnts, p, q, xx) {
		ssize_t pos = yy, nxt = -1;

		if (*(int *) BUNhloc(cnts, p) != zz) {
			zz = *(int *) BUNhloc(cnts, p);	/* current bit pattern */
			pos = ++yy;	/* normal increment */
		} else {
			/* look in the other chunks to find who's first */
			while (*(oid *) BUNhloc(b, src + offset) > *(oid *) BUNhloc(b, src + cnk[pos].cur)) {
				if ((pos = cnk[pos].nxt) < 0)
					break;
			}
			if (pos > 0) {
				cnk[nxt = --cnt] = cnk[pos];	/* insert in chain */
			} else {
				pos = --cnt;	/* insert at end */
			}
		}
		cnk[pos].cur = offset;
		offset += BUNsize(b) * (*(int *) BUNtloc(cnts, p));
		cnk[pos].end = offset;
		cnk[pos].nxt = nxt;
	}

	/* create destination bat and init some variables */
	if (any == TYPE_str && GDK_ELIMDOUBLES(b->T->vheap)) {
		tpe = any = b->T->width == 1 ? TYPE_bte : (b->T->width == 2 ? TYPE_sht : (b->T->width == 4 ? TYPE_int : TYPE_lng));
	}

	/* init variables for the decluster operation */
	offset = BATcount(b);
	bn = BATnew(TYPE_void, tpe, offset);
	if (bn == NULL)
		return GDK_FAIL;
	*res = NULL;
	bn->batBuns->free = offset * width;
	BATsetcount(bn, offset);
	bn->tsorted = 0;
	BATseqbase(bn, lim = 0);	/* current oid */
	lst = lim + BATcount(b);	/* highest oid */
	mult *= cnt;		/* window size */
	xx = BUNsize(b);

	/* do the merging work. TODO: multi-pass impl? */
	switch (any) {
	case TYPE_chr:
	case TYPE_bte:
		@:radix_decluster(bte,simple,tloc)@

	case TYPE_sht:
		@:radix_decluster(sht,simple,tloc)@

	case TYPE_int:
	case TYPE_flt:
		@:radix_decluster(int,simple,tloc)@

	case TYPE_lng:
	case TYPE_dbl:
		@:radix_decluster(lng,simple,tloc)@

	default:
		@:radix_decluster(var_t,atom,tail)@
	}
	*res = bn;
      bunins_failed:
	GDKfree(cnk);

	/* string heaps with double elimination were treated as ints */
	if (any != b->ttype && ATOMstorage(b->ttype) == TYPE_str) {
		BAT *bm = BATmirror(bn);

		bn->T->vheap = (Heap*)GDKzalloc(sizeof(Heap));
		if (bn->T->vheap == NULL) {
			BBPreclaim(bn);
			return GDK_FAIL;
		}
		bn->T->vheap->parentid = bn->batCacheid;
		if (b->T->vheap->filename) {
			char *nme = BBP_physical(bn->batCacheid);
			
			bn->T->vheap->filename = (str) GDKmalloc(strlen(nme) + 12);
			GDKfilepath(bn->T->vheap->filename, NULL, nme, "theap");
		}
		if (HEAPcopy(bn->T->vheap, b->T->vheap) < 0) {
			*res = NULL;
		} else {
			bm->htype = bn->ttype = b->ttype;
			bm->hvarsized = bn->tvarsized = 1;
			bn->T->width = b->T->width;
			bn->T->shift = b->T->shift;
		}
	}
	if (*res == NULL) {
		BBPreclaim(bn);
		return GDK_FAIL;
	}
	return GDK_SUCCEED;
}

int
M4_RDX_radix_decluster_m(BAT **res, BAT *b, BAT *cnts)
{
	int multiplier = 4;
	return M4_RDX_radix_decluster(res, b, cnts, &multiplier);
}

/*
 * @- radix_decluster
 * intends to improve performance by using a cheaper positional join:
 * 	join(bat[void,oid], bat[void,T])
 * instead of
 * 	join(bat[oid,oid], bat[void,T])
 *
 * price paid are two parameters to radix_decluster.
 */
@= radix_decluster2
{
	@1 *dst = ((@1*) BUNfirst(bn)) - lim;
	while (lim < lst) {
		lim += mult;
		yy = 0;
		while ((size_t) yy < cnt) {
			p = src1 + cnk[yy].cur1;
			q = src1 + cnk[yy].end1;
			r = src2 + cnk[yy].cur2;
			for (;;) {
				if (p >= q) {
					ssize_t nxt = cnk[yy].nxt;
					if (nxt < 0)
						nxt = --cnt;
					if (nxt >= 0)
						cnk[yy] = cnk[nxt];
					break; /* cluster exhausted; overwrite with last; start over */
				}
				cur = *(oid*) BUNtloc(b,p);
				if (cur >= lim) {
					cnk[yy].cur2 = r - src2;
					cnk[yy++].cur1 = p - src1;
					break; /* value exceeds current limit, go to next cluster */
				}
				p += xx;
				@2_put(dst[cur], @1, BUN@3(a,r));
				r += zz;
			}
		}
	}
	break;
}
@= radix_decluster3
{
	@1 *dst = (@1*) BUNfirst(bn);
	/* int prev,all=0,bad=0; */
	while (lim < lst) {
		/* prev = lim; */
		lim += mult;
		yy = 0;
		while ((size_t) yy < cnt) {
			ssize_t nxt = yy;
			do {
				ssize_t yyy = nxt;
				nxt = cnk[yyy].nxt;
				p = src1 + cnk[yyy].cur1;
				q = src1 + cnk[yyy].end1;
				r = src2 + cnk[yyy].cur2;
				for (; p < q; p += xx, r += zz) {
					cur = *(oid*) BUNtloc(b,p);
					if (cur >= lim) {
						break; /* value exceeds current limit, go to next cluster */
					}
					@2_put(dst[cur], @1, BUN@3(a,r));
/*
					all++;
					if (prev > cur || cur > lim)
						bad++;
*/
					/* printf("3: %9d %9d %9d\n", prev, cur, lim ); */
				}
				/* cluster exhausted; overwrite with last; start over*/
				cnk[yyy].cur2 = r - src2;
				cnk[yyy].cur1 = p - src1;
			} while (nxt>=0);
			yy++;
		}
	}
/*    if (bad) printf("3: %d/%d = %f% miss-located writes!\n",bad,all,(flt)bad/(flt)all); */
	break;
}
@= radix_decluster4
{
	@1 *dst = (@1*) BUNfirst(bn);
	/* int prev,all=0,bad=0; */
	while (lim < lst) {
		/* prev = lim; */
		lim += mult;
		yy = 0;
		while ((size_t) yy < cnt) {
			p = src1 + cnk[yy].cur1;
			q = src1 + cnk[yy].end1;
			r = src2 + cnk[yy].cur2;
			cur = cnk[yy].nxt;
			while ((p < q) && (cur < lim)) {
/*
				all++;
				if (prev > cur || cur > lim)
					bad++;
*/
				/* printf("4: %9d %9d %9d\n", prev, cur, lim ); */
				@2_put(dst[cur], @1, BUN@3(a,r));
				p += xx;
				r += zz;
				cur = *(oid*) BUNtloc(b,p);
			}
			cnk[yy].nxt = cur;
			cnk[yy].cur2 = r - src2;
			cnk[yy++].cur1 = p - src1;
		}
	}
	/* if (bad) printf("4: %d/%d = %f% miss-located writes!\n",bad,all,(flt)bad/(flt)all); */
	break;
}
@
@c
typedef struct {
	size_t cur1, end1, cur2;
	ssize_t nxt;
} cnk2_t;

typedef struct {
	size_t cur1, end1, cur2;
	oid nxt;
} cnk4_t;

int
M4_RDX_radix_decluster2(BAT **res, BAT *b, BAT *a, BAT *cnts, int *multiplier)
{
	size_t cnt = BATcount(cnts);
	int tpe = a->ttype;
	int any = ATOMstorage(tpe);
	int width = ATOMsize(ATOMtype(any));
	ssize_t mult = *multiplier;
	int xx, zz;
	ssize_t yy = 0;
	char *src1 = b->batBuns->base + b->tloc;
	char *src2 = a->batBuns->base + a->tloc;
	size_t offset1, offset2;
	oid cur, lim, lst;
	BUN p, q, r;
	BAT *bn;
	cnk2_t *cnk;

	if ((cnts->talign & RADIX_COUNT_MASK) != (b->talign & RADIX_COUNT_MASK)) {
		GDKerror("radix_decluster: %s is not a radix_count(%s)\n", 
			BATgetId(cnts),  BATgetId(b));
		return GDK_FAIL;
	}
/* TODO: propagation of tdense even when not sorted
    if (!b->hdense) {
	GDKerror("radix_decluster: %s may not contain a dense collection of oids.\n", BATgetId(b));
	return GDK_FAIL;
    }
*/
	if (ALIGNsynced(b, a) == 0 || !(b->hkey || a->hkey) || b->ttype == TYPE_void) {
		GDKerror("radix_decluster: %s and %s are not synced and unique\n", 
			BATgetId(b),  BATgetId(a));
		return GDK_FAIL;
	}
	/* init cluster offset arrays (byte distances) */
	cnk = (cnk2_t *) GDKmalloc(cnt * sizeof(cnk2_t));
	offset1 = BUNindex(b, BUNfirst(b)) * BUNsize(b);
	offset2 = BUNindex(a, BUNfirst(a)) * BUNsize(a);
	zz = -1;
	yy = -1;
	BATloopFast(cnts, p, q, xx) {
		ssize_t pos = yy, nxt = -1;

		if (*(int *) BUNhloc(cnts, p) != zz) {
			zz = *(int *) BUNhloc(cnts, p);	/* current bit pattern */
			pos = ++yy;	/* normal increment */
		} else {
			ssize_t prev = -1;

			/* look in the other chunks to find who's first */
			while (*(oid *) BUNtloc(b, src1 + offset1) > *(oid *) BUNtloc(b, src1 + cnk[pos].cur1)) {
				prev = pos;
				if ((pos = cnk[pos].nxt) < 0)
					break;
			}
			if (pos > 0) {
				cnk[nxt = --cnt] = cnk[pos];	/* insert in chain */
			} else {
				pos = --cnt;	/* insert at end */
				if (prev >= 0)
					cnk[prev].nxt = pos;
			}
		}
		cnk[pos].cur1 = offset1;
		cnk[pos].cur2 = offset2;
		offset1 += BUNsize(b) * (*(int *) BUNtloc(cnts, p));
		offset2 += BUNsize(a) * (*(int *) BUNtloc(cnts, p));
		cnk[pos].end1 = offset1;
		cnk[pos].nxt = nxt;
	}

	/* create destination bat and init some variables */
	if (any == TYPE_str && GDK_ELIMDOUBLES(a->T->vheap)) {
		tpe = any = a->T->width == 1 ? TYPE_bte : (a->T->width == 2 ? TYPE_sht : (a->T->width == 4 ? TYPE_int : TYPE_lng));
	}

	/* init variables for the decluster operation */
	offset1 = BATcount(b);
	bn = BATnew(TYPE_void, tpe, offset1);
	if (bn == NULL)
		return GDK_FAIL;
	*res = NULL;
	bn->batBuns->free = offset1 * width;
	BATsetcount(bn, offset1);
	bn->tsorted = 0;
	lim = b->hseqbase;
	BATseqbase(bn, lim);	/* current oid */
	lst = lim + BATcount(b);	/* highest oid */
	mult *= cnt;		/* window size */
	xx = BUNsize(b);
	zz = BUNsize(a);

	/* do the merging work. TODO: multi-pass impl? */
	switch (any) {
	case TYPE_chr:
	case TYPE_bte:
		@:radix_decluster2(bte,simple,tloc)@

	case TYPE_sht:
		@:radix_decluster2(sht,simple,tloc)@

	case TYPE_int:
	case TYPE_flt:
		@:radix_decluster2(int,simple,tloc)@

	case TYPE_lng:
	case TYPE_dbl:
		@:radix_decluster2(lng,simple,tloc)@

	default:
		@:radix_decluster2(var_t,atom,tail)@
	}
	*res = bn;
      bunins_failed:
	GDKfree(cnk);

	/* string heaps with double elimination were treated as ints */
	if (any != a->ttype && ATOMstorage(a->ttype) == TYPE_str) {
		BAT *bm = BATmirror(bn);

		bn->T->vheap = (Heap*)GDKzalloc(sizeof(Heap));
		if (bn->T->vheap == NULL) {
			BBPreclaim(bn);
			return GDK_FAIL;
		}
		bn->thead->parentid = bn->batCacheid;
		if (a->T->vheap->filename) {
			char *nme = BBP_physical(bn->batCacheid);
			
			bn->T->vheap->filename = (str) GDKmalloc(strlen(nme) + 12);
			GDKfilepath(bn->T->vheap->filename, NULL, nme, "theap");
		}
		if (HEAPcopy(bn->T->vheap, a->T->vheap) < 0) {
			*res = NULL;
		} else {
			bm->htype = bn->ttype = a->ttype;
			bm->hvarsized = bn->tvarsized = 1;
			bn->T->width = a->T->width;
			bn->T->shift = a->T->shift;
		}
	}
	if (*res == NULL) {
		BBPreclaim(bn);
		return GDK_FAIL;
	}
	return GDK_SUCCEED;
}

int
M4_RDX_radix_decluster3(BAT **res, BAT *b, BAT *a, BAT *cnts, int *multiplier)
{
	size_t cnt = BATcount(cnts);
	int tpe = a->ttype;
	int any = ATOMstorage(tpe);
	int width = ATOMsize(ATOMtype(any));
	ssize_t mult = *multiplier;
	int xx, zz;
	ssize_t yy = 0;
	char *src1 = b->batBuns->base + b->tloc;
	char *src2 = a->batBuns->base + a->tloc;
	size_t offset1, offset2;
	oid cur, lim, lst;
	BUN p, q, r;
	BAT *bn;
	cnk2_t *cnk;

	if ((cnts->talign & RADIX_COUNT_MASK) != (b->talign & RADIX_COUNT_MASK)) {
		GDKerror("radix_decluster: %s is not a radix_count(%s)\n", 
		BATgetId(cnts),  BATgetId(b));
		return GDK_FAIL;
	}
/* TODO: propagation of tdense even when not sorted
    if (!b->hdense) {
	GDKerror("radix_decluster: %s may not contain a dense collection of oids.\n", BATgetId(b));
	return GDK_FAIL;
    }
*/
	if (ALIGNsynced(b, a) == 0 || !(b->hkey || a->hkey) || b->ttype == TYPE_void) {
		GDKerror("radix_decluster: %s and %s are not synced and unique\n", 
			BATgetId(b),  BATgetId(a));
		return GDK_FAIL;
	}
	/* init cluster offset arrays (byte distances) */
	cnk = (cnk2_t *) GDKmalloc(cnt * sizeof(cnk2_t));
	offset1 = BUNindex(b, BUNfirst(b)) * BUNsize(b);
	offset2 = BUNindex(a, BUNfirst(a)) * BUNsize(a);
	zz = -1;
	yy = -1;
	BATloopFast(cnts, p, q, xx) {
		ssize_t pos = yy, nxt = -1;

		if (*(int *) BUNhloc(cnts, p) != zz) {
			zz = *(int *) BUNhloc(cnts, p);	/* current bit pattern */
			pos = ++yy;	/* normal increment */
		} else {
			ssize_t prev = -1;

			/* look in the other chunks to find who's first */
			while (*(oid *) BUNtloc(b, src1 + offset1) > *(oid *) BUNtloc(b, src1 + cnk[pos].cur1)) {
				prev = pos;
				if ((pos = cnk[pos].nxt) < 0)
					break;
			}
			if (pos > 0) {
				cnk[nxt = --cnt] = cnk[pos];	/* insert in chain */
			} else {
				pos = --cnt;	/* insert at end */
				if (prev >= 0)
					cnk[prev].nxt = pos;
			}
		}
		cnk[pos].cur1 = offset1;
		cnk[pos].cur2 = offset2;
		offset1 += BUNsize(b) * (*(int *) BUNtloc(cnts, p));
		offset2 += BUNsize(a) * (*(int *) BUNtloc(cnts, p));
		cnk[pos].end1 = offset1;
		cnk[pos].nxt = nxt;
	}

	/* create destination bat and init some variables */
	if (any == TYPE_str && GDK_ELIMDOUBLES(a->T->vheap)) {
		tpe = any = a->T->width == 1 ? TYPE_bte : (a->T->width == 2 ? TYPE_sht : (a->T->width == 4 ? TYPE_int : TYPE_lng));
	}

	/* init variables for the decluster operation */
	offset1 = BATcount(b);
	bn = *res = BATnew(TYPE_void, tpe, offset1);
	if (bn == NULL)
		return GDK_FAIL;
	*res = NULL;
	bn->batBuns->free = offset1 * width;
	BATsetcount(bn, offset1);
	bn->tsorted = 0;
	BATseqbase(bn, lim = 0);	/* current oid */
	lst = lim + BATcount(b);	/* highest oid */
	mult *= cnt;		/* window size */
	xx = BUNsize(b);
	zz = BUNsize(a);

	/* do the merging work. TODO: multi-pass impl? */
	switch (any) {
	case TYPE_chr:
	case TYPE_bte:
		@:radix_decluster3(bte,simple,tloc)@

	case TYPE_sht:
		@:radix_decluster3(sht,simple,tloc)@

	case TYPE_int:
	case TYPE_flt:
		@:radix_decluster3(int,simple,tloc)@

	case TYPE_lng:
	case TYPE_dbl:
		@:radix_decluster3(lng,simple,tloc)@

	default:
		@:radix_decluster3(var_t,atom,tail)@
	}
	*res = bn;
      bunins_failed:
	GDKfree(cnk);

	/* string heaps with double elimination were treated as ints */
	if (any != a->ttype && ATOMstorage(a->ttype) == TYPE_str) {
		BAT *bm = BATmirror(bn);

		bn->T->vheap = (Heap*)GDKzalloc(sizeof(Heap));
		if (bn->T->vheap == NULL) {
			BBPreclaim(bn);
			return GDK_FAIL;
		}
		bn->T->vheap->parentid = bn->batCacheid;
		if (a->T->vheap->filename) {
			char *nme = BBP_physical(bn->batCacheid);
			
			bn->T->vheap->filename = (str) GDKmalloc(strlen(nme) + 12);
			GDKfilepath(bn->T->vheap->filename, NULL, nme, "theap");
		}
		if (HEAPcopy(bn->T->vheap, a->T->vheap) < 0) {
			*res = NULL;
		} else {
			bm->htype = bn->ttype = a->ttype;
			bm->hvarsized = bn->tvarsized = 1;
			bn->T->width = a->T->width;
			bn->T->shift = a->T->shift;
		}
	}
	if (*res == NULL) {
		BBPreclaim(bn);
		return GDK_FAIL;
	}
	return GDK_SUCCEED;
}

int
M4_RDX_radix_decluster4(BAT **res, BAT *b, BAT *a, BAT *cnts, int *multiplier)
{
	size_t cnt = BATcount(cnts), cntX = 0;
	int tpe = a->ttype;
	int any = ATOMstorage(tpe);
	int width = ATOMsize(ATOMtype(any));
	ssize_t mult = *multiplier;
	int xx, zz;
	ssize_t yy = 0;
	char *src1 = b->batBuns->base + b->tloc;
	char *src2 = a->batBuns->base + a->tloc;
	size_t offset1, offset2;
	oid cur, lim, lst;
	BUN p, q, r;
	BAT *bn;
	cnk4_t *cnk;

	if ((cnts->talign & RADIX_COUNT_MASK) != (b->talign & RADIX_COUNT_MASK)) {
		GDKerror("radix_decluster: %s is not a radix_count(%s)\n", 
			BATgetId(cnts), BATgetId(b));
		return GDK_FAIL;
	}
/* TODO: propagation of tdense even when not sorted
    if (!b->hdense) {
	GDKerror("radix_decluster: %s may not contain a dense collection of oids.\n", BATgetId(b));
	return GDK_FAIL;
    }
*/
	if (ALIGNsynced(b, a) == 0 || !(b->hkey || a->hkey) || b->ttype == TYPE_void) {
		GDKerror("radix_decluster: %s and %s are not synced and unique\n", 
			BATgetId(b),  BATgetId(a));
		return GDK_FAIL;
	}
	/* init cluster offset arrays (byte distances) */
	cnk = (cnk4_t *) GDKmalloc(cnt * sizeof(cnk4_t));
	offset1 = BUNindex(b, BUNfirst(b)) * BUNsize(b);
	offset2 = BUNindex(a, BUNfirst(a)) * BUNsize(a);
	yy = -1;
	zz = -1;
	BATloopFast(cnts, p, q, xx) {
		if (*(int *) BUNhloc(cnts, p) != zz) {
			zz = *(int *) BUNhloc(cnts, p);	/* current bit pattern */
			cntX++;	/* new "real" cluster */
		}
		yy++;
		cnk[yy].nxt = *(oid *) BUNtloc(b, src1 + offset1);
		cnk[yy].cur1 = offset1;
		cnk[yy].cur2 = offset2;
		offset1 += BUNsize(b) * (*(int *) BUNtloc(cnts, p));
		offset2 += BUNsize(a) * (*(int *) BUNtloc(cnts, p));
		cnk[yy].end1 = offset1;
	}

	/* create destination bat and init some variables */
	if (any == TYPE_str && GDK_ELIMDOUBLES(a->T->vheap)) {
		tpe = any = a->T->width == 1 ? TYPE_bte : (a->T->width == 2 ? TYPE_sht : (a->T->width == 4 ? TYPE_int : TYPE_lng));
	}

	/* init variables for the decluster operation */
	offset1 = BATcount(b);
	bn = BATnew(TYPE_void, tpe, offset1);
	if (bn == NULL)
		return GDK_FAIL;
	*res = NULL;
	bn->batBuns->free = offset1 * width;
	BATsetcount(bn, offset1);
	bn->tsorted = 0;
	BATseqbase(bn, lim = 0);	/* current oid */
	lst = lim + BATcount(b);	/* highest oid */
	mult *= cntX;		/* window size */
	xx = BUNsize(b);
	zz = BUNsize(a);

	/* do the merging work. TODO: multi-pass impl? */
	switch (any) {
	case TYPE_chr:
	case TYPE_bte:
		@:radix_decluster4(bte,simple,tloc)@

	case TYPE_sht:
		@:radix_decluster4(sht,simple,tloc)@

	case TYPE_int:
	case TYPE_flt:
		@:radix_decluster4(int,simple,tloc)@

	case TYPE_lng:
	case TYPE_dbl:
		@:radix_decluster4(lng,simple,tloc)@

	default:
		@:radix_decluster4(var_t,atom,tail)@
	}
	*res = bn;
      bunins_failed:
	GDKfree(cnk);

	/* string heaps with double elimination were treated as ints */
	if (any != a->ttype && ATOMstorage(a->ttype) == TYPE_str) {
		BAT *bm = BATmirror(bn);

		bn->T->vheap = (Heap*)GDKzalloc(sizeof(Heap));
		if (bn->T->vheap == NULL) {
			BBPreclaim(bn);
			return GDK_FAIL;
		}
		bn->T->vheap->parentid = bn->batCacheid;
		if (a->T->vheap->filename) {
			char *nme = BBP_physical(bn->batCacheid);
			
			bn->T->vheap->filename = (str) GDKmalloc(strlen(nme) + 12);
			GDKfilepath(bn->T->vheap->filename, NULL, nme, "theap");
		}
		if (HEAPcopy(bn->T->vheap, a->T->vheap) < 0) {
			*res = NULL;
		} else {
			bm->htype = bn->ttype = a->ttype;
			bm->hvarsized = bn->tvarsized = 1;
			bn->T->width = a->T->width;
			bn->T->shift = a->T->shift;
		}
	}
	if (*res == NULL) {
		BBPreclaim(bn);
		return GDK_FAIL;
	}
	return GDK_SUCCEED;
}

@(
int
M4_RDX_posjoin_tuple(BAT **res, BAT *proj, BAT *attr)
{
	BAT *parent = VIEWparent(attr) ? BBP_cache(VIEWparent(attr)) : attr;
	BAT *bn = *res = BATnew(TYPE_void, TTYPE(attr), BATcount(proj));
	BAT *bm = BATmirror(attr);
	int width = ATOMsize(attr->ttype) >> 2;
	int distance = 1 << ((parent->ttype - attr->ttype) >> 1);
	BUN p, q, r, s;
	int xx, yy;

	if (bn == NULL) {
		GDKerror("posjoin_tuple: could not alloc BAT[void,%s] of size " BUNFMT ".\n", ATOMname(attr->ttype), BATcount(proj));
		return GDK_FAIL;
	}
	s = BUNfirst(bn);
	yy = BUNsize(bn);
	BATloopFast(proj, p, q, xx) {
		BUNfndVOID(r, attr, BUNtail(proj, p));
		integerCopy((int *) s, bm, r, width, distance);
		s += yy;
	}
	bn->batBuns->free = BUNsize(bn) * BATcount(proj);
	BATsetcount(bn, BATcount(proj));
	BATseqbase(bn, proj->hseqbase);
	ALIGNsetH(bn, proj);
	bn->tsorted = BATtordered(proj) & BATtordered(attr);
	if (proj->tkey && attr->tkey)
		BATkey(BATmirror(bn), TRUE);
	return GDK_SUCCEED;
}
@)

/*
 * @+ jivejoin
 *
 * First phase of jivejoin does a positional join of join index with the left table.
 * Its output is clustered inline on left head-oid in a single pass and consists of two separate
 * (synced on void head) bats, one with the attribute values, the other with the other table oids.
 */
@= jivejoin0
	if (@6 || proj->ttype != TYPE_void) {
		size_t off = BUNindex(attr, BUNfirst(attr)) - attr->hseqbase;
		if (proj->htype == TYPE_oid) {
			BATloopFast(proj, p, q, xx) {
				oid curoid = *(oid*) BUN@3(proj,p);
				size_t clusterid = (curoid&yy) >> zz;
				r = BUNptr(attr, off + *(oid*) BUN@4(proj, p));
				s = BUNptr(bn,cluster[clusterid]);
				*(oid*) BUNhloc(bn,s) = *(oid*) BUNhloc(proj,p);
				@2put(@1, bn->T->vheap, BUNtloc(bn,s), @5);
				cluster[clusterid]++;
			}
		} else {
			BATloopFast(proj, p, q, xx) {
				oid curoid = *(oid*) BUN@3(proj,p);
				size_t clusterid = (curoid&yy) >> zz;
				r = BUNptr(attr, off + *(oid*) BUN@4(proj, p));
				s = BUNptr(bn,cluster[clusterid]);
				@2put(@1, bn->T->vheap, s, @5);
				cluster[clusterid]++;
			}
		}
		break;
	}
@
@c
#define SIMPLEput(tpe,hp,dst,src) *(tpe*) (dst) = *(tpe*) (src)

int
M4_RDX_jivejoin0(BAT **res, BAT *proj, BAT *attr, BAT *radix_count, int *shift, int *nbits)
{
	int any = attr->ttype;
	BAT *bn = BATnew(proj->htype, any, BATcount(proj));
	int zz = *shift;
	size_t xx, yy = (size_t) 1 << *nbits;
	size_t *cluster = (size_t *) GDKmalloc((yy + 1) * sizeof(size_t));
	BUN p, q, r, s;

	if (bn == NULL || cluster == NULL) {
		GDKerror("jivejoin0: could not alloc BAT[void,%s] of size " BUNFMT ".\n", ATOMname(attr->ttype), BATcount(proj));
		return GDK_FAIL;
	}
	*res = NULL;

	/* find the destination byte-offsets using the radix-count bat */
	for (xx = 0; xx <= yy; xx++)
		cluster[xx] = 0;
	BATloopFast(radix_count, p, q, xx) {
		int idx = *(int *) BUNhloc(radix_count, p);

		if (idx < 0 || (size_t) idx >= yy) {
			BBPreclaim(bn);
			GDKerror("jivejoin0: illegal cluster id %d in %s.\n", idx, BATgetId(radix_count));
			return GDK_FAIL;
		}
		cluster[1 + idx] += *(int *) BUNtloc(radix_count, p);
	}
	cluster[0] = BUNindex(bn, BUNfirst(bn));
	for (xx = 1; xx <= yy; xx++)
		cluster[xx] += cluster[xx - 1];
	xx = cluster[yy] - cluster[0];
	if (xx != BATcount(proj)) {
		BBPreclaim(bn);
		GDKerror("jivejoin0: total cluster entries %lu in %s does not match size %lu of %s.\n", xx, BATgetId(radix_count), BATcount(proj), BATgetId(proj));
		return GDK_FAIL;
	}
	yy = (yy - 1) << zz;	/* yy is mask, zz is shift */

	switch (any = ATOMstorage(any)) {
	case TYPE_chr:
	case TYPE_bte:
		@:jivejoin0(bte,SIMPLE,hloc,tloc,r,0)@

	case TYPE_sht:
		@:jivejoin0(sht,SIMPLE,hloc,tloc,r,0)@

	case TYPE_int:
	case TYPE_flt:
		@:jivejoin0(int,SIMPLE,hloc,tloc,r,0)@

	case TYPE_dbl:
	case TYPE_lng:
		@:jivejoin0(lng,SIMPLE,hloc,tloc,r,0)@

	default:
	      @:jivejoin0(any,ATOM,head,tail,BUNtail(attr,r),1)@
	}
	*res = bn;
      bunins_failed:
	if (*res == NULL) {
		BBPreclaim(bn);
		return GDK_FAIL;
	}

	/* restore normality in destination bats */
	bn->batBuns->free = BATcount(proj) * BUNsize(bn);
	BATsetcount(bn, BATcount(proj));
	if (proj->htype == TYPE_oid) {
		bn->hsorted = 0;
		if (proj->hkey)
			BATkey(bn, TRUE);
	} else {
		BATseqbase(bn, 0);
	}
	bn->tsorted = 0;
	if (proj->tkey && attr->tkey)
		BATkey(BATmirror(bn), TRUE);
	return GDK_SUCCEED;
}

@= jivejoin1
	if (@6 || (proj->htype != TYPE_void && proj->ttype != TYPE_void)) {
		size_t off = BUNindex(attr, BUNfirst(attr)) - attr->hseqbase;
		BATloopFast(proj, p, q, xx) {
			oid curoid = *(oid*) BUN@3(proj,p);
			size_t clusterid = (curoid&yy) >> zz;
			r = BUNptr(attr, off + *(oid*) BUN@4(proj, p));
			*(oid*) BUNptr(bo,cluster[clusterid]) = curoid;
			@2put(@1, bn->T->vheap, BUNptr(bn,cluster[clusterid]), @5);
			cluster[clusterid]++;
		}
		break;
	}
@
@c
int
M4_RDX_jivejoin1(BAT **res, BAT *bo, BAT *proj, BAT *attr, BAT *radix_count, int *shift, int *nbits)
{
@(
	BAT *parent = VIEWparent(attr) ? BBP_cache(VIEWparent(attr)) : attr;
@)
	int any = attr->ttype;
	BAT *bn = BATnew(TYPE_void, TTYPE(attr), BATcount(proj));
	size_t xx = bo->batBuns->free, yy = (size_t) 1 << *nbits, cnt = bo->batCount;
	int zz = *shift;
	size_t *cluster = (size_t *) GDKmalloc((yy + 1) * sizeof(size_t));
	BUN p, q, r, bobak;

	if (bn == NULL || cluster == NULL) {
		GDKerror("jivejoin1: could not alloc BAT[void,%s] of size " BUNFMT ".\n", ATOMname(attr->ttype), BATcount(proj));
		return GDK_FAIL;
	}
	*res = NULL;

	bo->batBuns->free += BATcount(proj) * BUNsize(bo);
	bo->batCount += BATcount(proj);
	if (bo->batBuns->free > bo->batBuns->size) {
		bo->batBuns->free = xx;
		bo->batCount = cnt;
		BBPreclaim(bn);
		GDKerror("jivejoin1: preallocated second result BAT is too small.\n");
		return GDK_FAIL;
	}

	/* find the destination byte-offsets using the radix-count bat */
	for (xx = 0; xx <= yy; xx++)
		cluster[xx] = 0;
	BATloopFast(radix_count, p, q, xx) {
		int idx = *(int *) BUNhloc(radix_count, p);

		if (idx < 0 || (size_t) idx >= yy) {
			BBPreclaim(bn);
			GDKerror("jivejoin1: illegal cluster id %d in %s.\n", idx, BATgetId(radix_count));
			return GDK_FAIL;
		}
		cluster[1 + idx] += *(int *) BUNtloc(radix_count, p);
	}
	cluster[0] = BUNindex(bn, BUNfirst(bn));
	for (xx = 1; xx <= yy; xx++)
		cluster[xx] += cluster[xx - 1];
	xx = cluster[yy] - cluster[0];
	if (xx != BATcount(proj)) {
		BBPreclaim(bn);
		GDKerror("jivejoin1: total cluster entries " SZFMT " in %s does not match size " BUNFMT " of %s.\n", xx, BATgetId(radix_count), BATcount(proj), BATgetId(proj));
		return GDK_FAIL;
	}
	yy = (yy - 1) << zz;	/* yy is mask, zz is shift */

	/* make the BUNindex of bo the same as those in bn */
	bobak = bo->batBuns->base;
	bo->batBuns->base += BUNsize(bo) * (BUNindex(bo, BUNfirst(bo)) - cluster[0]);

@(
/* PETER start experimentation hack; must emulate relational projection cost  */
	if (any >=TYPE_integer1 && any <=TYPE_pax256) {
		BAT *bm = BATmirror(attr);
		int width = ATOMsize(attr->ttype) >> 2;
		int distance = 1 << (parent->ttype - attr->ttype);

		BATloopFast(proj, p, q, xx) {
			oid curoid = *(oid *) BUNhead(proj, p);
			size_t clusterid = (curoid & yy) >> zz;

			BUNfndVOID(r, attr, BUNtail(proj, p));
			*(oid *) BUNptr(bo, cluster[clusterid]) = curoid;
			integerCopy((int *) BUNptr(bn, cluster[clusterid]), bm, r, width, distance);

			cluster[clusterid]++;
		}
	} else
/* PETER end experimentation hack */
@)

		switch (any = ATOMstorage(any)) {
		case TYPE_chr:
		case TYPE_bte:
			@:jivejoin1(bte,SIMPLE,hloc,tloc,r,0)@

		case TYPE_sht:
			@:jivejoin1(sht,SIMPLE,hloc,tloc,r,0)@

		case TYPE_int:
		case TYPE_flt:
			@:jivejoin1(int,SIMPLE,hloc,tloc,r,0)@

		case TYPE_dbl:
		case TYPE_lng:
			@:jivejoin1(lng,SIMPLE,hloc,tloc,r,0)@

		default:
		      @:jivejoin1(any,ATOM,head,tail,BUNtail(attr,r),1)@
		}
	*res = bn;
      bunins_failed:

	/* restore normality in destination bats */
	bo->batBuns->base = bobak;
	bo->tsorted = *nbits << 1;
	if (proj->hkey)
		BATkey(BATmirror(bo), TRUE);
	BATseqbase(bo, 0);
	if (*res == NULL) {
		BBPreclaim(bn);
		return GDK_FAIL;
	}
	bn->batBuns->free = BATcount(proj) * BUNsize(bn);
	BATsetcount(bn, BATcount(proj));
	bn->tsorted = 0;
	if (proj->tkey && attr->tkey)
		BATkey(BATmirror(bn), TRUE);
	BATseqbase(bn, 0);
	return GDK_SUCCEED;
}

@= jivejoin2
	if (@6 || proj->ttype != TYPE_void) {
		ssize_t off = (ssize_t) (BUNindex(attr, BUNfirst(attr)) - attr->hseqbase);
		BATloopFast(proj, p, q, xx) {
			r = BUNptr(attr, off + *(oid*) BUN@4(proj, p));
			@2put(@1, bn->T->vheap, BUNptr(bn, *(oid*) BUN@3(proj,p)), @5);
		}
		break;
	}
@
@c
int
M4_RDX_jivejoin2(BAT **res, BAT *proj, BAT *attr)
{
@(
	BAT *parent = VIEWparent(attr) ? BBP_cache(VIEWparent(attr)) : attr;
@)
	int any = attr->ttype;
	BAT *bn = BATnew(TYPE_void, TTYPE(attr), BATcount(proj));
	BUN p, q, r, bnbak;
	int xx;

	if (bn == NULL) {
		GDKerror("jivejoin2: could not alloc BAT[void,%s] of size " BUNFMT ".\n", ATOMname(attr->ttype), BATcount(proj));
		return GDK_FAIL;
	}
	/* make BUNptr(0) point to BUNfirst */
	bnbak = bn->batBuns->base;
	bn->batBuns->base = BUNfirst(bn);

@(
/* PETER start experimentation hack; must emulate relational projection cost  */
	if (any >=TYPE_integer1 && any <=TYPE_pax256) {
		BAT *bm = BATmirror(attr);
		int width = ATOMsize(attr->ttype) >> 2;
		int distance = 1 << (parent->ttype - attr->ttype);

		BATloopFast(proj, p, q, xx) {
			BUNfndVOID(r, attr, BUNtail(proj, p));
			integerCopy((int *) BUNptr(bn, *(oid *) BUNhead(proj, p)), bm, r, width, distance);
		}
	} else
/* PETER end experimentation hack */
@)

		switch (any = ATOMstorage(any)) {
		case TYPE_chr:
		case TYPE_bte:
			@:jivejoin2(bte,SIMPLE,hloc,tloc,r,0)@

		case TYPE_sht:
			@:jivejoin2(sht,SIMPLE,hloc,tloc,r,0)@

		case TYPE_int:
		case TYPE_flt:
			@:jivejoin2(int,SIMPLE,hloc,tloc,r,0)@

		case TYPE_dbl:
		case TYPE_lng:
			@:jivejoin2(lng,SIMPLE,hloc,tloc,r,0)@

		default:
		      @:jivejoin2(any,ATOM,head,tail,BUNtail(attr,r),1)@
		}
	*res = bn;
      bunins_failed:
	if (*res == NULL) {
		BBPreclaim(bn);
		return GDK_FAIL;
	}

	/* restore correct properties in bn */
	bn->batBuns->base = bnbak;
	bn->batBuns->free = BUNsize(bn) * BATcount(proj);
	BATsetcount(bn, BATcount(proj));
	BATseqbase(bn, 0);
	bn->tsorted = ( BAThordered(proj) & BATtordered(proj) & BATtordered(attr) & 1 ? GDK_SORTED : FALSE);
	if (proj->tkey && attr->tkey)
		BATkey(BATmirror(bn), TRUE);
	return GDK_SUCCEED;
}

/*
 * @+ Prefetching
 * Seems hardware prefetching isn't up to its take on most platforms (P4 being the exception). Therefore we implement
 * software prefetching optimized per platform. A default implementation just sums int with cacheline strides.
 * For AMD Athlon systems backward read of 'movl's is optimal (see www.amd.com).  For systems without hardware
 * prefetching, a software prefetch is implemented, ie a simple prefetch loop.
 */
#ifdef HAVE_RESTRICT
#define __r	restrict
#else
#ifdef HAVE___RESTRICT__
#define __r	__restrict__
#else
#define __r
#endif
#endif

#if defined(ATHLON)
static inline size_t
Mem2Cache(int *__r from, size_t size)
{
	int cnt = (int) (size / 128);
	__asm__ __volatile__ (
		"movl %2, %%eax\n\t"
		"movl %0, %%esi\n\t"
		"addl %1, %%esi\n\t"
		"1: movl -128(%%esi), %%edx\n\t"
		"movl -64(%%esi), %%edx\n\t"
		"subl $128, %%esi\n\t"
		"dec %%eax\n\t"
		"jnz 1b\n\t"
		:  : "r" (from), "r" (size), "r" (cnt) : "memory",  "%eax", "%edx", "%esi");
	return (size_t) cnt;
}
#else

#ifdef HAVE_XMMINTRIN_H
/* L2 has 128 byte cachelines */
static inline size_t
Mem2Cache(int *__r from, size_t size)
{
	char *data = (char *) from;
	size_t i;

	for (i = 0; i < size; i += 64) {
#ifndef __GNUC__
		_mm_prefetch(data, _MM_HINT_NTA);
#else
		__builtin_prefetch(data);
#endif
		data += 64;
	}
	return i;
}
#else

#define CACHELINE (128)
#define INTCACHELINE (CACHELINE/sizeof(int))

static inline size_t
Mem2Cache(int *__r p, size_t size)
{
	size_t sum0 = 0, sum1 = 0, i = 0;
	size /= sizeof(int);

	while (i + (8 * INTCACHELINE) < size) {
		sum0 += p[i + 0 * INTCACHELINE];
		sum1 += p[i + 1 * INTCACHELINE];
		sum0 += p[i + 2 * INTCACHELINE];
		sum1 += p[i + 3 * INTCACHELINE];
		sum0 += p[i + 4 * INTCACHELINE];
		sum1 += p[i + 5 * INTCACHELINE];
		sum0 += p[i + 6 * INTCACHELINE];
		sum1 += p[i + 7 * INTCACHELINE];
		i += INTCACHELINE * 8;
	}
	return sum0 + sum1;
}

#endif
#endif

/*
 * @+ clustered positional join
 */
static dbl
prefetch(ssize_t n, int stride, dbl *__r src)
{
	dbl tot = 0;
	ssize_t i;

	n >>= 3;
	if (stride == 1) {
		Mem2Cache((int *) src, n);
	} else if (stride == 32) {
		for (i = 0; i + 16 < n; i += 16)
			tot += src[i] + src[i + 4] + src[i + 8] + src[i + 12];
	} else if (stride == 64) {
		for (i = 0; i + 32 < n; i += 32)
			tot += src[i] + src[i + 8] + src[i + 16] + src[i + 24];
	} else if (stride == 128) {
		for (i = 0; i + 64 < n; i += 64)
			tot += src[i] + src[i + 16] + src[i + 32] + src[i + 48];
	} else if (stride == 256) {
		for (i = 0; i + 128 < n; i += 128)
			tot += src[i] + src[i + 32] + src[i + 64] + src[i + 96];
	} else if (stride > 8) {
		for (i = 0; i < n; i += stride)
			tot += src[i];
	}
	return tot;
}

@= posjoin
static int
posjoin_@1(int pf, @1*__r p1, int oidhead, ssize_t n, @1*__r res, oid*__r pos, @1*__r val)
{
	@1 tot1 = 0, tot2 = 0, tot3 = 0, tot4 = 0;
	ssize_t i;
	if (p1) {
		if (oidhead) {
			/* pos = tail of BAT[oid,oid]
			for(i=0; i+7<n; i+=4) {
				res[i] = val[pos[i+i]];
				tot1 += p1[0];
				res[i+1] = val[pos[i+i+2]];
				tot2 += p1[pf];
				res[i+2] = val[pos[i+i+4]];
				tot3 += p1[pf+pf];
				res[i+3] = val[pos[i+i+6]];
				tot4 += p1[pf+pf+pf];
				p1 += pf+pf+pf+pf;
			}
*/
			for (i=0; i<n; i++) {
				res[i] = val[pos[i+i]];
				tot1 += *p1;
				p1 += pf;
			}
		} else {
			/* pos = tail of BAT[void,oid]
			for(i=0; i+3<n; i+=4) {
				res[i] = val[pos[i]];
				tot1 += p1[0];
				res[i+1] = val[pos[i+1]];
				tot2 += p1[pf];
				res[i+2] = val[pos[i+2]];
				tot3 += p1[pf+pf];
				res[i+3] = val[pos[i+3]];
				tot4 += p1[pf+pf+pf];
				p1 += pf+pf+pf+pf;
			}
*/
			for (i=0; i<n; i++) {
				res[i] = val[pos[i]];
				tot1 += *p1;
				p1 += pf;
			}
		}
	} else {
		if (oidhead) {
			/* pos = tail of BAT[oid,oid] */
			for (i=0; i+7<n; i+=4) {
				res[i] = val[pos[i+i]];
				res[i+1] = val[pos[i+i+2]];
				res[i+2] = val[pos[i+i+4]];
				res[i+3] = val[pos[i+i+6]];
			}
			for (i=0; i<n; i++) {
				res[i] = val[pos[i+i]];
			}
		} else {
			/* pos = tail of BAT[void,oid] */
			for (i=0; i+3<n; i+=4) {
				res[i] = val[pos[i]];
				res[i+1] = val[pos[i+1]];
				res[i+2] = val[pos[i+2]];
				res[i+3] = val[pos[i+3]];
			}
			for (i=0; i<n; i++) {
				res[i] = val[pos[i]];
			}
		}
	}
	return (int) (tot1 + tot2 + tot3 + tot4);
}
@
@c
@:posjoin(bte)@
@:posjoin(sht)@
@:posjoin(int)@
@:posjoin(dbl)@

int
M4_RDX_posjoin_clustered(BAT **r, BAT *c, BAT *v, int *rb, int *st, int *vs)
{
	ssize_t n, c_cnt = BATcount(c), v_cnt = BATcount(v);
	size_t prev_bits = ~(size_t) 0;
	int radix_shift, radix_bits = *rb;
	int oidhead = (BUNsize(c) != sizeof(oid));
	ssize_t width = BUNsize(v), i = MAX(1, BATcount(v) - 1);
	ssize_t vectorsize = *vs;
	int stride = *st;
	oid *pos, radix_mask;
	BUN res, cur_prefetch = NULL, vf = BUNfirst(v);
	dbl skiprate;
	BAT *bn;

	if (BUNsize(v) != ATOMsize(v->ttype)) {
		GDKerror("posjoin: %s does not have a void head column.\n", BATgetId(v));
		return GDK_FAIL;
	}
	bn = *r = BATnew(TYPE_void, v->ttype, c_cnt);
	if (bn == NULL)
		return GDK_FAIL;
	pos = (oid *) BUNtloc(c, BUNfirst(c));
	res = (BUN) BUNtloc(bn, BUNfirst(bn));

	for (radix_shift = -radix_bits; i > 0; radix_shift++) {
		i >>= 1;
	}
	/* radix_shift is #bits in i minus radix_bits */
	radix_mask = ((1 << radix_bits) - 1) << radix_shift;
	bn->batBuns->free = c_cnt * BUNsize(bn);
	BATsetcount(bn, c_cnt);

	/* compute skiprate and if set, prefetch full first cluster */
	skiprate = ((dbl) v_cnt) / c_cnt;
	if (stride < 0) {
		cur_prefetch = (char *) vf + width * MIN(v_cnt, (ssize_t) 1 << radix_shift);
		prefetch(cur_prefetch - vf, 128, (dbl *) vf);
	}

	for (i = 0; i < c_cnt; i += vectorsize) {
		ssize_t probe = MIN(i + vectorsize, c_cnt) - 1;
		size_t cur_bits = pos[probe << oidhead] & radix_mask;

		n = MIN(c_cnt - i, vectorsize);

		/* handle prefetching of the next cluster */
		if (cur_prefetch) {
			/* increment concurrent prefetch pointer */
			size_t prefetch_pos = (size_t) (i * skiprate) + ((size_t) 1 << radix_shift);

			cur_prefetch = vf + width * prefetch_pos;
			if (cur_prefetch + n * width * ((int) skiprate) > BUNlast(v))
				cur_prefetch = NULL;	/* prefetch would cause SEGV */
		} else if (stride >= 0) {
			/* handle sequential prefetching if applicable */
			if (prev_bits != cur_bits && radix_shift > 3) {
				size_t length = width * MIN(v_cnt - cur_bits, (size_t) 1 << radix_shift);

				(void) prefetch(length, stride, (dbl *) (vf + cur_bits * width));
				if (skiprate * width > 128) {
					(void) prefetch(length, stride, (dbl *) (vf + cur_bits * width));
				}
			}
		}
		/* do the join for this vector */
		switch (ATOMstorage(v->ttype)) {
		case TYPE_chr:
		case TYPE_bte:
			(void) posjoin_bte((int) skiprate, (bte *) cur_prefetch, oidhead, n, (bte *) res, pos + (i << oidhead), (bte *) vf);
			break;
		case TYPE_sht:
			(void) posjoin_sht((int) skiprate, (sht *) cur_prefetch, oidhead, n, (sht *) res, pos + (i << oidhead), (sht *) vf);
			break;
		case TYPE_int:
		case TYPE_flt:
			(void) posjoin_int((int) skiprate, (int *) cur_prefetch, oidhead, n, (int *) res, pos + (i << oidhead), (int *) vf);
			break;
		case TYPE_lng:
		case TYPE_dbl:
			(void) posjoin_dbl((int) skiprate, (dbl *) cur_prefetch, oidhead, n, (dbl *) res, pos + (i << oidhead), (dbl *) vf);
			break;
		default:
			GDKerror("posjoin: type %s not implemented\n", ATOMname(v->ttype));
			bn->batBuns->free -= c_cnt * BUNsize(bn);
			bn->batCount -= c_cnt;
			break;
		}
		prev_bits = cur_bits;
		res += vectorsize * BUNsize(bn);
	}
	BATseqbase(bn, c->hseqbase);
	bn->tsorted = 0;
	return GDK_SUCCEED;
}

int
M4_RDX_posjoin_clustered_v(BAT **r, BAT *c, BAT *v, int *rb, int *st)
{
	int vs = 512;
	return M4_RDX_posjoin_clustered(r, c, v, rb, st, &vs);
}

int
M4_RDX_posjoin_clustered_sv(BAT **r, BAT *c, BAT *v, int *rb)
{
	int st = 128;
	int vs = 512;
	return M4_RDX_posjoin_clustered(r, c, v, rb, &st, &vs);
}


static int 
log_2(size_t i)
{
	int n = 0;

	while (i > 1)	{
		i >>= 1;
		n += 1;
	}
	return n;
}

int
M4_RDX_cache_join(BAT **ret, BAT *left, BAT *right, int *_cache_size, int *_cache_line_size)
{
	int cache_size = *_cache_size, cache_line_size = *_cache_line_size, cache_lines;
	size_t right_batsize = BATcount(right) * BUNsize(right);
	size_t estimate = MIN(BATcount(left), BATcount(right));

	if (cache_size < 1) {
		GDKerror("cache_join: cache_size must be > 0");
		return GDK_FAIL;
	}
	if (cache_line_size < 1) {
		GDKerror("cache_join: cache_line_size must be > 0");
		return GDK_FAIL;
	}
	
	cache_lines = cache_size / cache_line_size;

	if (!(BATtordered(left)&1) && right_batsize > (size_t)cache_size) {
		int multiplier = 8;
		int nbits = 1 + log_2(right_batsize / cache_size);
		int nignore = MAX(0, (1 + log_2(BATcount(right))) - nbits);
		int npasses = 1 + (nbits + 1) / log_2((size_t) cache_lines);

		BAT *cluster=NULL, *borders=NULL, *cluster_values=NULL, *cluster_ids=NULL;
		if (M4_RDX_radix_cluster2(&cluster, left, &npasses, &nbits, &nignore) != GDK_SUCCEED) {
			GDKerror("cache_join: radix_cluster2() failed");
@= cache_join_end
			if (cluster)		BBPreclaim(cluster);
			if (borders)		BBPreclaim(borders);
			if (cluster_values)	BBPreclaim(cluster_values);
			if (cluster_ids)	BBPreclaim(cluster_ids);
			return @1;
@
@c
			@:cache_join_end(GDK_FAIL)@
		}
		if (M4_RDX_radix_count(&borders, cluster, &nignore, &nbits) != GDK_SUCCEED) {
			GDKerror("cache_join: radix_count() failed");
			@:cache_join_end(GDK_FAIL)@
		}
		if ((cluster_values = BATmirror(BATmark(BATmirror(cluster),(oid)0))) == NULL) {
			GDKerror("cache_join: BATmirror(BATmark(BATmirror())) failed");
			@:cache_join_end(GDK_FAIL)@
		}
		if ((cluster_ids = BATmirror(BATmark(cluster,(oid)0))) == NULL) {
			GDKerror("cache_join: BATmirror(BATmark()) failed");
			@:cache_join_end(GDK_FAIL)@
		}
		BBPreclaim(cluster); cluster=NULL;
		if ((cluster = BATjoin(cluster_values, right, estimate)) == NULL) {
			GDKerror("cache_join: BATjoin() failed");
			@:cache_join_end(GDK_FAIL)@
		}
		BBPreclaim(cluster_values); cluster_values=NULL;
		if (M4_RDX_radix_decluster2(ret, cluster_ids, cluster, borders, &multiplier) != GDK_SUCCEED) {
			GDKerror("cache_join: radix_decluster2() failed");
			@:cache_join_end(GDK_FAIL)@
		}
		@:cache_join_end(GDK_SUCCEED)@
	}
	if ((*ret = BATjoin(left, right, estimate)) == NULL) {
		GDKerror("cache_join: BATjoin() failed");
		return GDK_FAIL;
	}
	return GDK_SUCCEED;
}

int
M4_RDX_cache_join_256k_32(BAT **ret, BAT *left, BAT *right)
{
	int cache_size = 256*1024;
	int cache_line_size = 32;
	return M4_RDX_cache_join(ret, left, right, &cache_size, &cache_line_size);
}


@h
/*
 * @+ M5 header file & M5/MAL Wrappers
 *
 * In M4, this was generated by MEL, in M5 we have to do it by hand.
 */
#ifndef _M5_RDX_H
#define _M5_RDX_H

#ifdef WIN32
#if !defined(LIBMAL) && !defined(LIBATOMS) && !defined(LIBKERNEL) && !defined(LIBMAL) && !defined(LIBOPTIMIZER) && !defined(LIBSCHEDULER) && !defined(LIBMONETDB5)
#define radix_export extern __declspec(dllimport)
#else
#define radix_export extern __declspec(dllexport)
#endif
#else
#define radix_export extern
#endif

@= M5_RDX_init
	int rtrn = GDK_FAIL;
	@1 M4res;
@
@= M5_RDX_get_BATdesc
	if ((M4@2 = BATdescriptor(*M5@2)) == NULL) {
		@3
		throw(MAL, "radix.@1", RUNTIME_OBJECT_MISSING);
	}
@
@= M5_RDX_get_BATdesc_1
	BAT *M4@2;
	@:M5_RDX_get_BATdesc(@1,@2,)@
@
@= M5_RDX_get_BATdesc_2
	BAT *M4@2, *M4@3;
	@:M5_RDX_get_BATdesc(@1,@2,)@
	@:M5_RDX_get_BATdesc(@1,@3,BBPreleaseref(*M5@2);)@
@
@= M5_RDX_get_BATdesc_3
	BAT *M4@2, *M4@3, *M4@4;
	@:M5_RDX_get_BATdesc(@1,@2,)@
	@:M5_RDX_get_BATdesc(@1,@3,BBPreleaseref(*M5@2);)@
	@:M5_RDX_get_BATdesc(@1,@4,BBPreleaseref(*M5@2); BBPreleaseref(*M5@3);)@
@
@= M5_RDX_get_BATdesc_4
	BAT *M4@2, *M4@3, *M4@4, *M4@5;
	@:M5_RDX_get_BATdesc(@1,@2,)@
	@:M5_RDX_get_BATdesc(@1,@3,BBPreleaseref(*M5@2);)@
	@:M5_RDX_get_BATdesc(@1,@4,BBPreleaseref(*M5@2); BBPreleaseref(*M5@3);)@
	@:M5_RDX_get_BATdesc(@1,@5,BBPreleaseref(*M5@2); BBPreleaseref(*M5@3); BBPreleaseref(*M5@4);)@
@
@= M5_RDX_finish
	if (rtrn != GDK_SUCCEED) {
		throw(MAL, "radix.@1", OPERATION_FAILED " M4_RDX_@1 failed");
	}
	*M5res = M4res;
	return MAL_SUCCEED;
@
@= M5_RDX_finish_BAT
	if (rtrn != GDK_SUCCEED) {
		throw(MAL, "radix.@1", OPERATION_FAILED " M4_RDX_@1 failed");
	}
	if (!(M4res->batDirty&2)) {
		/*M4res->batRestricted = BAT_READ;*/
		M4res = BATsetaccess(M4res, BAT_READ);
	}
	*M5res = M4res->batCacheid;
	BBPkeepref(*M5res);
	return MAL_SUCCEED;
@
@h
radix_export str 
M5_RDX_BATuniform(int *M5res, oid *base, int *size, int *domain);
@c
str
M5_RDX_BATuniform(int *M5res, oid *base, int *size, int *domain)
{
	@:M5_RDX_init(BAT*)@
	rtrn = M4_RDX_BATuniform( &M4res, base, size, domain);
	@:M5_RDX_finish_BAT(BATuniform)@
}
@h
radix_export str 
M5_RDX_BATuniform_b(int *M5res, int *size, int *domain);
@c
str
M5_RDX_BATuniform_b(int *M5res, int *size, int *domain)
{
	@:M5_RDX_init(BAT*)@
	rtrn = M4_RDX_BATuniform_b( &M4res, size, domain);
	@:M5_RDX_finish_BAT(BATuniform_b)@
}
@h
radix_export str 
M5_RDX_BATuniform_bd(int *M5res, int *size);
@c
str
M5_RDX_BATuniform_bd(int *M5res, int *size)
{
	@:M5_RDX_init(BAT*)@
	rtrn = M4_RDX_BATuniform_bd( &M4res, size);
	@:M5_RDX_finish_BAT(BATuniform_bd)@
}
@h
radix_export str 
M5_RDX_BATnormal(int *M5res, oid *base, int *size, int *domain, int *stddev, int *mean);
@c
str
M5_RDX_BATnormal(int *M5res, oid *base, int *size, int *domain, int *stddev, int *mean)
{
	@:M5_RDX_init(BAT*)@
	rtrn = M4_RDX_BATnormal( &M4res, base, size, domain, stddev, mean);
	@:M5_RDX_finish_BAT(BATnormal)@
}
@h
radix_export str 
M5_RDX_BATnormal_b(int *M5res, int *size, int *domain, int *stddev, int *mean);
@c
str
M5_RDX_BATnormal_b(int *M5res, int *size, int *domain, int *stddev, int *mean)
{
	@:M5_RDX_init(BAT*)@
	rtrn = M4_RDX_BATnormal_b( &M4res, size, domain, stddev, mean);
	@:M5_RDX_finish_BAT(BATnormal_b)@
}
@h
radix_export str 
M5_RDX_BATnormal_m(int *M5res, oid *base, int *size, int *domain, int *stddev);
@c
str
M5_RDX_BATnormal_m(int *M5res, oid *base, int *size, int *domain, int *stddev)
{
	@:M5_RDX_init(BAT*)@
	rtrn = M4_RDX_BATnormal_m( &M4res, base, size, domain, stddev);
	@:M5_RDX_finish_BAT(BATnormal_m)@
}
@h
radix_export str 
M5_RDX_BATnormal_bm(int *M5res, int *size, int *domain, int *stddev);
@c
str
M5_RDX_BATnormal_bm(int *M5res, int *size, int *domain, int *stddev)
{
	@:M5_RDX_init(BAT*)@
	rtrn = M4_RDX_BATnormal_bm( &M4res, size, domain, stddev);
	@:M5_RDX_finish_BAT(BATnormal_bm)@
}
@h
radix_export str 
M5_RDX_BATnormal_vm(int *M5res, oid *base, int *size, int *domain);
@c
str
M5_RDX_BATnormal_vm(int *M5res, oid *base, int *size, int *domain)
{
	@:M5_RDX_init(BAT*)@
	rtrn = M4_RDX_BATnormal_vm( &M4res, base, size, domain);
	@:M5_RDX_finish_BAT(BATnormal_vm)@
}
@h
radix_export str 
M5_RDX_BATnormal_bvm(int *M5res, int *size, int *domain);
@c
str
M5_RDX_BATnormal_bvm(int *M5res, int *size, int *domain)
{
	@:M5_RDX_init(BAT*)@
	rtrn = M4_RDX_BATnormal_bvm( &M4res, size, domain);
	@:M5_RDX_finish_BAT(BATnormal_bvm)@
}
@h
radix_export str 
M5_RDX_BATnormal_dvm(int *M5res, oid *base, int *size);
@c
str
M5_RDX_BATnormal_dvm(int *M5res, oid *base, int *size)
{
	@:M5_RDX_init(BAT*)@
	rtrn = M4_RDX_BATnormal_dvm( &M4res, base, size);
	@:M5_RDX_finish_BAT(BATnormal_dvm)@
}
@h
radix_export str 
M5_RDX_BATnormal_bdvm(int *M5res, int *size);
@c
str
M5_RDX_BATnormal_bdvm(int *M5res, int *size)
{
	@:M5_RDX_init(BAT*)@
	rtrn = M4_RDX_BATnormal_bdvm( &M4res, size);
	@:M5_RDX_finish_BAT(BATnormal_bdvm)@
}
@h
radix_export str 
M5_RDX_radix_cluster(Client cntxt, MalBlkPtr mb, MalStkPtr stk, InstrPtr pci);
@c
str
M5_RDX_radix_cluster(Client cntxt, MalBlkPtr mb, MalStkPtr stk, InstrPtr pci)
/* (int *M5res, int *M5b, str nme, flt *perc, int *nbits, ... ) */
{
#define M5_RDX_radix_cluster_MAX_PASSES 9
	int j = 0;
	int *M5res = (int*)getArgReference(stk, pci, j++);
	int *M5b   = (int*)getArgReference(stk, pci, j++);
	str nme    = (str )getArgReference(stk, pci, j++);
	flt *perc  = (flt*)getArgReference(stk, pci, j++);
	int nbits[M5_RDX_radix_cluster_MAX_PASSES];
	int passes = pci->argc - j;
	int i;
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_1(radix_cluster,b)@
	(void) cntxt;
	assert(passes > 0);
	assert(passes <= M5_RDX_radix_cluster_MAX_PASSES);
	(void) mb;
	for (i = 0; i < passes; i++) {
		nbits[i] = *(int*)getArgReference(stk, pci, j+i);
	}
	switch(passes) {
	case 1: rtrn = M4_RDX_radix_cluster( &M4res, M4b, nme, perc, nbits, NULL ); break;
	case 2: rtrn = M4_RDX_radix_cluster( &M4res, M4b, nme, perc, nbits, nbits+1, NULL ); break;
	case 3: rtrn = M4_RDX_radix_cluster( &M4res, M4b, nme, perc, nbits, nbits+1, nbits+2, NULL ); break;
	case 4: rtrn = M4_RDX_radix_cluster( &M4res, M4b, nme, perc, nbits, nbits+1, nbits+2, nbits+3, NULL ); break;
	case 5: rtrn = M4_RDX_radix_cluster( &M4res, M4b, nme, perc, nbits, nbits+1, nbits+2, nbits+3, nbits+4, NULL ); break;
	case 6: rtrn = M4_RDX_radix_cluster( &M4res, M4b, nme, perc, nbits, nbits+1, nbits+2, nbits+3, nbits+4, nbits+5, NULL ); break;
	case 7: rtrn = M4_RDX_radix_cluster( &M4res, M4b, nme, perc, nbits, nbits+1, nbits+2, nbits+3, nbits+4, nbits+5, nbits+6, NULL ); break;
	case 8: rtrn = M4_RDX_radix_cluster( &M4res, M4b, nme, perc, nbits, nbits+1, nbits+2, nbits+3, nbits+4, nbits+5, nbits+6, nbits+7, NULL ); break;
	case 9: rtrn = M4_RDX_radix_cluster( &M4res, M4b, nme, perc, nbits, nbits+1, nbits+2, nbits+3, nbits+4, nbits+5, nbits+6, nbits+7, nbits+8, NULL ); break;
	}
	@:M5_RDX_finish_BAT(radix_cluster)@
}
@h
radix_export str 
M5_RDX_radix_cluster_l(Client cntxt, MalBlkPtr mb, MalStkPtr stk, InstrPtr pci);
@c
str
M5_RDX_radix_cluster_l(Client cntxt, MalBlkPtr mb, MalStkPtr stk, InstrPtr pci)
/* (int *M5res, int *M5b, flt *perc, int *nbits, ... ) */
{
	int j = 0;
	int *M5res = (int*)getArgReference(stk, pci, j++);
	int *M5b   = (int*)getArgReference(stk, pci, j++);
	flt *perc  = (flt*)getArgReference(stk, pci, j++);
	int nbits[M5_RDX_radix_cluster_MAX_PASSES];
	int passes = pci->argc - j;
	int i;
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_1(radix_cluster_l,b)@
	(void) cntxt;
	assert(passes > 0);
	assert(passes <= M5_RDX_radix_cluster_MAX_PASSES);
	(void) mb;
	for (i = 0; i < passes; i++) {
		nbits[i] = *(int*)getArgReference(stk, pci, j+i);
	}
	switch(passes) {
	case 1: rtrn = M4_RDX_radix_cluster_l( &M4res, M4b, perc, nbits, NULL ); break;
	case 2: rtrn = M4_RDX_radix_cluster_l( &M4res, M4b, perc, nbits, nbits+1, NULL ); break;
	case 3: rtrn = M4_RDX_radix_cluster_l( &M4res, M4b, perc, nbits, nbits+1, nbits+2, NULL ); break;
	case 4: rtrn = M4_RDX_radix_cluster_l( &M4res, M4b, perc, nbits, nbits+1, nbits+2, nbits+3, NULL ); break;
	case 5: rtrn = M4_RDX_radix_cluster_l( &M4res, M4b, perc, nbits, nbits+1, nbits+2, nbits+3, nbits+4, NULL ); break;
	case 6: rtrn = M4_RDX_radix_cluster_l( &M4res, M4b, perc, nbits, nbits+1, nbits+2, nbits+3, nbits+4, nbits+5, NULL ); break;
	case 7: rtrn = M4_RDX_radix_cluster_l( &M4res, M4b, perc, nbits, nbits+1, nbits+2, nbits+3, nbits+4, nbits+5, nbits+6, NULL ); break;
	case 8: rtrn = M4_RDX_radix_cluster_l( &M4res, M4b, perc, nbits, nbits+1, nbits+2, nbits+3, nbits+4, nbits+5, nbits+6, nbits+7, NULL ); break;
	case 9: rtrn = M4_RDX_radix_cluster_l( &M4res, M4b, perc, nbits, nbits+1, nbits+2, nbits+3, nbits+4, nbits+5, nbits+6, nbits+7, nbits+8, NULL ); break;
	}
	@:M5_RDX_finish_BAT(radix_cluster_l)@
}
@h
radix_export str 
M5_RDX_radix_cluster_p(Client cntxt, MalBlkPtr mb, MalStkPtr stk, InstrPtr pci);
@c
str
M5_RDX_radix_cluster_p(Client cntxt, MalBlkPtr mb, MalStkPtr stk, InstrPtr pci)
/* (int *M5res, int *M5b, str nme, int *nbits, ... ) */
{
	int j = 0;
	int *M5res = (int*)getArgReference(stk, pci, j++);
	int *M5b   = (int*)getArgReference(stk, pci, j++);
	str nme    = (str )getArgReference(stk, pci, j++);
	int nbits[M5_RDX_radix_cluster_MAX_PASSES];
	int passes = pci->argc - j;
	int i;
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_1(radix_cluster_p,b)@
	(void) cntxt;
	assert(passes > 0);
	assert(passes <= M5_RDX_radix_cluster_MAX_PASSES);
	(void) mb;
	for (i = 0; i < passes; i++) {
		nbits[i] = *(int*)getArgReference(stk, pci, j+i);
	}
	switch(passes) {
	case 1: rtrn = M4_RDX_radix_cluster_p( &M4res, M4b, nme, nbits, NULL ); break;
	case 2: rtrn = M4_RDX_radix_cluster_p( &M4res, M4b, nme, nbits, nbits+1, NULL ); break;
	case 3: rtrn = M4_RDX_radix_cluster_p( &M4res, M4b, nme, nbits, nbits+1, nbits+2, NULL ); break;
	case 4: rtrn = M4_RDX_radix_cluster_p( &M4res, M4b, nme, nbits, nbits+1, nbits+2, nbits+3, NULL ); break;
	case 5: rtrn = M4_RDX_radix_cluster_p( &M4res, M4b, nme, nbits, nbits+1, nbits+2, nbits+3, nbits+4, NULL ); break;
	case 6: rtrn = M4_RDX_radix_cluster_p( &M4res, M4b, nme, nbits, nbits+1, nbits+2, nbits+3, nbits+4, nbits+5, NULL ); break;
	case 7: rtrn = M4_RDX_radix_cluster_p( &M4res, M4b, nme, nbits, nbits+1, nbits+2, nbits+3, nbits+4, nbits+5, nbits+6, NULL ); break;
	case 8: rtrn = M4_RDX_radix_cluster_p( &M4res, M4b, nme, nbits, nbits+1, nbits+2, nbits+3, nbits+4, nbits+5, nbits+6, nbits+7, NULL ); break;
	case 9: rtrn = M4_RDX_radix_cluster_p( &M4res, M4b, nme, nbits, nbits+1, nbits+2, nbits+3, nbits+4, nbits+5, nbits+6, nbits+7, nbits+8, NULL ); break;
	}
	@:M5_RDX_finish_BAT(radix_cluster_p)@
}
@h
radix_export str 
M5_RDX_radix_cluster_lp(Client cntxt, MalBlkPtr mb, MalStkPtr stk, InstrPtr pci);
@c
str
M5_RDX_radix_cluster_lp(Client cntxt, MalBlkPtr mb, MalStkPtr stk, InstrPtr pci)
/* (int *M5res, int *M5b, int *nbits, ... ) */
{
	int j = 0;
	int *M5res = (int*)getArgReference(stk, pci, j++);
	int *M5b   = (int*)getArgReference(stk, pci, j++);
	int nbits[M5_RDX_radix_cluster_MAX_PASSES];
	int passes = pci->argc - j;
	int i;

	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_1(radix_cluster_lp,b)@

	(void) cntxt;
	assert(passes > 0);
	assert(passes <= M5_RDX_radix_cluster_MAX_PASSES);
	(void) mb;
	for (i = 0; i < passes; i++) {
		nbits[i] = *(int*)getArgReference(stk, pci, j+i);
	}
	switch(passes) {
	case 1: rtrn = M4_RDX_radix_cluster_lp( &M4res, M4b, nbits, NULL ); break;
	case 2: rtrn = M4_RDX_radix_cluster_lp( &M4res, M4b, nbits, nbits+1, NULL ); break;
	case 3: rtrn = M4_RDX_radix_cluster_lp( &M4res, M4b, nbits, nbits+1, nbits+2, NULL ); break;
	case 4: rtrn = M4_RDX_radix_cluster_lp( &M4res, M4b, nbits, nbits+1, nbits+2, nbits+3, NULL ); break;
	case 5: rtrn = M4_RDX_radix_cluster_lp( &M4res, M4b, nbits, nbits+1, nbits+2, nbits+3, nbits+4, NULL ); break;
	case 6: rtrn = M4_RDX_radix_cluster_lp( &M4res, M4b, nbits, nbits+1, nbits+2, nbits+3, nbits+4, nbits+5, NULL ); break;
	case 7: rtrn = M4_RDX_radix_cluster_lp( &M4res, M4b, nbits, nbits+1, nbits+2, nbits+3, nbits+4, nbits+5, nbits+6, NULL ); break;
	case 8: rtrn = M4_RDX_radix_cluster_lp( &M4res, M4b, nbits, nbits+1, nbits+2, nbits+3, nbits+4, nbits+5, nbits+6, nbits+7, NULL ); break;
	case 9: rtrn = M4_RDX_radix_cluster_lp( &M4res, M4b, nbits, nbits+1, nbits+2, nbits+3, nbits+4, nbits+5, nbits+6, nbits+7, nbits+8, NULL ); break;
	}
	@:M5_RDX_finish_BAT(radix_cluster_lp)@
}
@h
radix_export str 
M5_RDX_radix_cluster2(int *M5res, int *M5b, int *p, int *r, int *i);
@c
str
M5_RDX_radix_cluster2(int *M5res, int *M5b, int *p, int *r, int *i)
{
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_1(radix_cluster2,b)@
	rtrn = M4_RDX_radix_cluster2( &M4res, M4b, p, r, i);
	@:M5_RDX_finish_BAT(radix_cluster2)@
}
@h
radix_export str 
M5_RDX_radix_bits(int *M5res, int *M5b);
@c
str
M5_RDX_radix_bits(int *M5res, int *M5b)
{
	@:M5_RDX_init(int)@
	@:M5_RDX_get_BATdesc_1(radix_bits,b)@
	rtrn = M4_RDX_radix_bits( &M4res, M4b);
	@:M5_RDX_finish(radix_bits)@
}
@h
radix_export str 
M5_RDX_radix_count(int *M5res, int *M5b, int *shift, int *radix);
@c
str
M5_RDX_radix_count(int *M5res, int *M5b, int *shift, int *radix)
{
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_1(radix_count,b)@
	rtrn = M4_RDX_radix_count( &M4res, M4b, shift, radix);
	@:M5_RDX_finish_BAT(radix_count)@
}
@h
radix_export str 
M5_RDX_radix_count2(int *M5res, int *M5b, int *shift, int *radix);
@c
str
M5_RDX_radix_count2(int *M5res, int *M5b, int *shift, int *radix)
{
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_1(radix_count2,b)@
	rtrn = M4_RDX_radix_count2( &M4res, M4b, shift, radix);
	@:M5_RDX_finish_BAT(radix_count2)@
}
@h
radix_export str 
M5_RDX_phash_join(int *M5res, int *M5l, int *M5r, int *radix, int *hitrate, bit *cutoff);
@c
str
M5_RDX_phash_join(int *M5res, int *M5l, int *M5r, int *radix, int *hitrate, bit *cutoff)
{
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_2(phash_join,l,r)@
	rtrn = M4_RDX_phash_join( &M4res, M4l, M4r, radix, hitrate, cutoff);
	@:M5_RDX_finish_BAT(phash_join)@
}
@h
radix_export str 
M5_RDX_phash_join_c(int *M5res, int *M5l, int *M5r, int *radix, int *hitrate);
@c
str
M5_RDX_phash_join_c(int *M5res, int *M5l, int *M5r, int *radix, int *hitrate)
{
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_2(phash_join_c,l,r)@
	rtrn = M4_RDX_phash_join_c( &M4res, M4l, M4r, radix, hitrate);
	@:M5_RDX_finish_BAT(phash_join_c)@
}
@h
radix_export str 
M5_RDX_phash_join_h(int *M5res, int *M5l, int *M5r, int *radix, bit *cutoff);
@c
str
M5_RDX_phash_join_h(int *M5res, int *M5l, int *M5r, int *radix, bit *cutoff)
{
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_2(phash_join_h,l,r)@
	rtrn = M4_RDX_phash_join_h( &M4res, M4l, M4r, radix, cutoff);
	@:M5_RDX_finish_BAT(phash_join_h)@
}
@h
radix_export str 
M5_RDX_phash_join_hc(int *M5res, int *M5l, int *M5r, int *radix);
@c
str
M5_RDX_phash_join_hc(int *M5res, int *M5l, int *M5r, int *radix)
{
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_2(phash_join_hc,l,r)@
	rtrn = M4_RDX_phash_join_hc( &M4res, M4l, M4r, radix);
	@:M5_RDX_finish_BAT(phash_join_hc)@
}
@h
radix_export str 
M5_RDX_phash_join_xh(int *M5res, int *M5l, int *M5r, bit *cutoff);
@c
str
M5_RDX_phash_join_xh(int *M5res, int *M5l, int *M5r, bit *cutoff)
{
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_2(phash_join_xh,l,r)@
	rtrn = M4_RDX_phash_join_xh( &M4res, M4l, M4r, cutoff);
	@:M5_RDX_finish_BAT(phash_join_xh)@
}
@h
radix_export str 
M5_RDX_phash_join_xhc(int *M5res, int *M5l, int *M5r);
@c
str
M5_RDX_phash_join_xhc(int *M5res, int *M5l, int *M5r)
{
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_2(phash_join_xhc,l,r)@
	rtrn = M4_RDX_phash_join_xhc( &M4res, M4l, M4r);
	@:M5_RDX_finish_BAT(phash_join_xhc)@
}
@h
radix_export str 
M5_RDX_radix_join(int *M5res, int *M5l, int *M5r, int *radix, int *hitrate);
@c
str
M5_RDX_radix_join(int *M5res, int *M5l, int *M5r, int *radix, int *hitrate)
{
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_2(radix_join,l,r)@
	rtrn = M4_RDX_radix_join( &M4res, M4l, M4r, radix, hitrate);
	@:M5_RDX_finish_BAT(radix_join)@
}
@h
radix_export str 
M5_RDX_radix_join_h(int *M5res, int *M5l, int *M5r, int *radix);
@c
str
M5_RDX_radix_join_h(int *M5res, int *M5l, int *M5r, int *radix)
{
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_2(radix_join_h,l,r)@
	rtrn = M4_RDX_radix_join_h( &M4res, M4l, M4r, radix);
	@:M5_RDX_finish_BAT(radix_join_h)@
}
@h
radix_export str 
M5_RDX_radix_join_xh(int *M5res, int *M5l, int *M5r);
@c
str
M5_RDX_radix_join_xh(int *M5res, int *M5l, int *M5r)
{
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_2(radix_join_xh,l,r)@
	rtrn = M4_RDX_radix_join_xh( &M4res, M4l, M4r);
	@:M5_RDX_finish_BAT(radix_join_xh)@
}
@h
radix_export str 
M5_RDX_radix_decluster(int *M5res, int *M5b, int *M5cnts, int *multiplier);
@c
str
M5_RDX_radix_decluster(int *M5res, int *M5b, int *M5cnts, int *multiplier)
{
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_2(radix_decluster,b,cnts)@
	rtrn = M4_RDX_radix_decluster( &M4res, M4b, M4cnts, multiplier);
	@:M5_RDX_finish_BAT(radix_decluster)@
}
@h
radix_export str 
M5_RDX_radix_decluster_m(int *M5res, int *M5b, int *M5cnts);
@c
str
M5_RDX_radix_decluster_m(int *M5res, int *M5b, int *M5cnts)
{
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_2(radix_decluster_m,b,cnts)@
	rtrn = M4_RDX_radix_decluster_m( &M4res, M4b, M4cnts);
	@:M5_RDX_finish_BAT(radix_decluster_m)@
}
@h
radix_export str 
M5_RDX_radix_decluster2(int *M5res, int *M5b, int *M5a, int *M5cnts, int *multiplier);
@c
str
M5_RDX_radix_decluster2(int *M5res, int *M5b, int *M5a, int *M5cnts, int *multiplier)
{
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_3(radix_decluster2,b,a,cnts)@
	rtrn = M4_RDX_radix_decluster2( &M4res, M4b, M4a, M4cnts, multiplier);
	@:M5_RDX_finish_BAT(radix_decluster2)@
}
@h
radix_export str 
M5_RDX_radix_decluster3(int *M5res, int *M5b, int *M5a, int *M5cnts, int *multiplier);
@c
str
M5_RDX_radix_decluster3(int *M5res, int *M5b, int *M5a, int *M5cnts, int *multiplier)
{
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_3(radix_decluster3,b,a,cnts)@
	rtrn = M4_RDX_radix_decluster3( &M4res, M4b, M4a, M4cnts, multiplier);
	@:M5_RDX_finish_BAT(radix_decluster3)@
}
@h
radix_export str 
M5_RDX_radix_decluster4(int *M5res, int *M5b, int *M5a, int *M5cnts, int *multiplier);
@c
str
M5_RDX_radix_decluster4(int *M5res, int *M5b, int *M5a, int *M5cnts, int *multiplier)
{
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_3(radix_decluster4,b,a,cnts)@
	rtrn = M4_RDX_radix_decluster4( &M4res, M4b, M4a, M4cnts, multiplier);
	@:M5_RDX_finish_BAT(radix_decluster4)@
}

@(
 @h
radix_export str 
M5_RDX_posjoin_tuple(int *M5res, int *M5proj, int *M5attr);
 @c
str
M5_RDX_posjoin_tuple(int *M5res, int *M5proj, int *M5attr)
{
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_2(posjoin_tuple,proj,attr)@
	rtrn = M4_RDX_posjoin_tuple( &M4res, M4proj, M4attr);
	@:M5_RDX_finish_BAT(posjoin_tuple)@
}
@)

@h
radix_export str 
M5_RDX_jivejoin0(int *M5res, int *M5proj, int *M5attr, int *M5radix_count, int *shift, int *nbits);
@c
str
M5_RDX_jivejoin0(int *M5res, int *M5proj, int *M5attr, int *M5radix_count, int *shift, int *nbits)
{
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_3(jivejoin0,proj,attr,radix_count)@
	rtrn = M4_RDX_jivejoin0( &M4res, M4proj, M4attr, M4radix_count, shift, nbits);
	@:M5_RDX_finish_BAT(jivejoin0)@
}
@h
radix_export str 
M5_RDX_jivejoin1(int *M5res, int *M5bo, int *M5proj, int *M5attr, int *M5radix_count, int *shift, int *nbits);
@c
str
M5_RDX_jivejoin1(int *M5res, int *M5bo, int *M5proj, int *M5attr, int *M5radix_count, int *shift, int *nbits)
{
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_4(jivejoin1,bo,proj,attr,radix_count)@
	rtrn = M4_RDX_jivejoin1( &M4res, M4bo, M4proj, M4attr, M4radix_count, shift, nbits);
	@:M5_RDX_finish_BAT(jivejoin1)@
}
@h
radix_export str 
M5_RDX_jivejoin2(int *M5res, int *M5proj, int *M5attr);
@c
str
M5_RDX_jivejoin2(int *M5res, int *M5proj, int *M5attr)
{
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_2(jivejoin2,proj,attr)@
	rtrn = M4_RDX_jivejoin2( &M4res, M4proj, M4attr);
	@:M5_RDX_finish_BAT(jivejoin2)@
}
@h
radix_export str 
M5_RDX_posjoin_clustered(int *M5res, int *M5c, int *M5v, int *rb, int *st, int *vs);
@c
str
M5_RDX_posjoin_clustered(int *M5res, int *M5c, int *M5v, int *rb, int *st, int *vs)
{
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_2(posjoin_clustered,c,v)@
	rtrn = M4_RDX_posjoin_clustered( &M4res, M4c, M4v, rb, st, vs);
	@:M5_RDX_finish_BAT(posjoin_clustered)@
}
@h
radix_export str 
M5_RDX_posjoin_clustered_v(int *M5res, int *M5c, int *M5v, int *rb, int *st);
@c
str
M5_RDX_posjoin_clustered_v(int *M5res, int *M5c, int *M5v, int *rb, int *st)
{
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_2(posjoin_clustered_v,c,v)@
	rtrn = M4_RDX_posjoin_clustered_v( &M4res, M4c, M4v, rb, st);
	@:M5_RDX_finish_BAT(posjoin_clustered_v)@
}
@h
radix_export str 
M5_RDX_posjoin_clustered_sv(int *M5res, int *M5c, int *M5v, int *rb);
@c
str
M5_RDX_posjoin_clustered_sv(int *M5res, int *M5c, int *M5v, int *rb)
{
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_2(posjoin_clustered_sv,c,v)@
	rtrn = M4_RDX_posjoin_clustered_sv( &M4res, M4c, M4v, rb);
	@:M5_RDX_finish_BAT(posjoin_clustered_sv)@
}
@h
radix_export str 
M5_RDX_cache_join(int *M5res, int *M5left, int *M5right, int *_cache_size, int *_cache_line_size);
@c
str
M5_RDX_cache_join(int *M5res, int *M5left, int *M5right, int *_cache_size, int *_cache_line_size)
{
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_2(cache_join,left,right)@
	rtrn = M4_RDX_cache_join( &M4res, M4left, M4right, _cache_size, _cache_line_size);
	@:M5_RDX_finish_BAT(cache_join)@
}
@h
radix_export str 
M5_RDX_cache_join_256k_32(int *M5res, int *M5left, int *M5right);
@c
str
M5_RDX_cache_join_256k_32(int *M5res, int *M5left, int *M5right)
{
	@:M5_RDX_init(BAT*)@
	@:M5_RDX_get_BATdesc_2(cache_join_256k_32,left,right)@
	rtrn = M4_RDX_cache_join_256k_32( &M4res, M4left, M4right);
	@:M5_RDX_finish_BAT(cache_join_256k_32)@
}

@h
#endif /* _M5_RDX_H */
