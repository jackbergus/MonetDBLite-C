@' pathfinder.mx
@'
@' Monet runtime support for the Pathfinder XQuery compiler
@'
@' Copyright Notice:
@' -----------------
@'
@' The contents of this file are subject to the Pathfinder Public License
@' Version 1.1 (the "License"); you may not use this file except in
@' compliance with the License.  You may obtain a copy of the License at
@' http://monetdb.cwi.nl/Legal/PathfinderLicense-1.1.html
@'
@' Software distributed under the License is distributed on an "AS IS"
@' basis, WITHOUT WARRANTY OF ANY KIND, either express or implied.  See
@' the License for the specific language governing rights and limitations
@' under the License.
@'
@' The Original Code is the Pathfinder system.
@'
@' The Initial Developer of the Original Code is the Database &
@' Information Systems Group at the University of Konstanz, Germany.
@' Portions created by the University of Konstanz are Copyright (C)
@' 2000-2005 University of Konstanz.  All Rights Reserved.
@'
@' $Id$
@'

@f pathfinder
@a Torsten Teggy Grust
@a Maurice van Keulen
@a Henning Rode
@a Jan Flokstra
@a Jens Teubner
@a Stefan Manegold
@a Peter Boncz 

@t Runtime Support for the Pathfinder XQuery Compiler

@m
.MODULE pathfinder;

.USE pf_support;
.USE streams;

.BUILTIN xquery_server(Stream in, Stream out) : void = CMDxquery_server;
 "read xquery from stream, and print on a stream."

.COMMAND xquery(str, str) : str = CMDxquery;
 "xquery execution.
 usage:  var result := xquery(\"xml\", str); printf(result);
 or:  printf(xquery(\"xml\", str));"

.PRELUDE = pathfinder_prelude;
.EPILOGUE = pathfinder_epilogue;

.END pathfinder;
@mil
module(mapi);

PROC pfstart(bit open) : void {
    mapi_listen(int(monet_environment.find("xquery_port")), 5, "xquery_server", open).fork();
}
ADDHELP("pfstart", "boncz", "April 2005",
"PARAMETERS:\n\
bit whether the listener should listen to outside requests \
(default false: only local connections).\n\
DESCRIPTION:\n\
start the xquery server.",
"pathfinder");

PROC pfstart() : void {
    pfstart(false);
}
ADDHELP("pfstart", "boncz", "April 2005",
"DESCRIPTION:\n\
start the xquery server.",
"pathfinder");

PROC xquery_server_start(bit global) : void {
    fork(mapi_listen(
                int(monet_environment.find("xquery_port")),
                5,
                "xquery_server",
                global
                )
        );
}

PROC xquery_server_start() : void {
    xquery_server_start(false);
}

PROC xquery(str query) : str {
    return xquery("xml", query);
}
ADDHELP("xquery", "boncz", "Juli 2005",
"shortcut for 'xquery(\"xml\", str query) : str'",
"pathfinder");

#############################################
# User-level document management functions:
#
# - shred_doc
# - delete_doc
# - delete_all_docs
#
# Each imported XML-document is stored as a collection of persistent BATs 
# whose name starts with the name of the BAT and ends with the document ID. 
# The execution of an XQuery is not performed on the persistent BATs, but on 
# the transient 'working set', a global document collection represented
# by the ws-BAT.
#
# Each collection contains the following BATs:
# - pre_size    : BAT[oid,int]  pre-value of node / and its descendant size,
# - pre_level   : BAT[oid,chr]  pre-value of node / and its level,
# - pre_prop    : BAT[oid,oid]  pre-value of node / and property ID,
# - pre_kind    : BAT[oid,chr]  pre-value of node / and node kind,
#
# - qn_ns        : BAT[oid,str]  property ID / and name space (elements and attributes),
# - qn_loc       : BAT[oid,str]  property ID / and local name (elements and attributes),
# - prop_text    : BAT[oid,str]  property ID / and text,
# - prop_com     : BAT[oid,str]  property ID / and comment,
# - prop_ins     : BAT[oid,str]  property ID / and processing instruction,
# - prop_tgt     : BAT[oid,str]  property ID / and processing instruction target,
#
# - attr_own  : BAT[oid,oid]  attribute ID / pre-value of owner
# - attr_qn   : BAT[oid,oid]  attribute ID / and qname/property ID
# - attr_prop : BAT[oid,oid]  attribute ID / value ID
#
# - prop_val  : BAT[oid,str]  value ID / value of attribute nodes
#
# Global information on persistent stored documents:
# - doc_name      : BAT[oid,str]        document ID / document name
# - doc_height    : BAT[oid,int]        document ID / height of document
# - doc_size      : BAT[oid,lng]        document ID / size in bytes
# - doc_timestamp : BAT[oid,timestamp]  document ID / end-of-cache-time (nil if none)
# - uri_lifetime  : BAT[str,lng]        URI prefix  / time-to-live (seconds), nil if not to be cached 
# 
# Information on documents/fragments in the working set:
# - PRE_FRAG   : BAT[void,BAT[void,oid]] list of bats with fragments
#                oids for each pre value (copied in element construction)
# - ATTR_FRAG  : BAT[void,BAT[void,oid]] list of bats with fragments
#                oids for each attr value (copied in element/attribute 
#                construction)
# - DOC_LOADED : BAT[void,str] fragment ID / name of document 
#                (starts with 1@0 because WS is first 'loaded' document
# - FRAG       : BAT[void,oid] 0@0 / oid of the newest added fragment
# - WS_FRAG    : BAT[void,oid] enumeration / root-pre values of
#                generated element fragments
# - HEIGTH     : BAT[void,int] enumeration of the fragments / maximal
#                depth of the fragment (starts with 0@0 - WS)

# get a handle to the global lock
var pf_lock := pflock();

# DEFINITION of constants concerning the data model
const PRE_BASE := 0@0;
const ELEMENT := chr(0);
const TEXT := chr(1);
const COMMENT := chr(2);
const PI := chr(3);
const DOCUMENT := chr(4);

# types in translation
const QNAME := 2;
const BOOL := 3;
const INT := 4;
const DEC := 5;
const DBL := 6;
const STR := 7;
const U_A := 8;
# ATOMIC is not a type but all atomic types
# can be retrieved with 'select(int(nil),ATOMIC)'
const ATOMIC := 31;
# NODE is not a type but all node types
# can be retrieved with 'select(NODE,int(nil))'
const NODE := 32;
const ELEM := 33;
const ATTR := 34;

const WS := 0@0;

@- ws definition

This macro is used for the MIL const defs, the C const defs,
but also in the MIL procs for creating, filling and destroying
a working set.

We define a table with the column name, its number,
the type of data (typically again a bat), and if
so the head and tail type of that bat.

actually the field 'tpe' is taken to be bat *always*, 
EXCEPT when (child-T == void)

('tpe' = void means it is a view; without persistent name)

        name       number  tpe  col[H,T]    col-seqbase 
        ========   ======  ===  ==========  ===========
@= ws
@:@1_ws(PRE_SIZE,       0, bat, void,  int, PRE_BASE)@
@:@1_ws(PRE_LEVEL,      1, bat, void,  chr, PRE_BASE)@
@:@1_ws(PRE_PROP,       2, bat, void,  oid, PRE_BASE)@
@:@1_ws(PRE_KIND,       3, bat, void,  chr, PRE_BASE)@
@:@1_ws(PRE_FRAG,       4, void,void,  oid, PRE_BASE)@
@:@1_ws(QN_URI,         5, bat, void,  str, PRE_BASE)@
@:@1_ws(QN_PREFIX,      6, bat, void,  str, PRE_BASE)@
@:@1_ws(QN_LOC,         7, bat, void,  str, PRE_BASE)@
@:@1_ws(PROP_TEXT,      8, bat, void,  str, PRE_BASE)@
@:@1_ws(PROP_COM,       9, bat, void,  str, PRE_BASE)@
@:@1_ws(PROP_INS,      10, bat, void,  str, PRE_BASE)@
@:@1_ws(PROP_TGT,      11, bat, void,  str, PRE_BASE)@
@:@1_ws(ID_PRE,        12, bat,  str,  oid, nil)@
@:@1_ws(IDREF_PRE,     13, bat,  str,  oid, nil)@
@:@1_ws(ATTR_OWN,      14, bat, void,  oid, PRE_BASE)@
@:@1_ws(ATTR_QN,       15, bat, void,  oid, PRE_BASE)@
@:@1_ws(ATTR_PROP,     16, bat, void,  oid, PRE_BASE)@
@:@1_ws(ATTR_FRAG,     17, void,void,  oid, PRE_BASE)@
@:@1_ws(PROP_VAL,      18, bat, void,  str, PRE_BASE)@
@:@1_ws(DOC_LOADED,    19, str, void, void, nil)@
@:@1_ws(FRAG,          20, oid, void, void, nil)@
@:@1_ws(WS_FRAG,       21, oid, void, void, nil)@
@:@1_ws(HEIGHT,        22, int, void, void, nil)@
@:@1_ws(QN_PREFIX_URI, 23, bat, void,  str, PRE_BASE)@
@:@1_ws(QN_LOC_URI,    24, bat, void,  str, PRE_BASE)@
@:@1_ws(KIND_PRE_0,    25, bat, void,  oid, nil)@
@:@1_ws(KIND_PRE_1,    26, bat, void,  oid, nil)@
@:@1_ws(KIND_PRE_2,    27, bat, void,  oid, nil)@
@:@1_ws(KIND_PRE_3,    28, bat, void,  oid, nil)@
@:@1_ws(KIND_PRE_4,    29, bat, void,  oid, nil)@
@:@1_ws(PROP_PRE_0,    30, bat,  oid,  oid, nil)@
@:@1_ws(PROP_PRE_3,    31, bat,  oid,  oid, nil)@

@= c_decl_ws
#define @1 @2
@h
@:ws(c_decl)@
#define WS_SIZE 32  /* should be last */

@= mil_decl_ws
const @1 := @2;
@mil
@:ws(mil_decl)@
const WS_SIZE  := 32; # WS_SIZE should be last
const KIND_PRE := KIND_PRE_0;
const PROP_PRE := PROP_PRE_0;

var doc_query; # bat[oid,int] active working-sets (queries) that use a particular document 
var doc_sema;  # bat[oid,sema] used by delete_doc to wait until all doc users have gone

PROC doc_init() : void
{
    # check if anythimg is initialized at all
    if (type(doc_query) != bat) {
        if (not(isnil(CATCH(bat("doc_name").count())))) {
            # create doc_name table in case it does not exist
            new(oid,str).persists(true).bbpname("doc_name");
            new(oid,str).persists(true).bbpname("doc_location");
            new(oid,timestamp).persists(true).bbpname("doc_timestamp");
            new(oid,int).persists(true).bbpname("doc_height");
            new(oid,lng).persists(true).bbpname("doc_size");
            new(str,lng).persists(true).bbpname("uri_lifetime");
        }
        if (isnil(CATCH(bat("doc_query").count()))) {
            doc_query := bat("doc_query"); 
            doc_sema := bat("doc_sema"); 
        } else {
            (doc_query := new(oid,int)).bbpname("doc_query"); 
            (doc_sema := new(oid,sema)).bbpname("doc_sema"); 
        }
    }
}

PROC doc_check(str name) : bit
{
    doc_init();
    return bat("doc_name").reverse().exist(name);
}

PROC create_ws () : BAT[void,bat]
{
    var ws := bat(void,bat,WS_SIZE).seqbase(PRE_BASE);

    # use our ws macro to instatiate a working set with the proper types
@= create_ws
    if (@5 != void) { # a bat of bats (persistent or view)
        var b := bat(@4,@5);
        if (not(isnil(@6))) b.seqbase(@6); 
        ws.insert(nil, bat(void,bat).seqbase(PRE_BASE).insert(nil, b));
    } else {
        ws.insert(nil, bat(void,@3).seqbase(PRE_BASE)); # a constant bat
    }
@mil
    @:ws(create)@

    # fill the constant bats with initial values
    ws.fetch(DOC_LOADED).seqbase(1@0);
    ws.fetch(FRAG).seqbase(0@0).insert(0@0,nil);
    ws.fetch(HEIGHT).seqbase(0@0).insert(nil,0);
    return ws.access(BAT_READ);
}

ADDHELP("create_ws", "tsheyar", "July 2004",
"DESCRIPITON:\n\
creates a new working set and gives back the reference",
"pathfinder");

##################################################
## PROCs required by the algebraic translation ###
##################################################

PROC doc_tbl (BAT[void, BAT] ws, BAT[void, str] item) : BAT[void,BAT]
{
    # consider each document only once
    var docs := item.tuniqueALT ().mark (0@0).reverse ();

    # throw out documents we already have in the working set
    docs := docs.reverse ()
                .kdiff (ws.fetch (DOC_LOADED).reverse ())
                .mark (0@0)
                .reverse ();

    # load all remaining documents into the working set
    docs@batloop () {
        ws := add_doc (ws, $t);
    }

    # The working set now contains all documents we need.

    # pick the according frag value for each document requested
    var ret_frag := item.leftjoin (ws.fetch (DOC_LOADED).reverse ())
                        .reverse ()
                        .mark (0@0)
                        .reverse ();

    # pre values will all be zero
    var ret_item := ret_frag.project (0@0);

    # return result as a BAT of BATs
    return new (void, BAT).insert (nil, ws)
                          .insert (nil, ret_item)
                          .insert (nil, ret_frag)
                          .seqbase (0@0);
}

ADDHELP("doc_tbl", "teubner", "Aug 2005",
		"PARAMETERS:\n\
ws    current working set; will be modified\n\
item  list of documents to add to the working set\n\
DESCRIPTION:\n\
Implementation of the algebra operator `doc_tbl' that\n\
loads persistent documents into the working set.\n\
Input is a list of document names. Output is a BAT of\n\
BATs with the components\n\
(a) the modified working set,\n\
(b) an `item' column with the pre values of the\n\
document roots, and\n\
(c) a `frag' column that encodes document\n\
container (within the working set) according to our\n\
working set representation.",
		"pathfinder");

# primitive for supporting highly specific XQuery functionality
PROC merge_adjacent_text_nodes (BAT[void,oid] iter,
                                BAT[void,oid] pre,
                                BAT[void,oid] pfrag,
                                BAT[void,BAT] ws) : BAT[void,oid]
{
    var map := pre.ord_uselect(nil,nil).mark(0@0).reverse();
    iter := map.leftfetchjoin(iter);
    pre := map.leftfetchjoin(pre);
    pfrag := map.leftfetchjoin(pfrag);

    var kind := mposjoin (pre, pfrag, ws.fetch(PRE_KIND));
    var text := kind.[=](TEXT);
    var text_sel := text.select(true).mark(0@0).reverse();
    var text_pre := text_sel.leftfetchjoin(pre)
                            .reverse().mark(0@0).reverse();
    var text_frag := text_sel.leftfetchjoin(pfrag)
                             .reverse().mark(0@0).reverse();

    var text_prop := mposjoin (mposjoin (text_pre, text_frag, ws.fetch(PRE_PROP)),
                               mposjoin (text_pre, text_frag, ws.fetch(PRE_FRAG)),
                               ws.fetch(PROP_TEXT));
    text_pre := nil;
    text_frag := nil;

    var pre_prop := pre.mirror()
                       .outerjoin(text_prop.reverse()
                                           .leftfetchjoin(text_sel)
                                           .reverse());
    var pre_enum := [oid](text);
    var res_size := (iter.tunique().count() 
                  + text.count() + 1)
                  - text_sel.count();

    var res_strs := combine_text_string (iter.chk_order(),
                                         pre_enum,
                                         pre_prop,
                                         res_size);
    iter := nil;
    pre_enum := nil;
    pre_prop := nil;
    res_size := nil;
    var res_texts := text_constr (res_strs.reverse().mark(0@0).reverse(), ws);
    ws := res_texts.fetch(0);
    var textnodes := res_texts.fetch(1);
    res_texts := nil;
    
    text_pre := pre.mirror()
                   .outerjoin(res_strs.mark(0@0)
                                      .leftfetchjoin(textnodes));
    text_frag := pre.mirror()
                    .outerjoin(res_strs.project(WS));
    res_strs := nil;
    textnodes := nil;
    pre := map.reverse()
              .leftfetchjoin([ifthenelse](text,text_pre,pre));
    pfrag := map.reverse()
                .leftfetchjoin([ifthenelse](text,text_frag,pfrag));

    # return result as a BAT of BATs
    return new (void, BAT).insert (nil, ws)
                          .insert (nil, pre)
                          .insert (nil, pfrag)
                          .seqbase (0@0);
}
ADDHELP("merge_adjacent_text_nodes", "tsheyar", "Oct 2005",
        "PARAMETERS:\n\
BAT[void,str] iter : prefix of qname\n\
BAT[void,str] pre  : URI of the qname\n\
BAT[void,str] frag : local part of the qname\n\
BAT[void,BAT] ws   : working set that stores the qname\n\
DESCRIPTION:\n\
merge_adjacent_text_nodes takes an iter|pre|frag schema and\n\
combines within each iteration all adjacent text nodes.\n\
(Note that the order is given by the input order.)\n\
New textnodes are added into the working set ws and the result is\n\
a bat of bats (ws, modified_pres, modified_frags). The heads of the\n\
pre and frag are aligned to the input relations.",
        "pathfinder");
                                
# add_qnames changes the working set as side effect
# without a 'prefix\1uri\1local' index this could be quite expensive
#
# it basically does:
# [ifthenelse]([isnil](local),local.project(nil),[add_qname](prefix,uri,local,local.project(ws)));
PROC add_qnames (BAT[void,str] prefix,
                 BAT[void,str] uri,
                 BAT[void,str] local,
                 BAT[void,BAT] ws) : BAT[void,oid]
{
    # lookup already stored qnames
    var prefix_uri := prefix.[+](str('\1')).[+](uri);
    var propIDs := ws.fetch(QN_PREFIX_URI)
                     .fetch(WS)
                     .leftjoin(prefix_uri.reverse()).mirror();
    var prop_str := propIDs.leftfetchjoin(ws.fetch(QN_LOC).fetch(WS));
    propIDs := prop_str.leftjoin(local.reverse());

    # add missing qnames, that are not the ``nil'' string, to the storage
    var missing := local.select(str(nil),str(nil)).kdiff(propIDs.reverse()).mark(0@0).reverse();
    var seqb := oid (ws.fetch(QN_LOC).fetch(WS).count() +
                     int (ws.fetch(QN_LOC).fetch(WS).seqbase()));
    var new_names := missing.leftfetchjoin(prefix).[+](str('\1')).[+](
                     missing.leftfetchjoin(uri).[+](str('\1')).[+](
                     missing.leftfetchjoin(local)));
        # only search unique values
        new_names := new_names.kunique();
        # create new key with references to the original ones
        new_names := new_names.mark(seqb).reverse().leftfetchjoin(missing);

    var new_prefix := new_names.leftfetchjoin(prefix);
    var new_uri := new_names.leftfetchjoin(uri);
    var new_local := new_names.leftfetchjoin(local);
    ws.fetch(QN_URI).fetch(WS).insert(new_uri);
    ws.fetch(QN_PREFIX).fetch(WS).insert(new_prefix);
    ws.fetch(QN_LOC).fetch(WS).insert(new_local);
    ws.fetch(QN_PREFIX_URI).fetch(WS).insert(new_prefix.[+](str('\1')).[+](new_uri));
    ws.fetch(QN_LOC_URI).fetch(WS).insert(new_local.[+](str('\1')).[+](new_uri));

    # lookup IDs of the qnames
    var ns := ws.fetch(QN_PREFIX_URI)
                .fetch(WS)
                .leftjoin(prefix_uri.reverse());
    var loc := ws.fetch(QN_LOC).fetch(WS)
                 .leftjoin(local.reverse());
    propIDs := ns.intersect(loc);
    # add missing nil values again
    return local.mirror()
                .outerjoin(propIDs.reverse())
                # the result head is aligned with the heads from the input
                .reverse().mark(seqbase(local)).reverse();
}
ADDHELP("add_qnames", "tsheyar", "Oct 2005",
        "PARAMETERS:\n\
BAT[void,str] prefix : prefix of qname\n\
BAT[void,str] uri    : URI of the qname\n\
BAT[void,str] local  : local part of the qname\n\
BAT[void,BAT] ws     : working set that stores the qname\n\
DESCRIPTION:\n\
add_qnames adds qnames consisting of the three strings prefix,\n\
uri, and local to the working set (ws). The return value is the\n\
identifier that corresponds to the qname in the qname container\n\
of the transient nodes.\n\
NOTE: the working set 'ws' is changed as side effect!",
        "pathfinder");

# add_qname changes the working set as side effect
PROC add_qname (str prefix, str uri, str local, BAT[void,BAT] ws) : oid
{
    var propID := ws.fetch(QN_PREFIX_URI)
                    .fetch(WS)
                    .ord_uselect(prefix + str('\1') + uri).mirror();
    var prop_str := propID.leftfetchjoin(ws.fetch(QN_LOC).fetch(WS));
    propID := prop_str.ord_uselect(local);
    prop_str := nil;
    var itemID;
    if (propID.count() = 0)
    {
        itemID := oid(ws.fetch(QN_LOC).fetch(WS).count());
        ws.fetch(QN_URI).fetch(WS).insert(itemID, uri);
        ws.fetch(QN_PREFIX).fetch(WS).insert(itemID, prefix);
        ws.fetch(QN_LOC).fetch(WS).insert(itemID, local);
        ws.fetch(QN_PREFIX_URI).fetch(WS).insert(itemID, prefix + str('\1') + uri);
        ws.fetch(QN_LOC_URI).fetch(WS).insert(itemID, local + str('\1') + uri);
    } else { 
        itemID := propID.reverse().fetch(0);
    }
    return itemID;
}
ADDHELP("add_qname", "tsheyar", "Oct 2005",
        "PARAMETERS:\n\
str           prefix : prefix of qname\n\
str           uri    : URI of the qname\n\
str           local  : local part of the qname\n\
BAT[void,BAT] ws     : working set that stores the qname\n\
DESCRIPTION:\n\
add_qname adds a qname consisting of the three strings prefix,\n\
uri, and local to the working set ws. The return value is the\n\
identifier that corresponds to the qname in the qname container\n\
of the transient nodes.\n\
NOTE: the working set 'ws' is changed as side effect!",
        "pathfinder");

PROC text_constr (BAT[void, str] item, BAT[void, BAT] ws) : BAT[void,BAT]
{
    # find all strings that are not already in the working set ...
    var ws_prop_text := ws.fetch(PROP_TEXT).fetch(WS);
    var unq_str := item.tunique().mark(0@0).reverse();
    var str_unq := unq_str.reverse().kdiff(ws_prop_text.reverse());
    unq_str := nil;
    # ... and add them to the PROP_TEXT container
    var seqb := oid(int(ws_prop_text.seqbase()) + ws_prop_text.count());
    unq_str := str_unq.mark(seqb).reverse();
    str_unq := nil;
    ws_prop_text := ws_prop_text.insert(unq_str);
    unq_str := nil;

    # get the property values of the strings;
    # we invest in sorting ws_prop_text &  X_strings/item on the TYPE_str 
    # join columns as the mergejoin proved to be faster and more robust with
    # large BATs than a hashjoin
    var ws_text_prop := ws_prop_text.reverse().sort();
    var X_item := item.mark(0@0);
    var X_strings := item.reverse().mark(0@0).sort().reverse();
    var X_prop := X_strings.leftjoin(ws_text_prop);
    X_strings := nil;
    ws_text_prop := nil;
    var newProp := X_item.leftjoin(X_prop);
    X_item := nil;
    X_prop := nil;

    # add new text nodes to the working set
    var seqb := oid(count(ws.fetch(PRE_KIND).fetch(WS)) +
                    int(ws.fetch(PRE_KIND).fetch(WS).seqbase()));
    var newPre_prop := newProp.reverse().mark(seqb).reverse();
    newProp := nil;
    ws.fetch(PRE_PROP).fetch(WS).insert(newPre_prop);
    ws.fetch(PRE_SIZE).fetch(WS).insert(newPre_prop.project(0));
    ws.fetch(PRE_LEVEL).fetch(WS).insert(newPre_prop.project(chr(0)));
    ws.fetch(PRE_KIND).fetch(WS).insert(newPre_prop.project(TEXT));
    ws.fetch(PRE_FRAG).fetch(WS).insert(newPre_prop.project(WS));

    { # add index entries for optimized scj
        var kind_pre_ := newPre_prop.mark(nil).reverse().chk_order();
        ws.fetch(KIND_PRE + int(TEXT)).fetch(WS).insert(kind_pre_);
    }
    newPre_prop := nil;
    item := item.mark(seqb);
    seqb := nil;

    # return result as a BAT of BATs
    var res := new (void, BAT).insert (nil, ws)
                              .insert (nil, item)
                              .insert (nil, item.project(WS))
                              .seqbase (0@0);

    { # adding new fragments to the WS_FRAG bat
        var seqb := ws.fetch(WS_FRAG).count();
        seqb := oid(seqb);
        var new_pres := item.reverse().mark(seqb).reverse();
        seqb := nil;
        ws.fetch(WS_FRAG).insert(new_pres);
        new_pres := nil;
        ws.fetch(HEIGHT).replace(WS, max(ws.fetch(HEIGHT).fetch(WS), 1));
    }

    return res;
}
ADDHELP("text_constr", "tsheyar", "Oct 2005",
        "PARAMETERS:\n\
BAT[void,str] item : content of the text nodes\n\
BAT[void,BAT] ws   : working set that stores the document representations\n\
DESCRIPTION:\n\
text_constr is a generic text constructor that creates for each\n\
item a new textnode. These textnodes are added to the working\n\
set ws. Output is a BAT of BATs with the following components:\n\
(a) the modified working set,\n\
(b) the pre values of the textnodes, and\n\
(c) the frag values of the textnodes",
        "pathfinder");

PROC attr_constr (BAT[void, oid] qn, BAT[void, str] item, BAT[void, BAT] ws) : BAT[void,BAT]
{
    # find all strings that are not already in the working set ...
    var ws_prop_val := ws.fetch(PROP_VAL).fetch(WS);
    var unq_str := item.tunique().mark(0@0).reverse();
    var str_unq := unq_str.reverse().kdiff(ws_prop_val.reverse());
    unq_str := nil;
    # ... and add them to the PROP_VAL container
    var seqb := oid(int(ws_prop_val.seqbase()) + ws_prop_val.count());
    unq_str := str_unq.mark(seqb).reverse();
    str_unq := nil;
    ws_prop_val := ws_prop_val.insert(unq_str);
    unq_str := nil;

    # get the property values of the strings;
    # we invest in sorting ws_prop_val &  X_strings/item on the TYPE_str 
    # join columns as the mergejoin proved to be faster and more robust with
    # large BATs than a hashjoin
    var ws_val_prop := ws_prop_val.reverse().sort();
    var X_item := item.mark(0@0);
    var X_strings := item.reverse().mark(0@0).sort().reverse();
    var X_prop := X_strings.leftjoin(ws_val_prop);
    X_strings := nil;
    ws_val_prop := nil;
    var newProp := X_item.leftjoin(X_prop);
    X_item := nil;
    X_prop := nil;

    # add new text nodes to the working set
    var seqb := oid(count(ws.fetch(ATTR_OWN).fetch(WS)) +
                    int(ws.fetch(ATTR_OWN).fetch(WS).seqbase()));
    var newAttr_prop := newProp.reverse().mark(seqb).reverse();
    newProp := nil;
    ws.fetch(ATTR_PROP).fetch(WS).insert(newAttr_prop);
    ws.fetch(ATTR_OWN).fetch(WS).insert(newAttr_prop.mark(nil));
    ws.fetch(ATTR_QN).fetch(WS).insert(qn.reverse().mark(seqb).reverse());
    ws.fetch(ATTR_FRAG).fetch(WS).insert(newAttr_prop.project(WS));

    newAttr_prop := nil;
    item := item.mark(seqb);
    seqb := nil;

    # return result as a BAT of BATs
    var res := new (void, BAT).insert (nil, ws)
                              .insert (nil, item)
                              .insert (nil, item.project(WS))
                              .seqbase (0@0);

    return res;
}
ADDHELP("attr_constr", "tsheyar", "Oct 2005",
        "PARAMETERS:\n\
BAT[void,oid] qn   : names of the attributes\n\
BAT[void,str] item : values of the attributes\n\
BAT[void,BAT] ws   : working set that stores the document representations\n\
DESCRIPTION:\n\
attr_constr is a generic attribute constructor that creates for\n\
each aligned qn|item pair a new attribute. These attributes are\n\
added to the working set ws. Output is a BAT of BATs with the\n\
following components:\n\
(a) the modified working set,\n\
(b) the attribute ids, and\n\
(c) the frag values of the attributes",
        "pathfinder");

PROC elem_constr_empty (BAT[void, oid] qn, BAT[void, BAT] ws) : BAT[void,BAT]
{
    # add new element nodes to the working set
    var seqb := oid(count(ws.fetch(PRE_KIND).fetch(WS)) +
                    int(ws.fetch(PRE_KIND).fetch(WS).seqbase()));
    var newPre_prop := qn.reverse().mark(seqb).reverse();
    ws.fetch(PRE_PROP).fetch(WS).insert(newPre_prop);
    ws.fetch(PRE_SIZE).fetch(WS).insert(newPre_prop.project(0));
    ws.fetch(PRE_LEVEL).fetch(WS).insert(newPre_prop.project(chr(0)));
    ws.fetch(PRE_KIND).fetch(WS).insert(newPre_prop.project(ELEMENT));
    ws.fetch(PRE_FRAG).fetch(WS).insert(newPre_prop.project(WS));

    { # add index entries for optimized scj
        var kind_pre_ := newPre_prop.mark(nil).reverse().chk_order();
        ws.fetch(KIND_PRE + int(ELEMENT)).fetch(WS).insert(kind_pre_);
    }
    newPre_prop := nil;
    qn := qn.mark(seqb);
    seqb := nil;

    # return result as a BAT of BATs
    var res := new (void, BAT).insert (nil, ws)
                              .insert (nil, qn)
                              .insert (nil, qn.project(WS))
                              .seqbase (0@0);

    { # adding new fragments to the WS_FRAG bat
        var seqb := ws.fetch(WS_FRAG).count();
        seqb := oid(seqb);
        var new_pres := qn.reverse().mark(seqb).reverse();
        seqb := nil;
        ws.fetch(WS_FRAG).insert(new_pres);
        new_pres := nil;
        ws.fetch(HEIGHT).replace(WS, max(ws.fetch(HEIGHT).fetch(WS), 1));
    }

    return res;
}
ADDHELP("elem_constr_empty", "tsheyar", "Oct 2005",
        "PARAMETERS:\n\
BAT[void,oid] qn   : names of the elements\n\
BAT[void,BAT] ws   : working set that stores the document representations\n\
DESCRIPTION:\n\
elem_constr_empty is a generic element constructor for empty\n\
elements that creates for each qname qn a new element. These\n\
elements are added to the working set ws. Output is a BAT of\n\
BATs with the following components:\n\
(a) the modified working set,\n\
(b) the pre values, and\n\
(c) the frag values of the elements",
        "pathfinder");

PROC elem_constr (BAT[void, oid] qn_iter,
                  BAT[void, oid] qn_item,
                  BAT[void, oid] iter,
                  BAT[oid, oid] pre,
                  BAT[oid, oid] pfrag,
                  BAT[void, oid] attr,
                  BAT[void, oid] afrag,
                  BAT[void, BAT] ws) : BAT[void,BAT]
{
    var root_iter;
    var root_size;
    var root_prop;
    var root_kind;
    var root_frag;
    var root_level;
    # attr
        var root_pre;
        var root_pre_frag;
        
    # throw out nil values and generate iter|item|frag representation
    # for attributes
    var selected := pre.select(nil,nil);
    var piter := selected.mark(0@0).reverse()
                         .leftfetchjoin(iter)
                         # make it void
                         .reverse().mark(0@0).reverse();
    pre := selected.reverse().mark(0@0).reverse();
    pfrag := pfrag.select(nil,nil).reverse().mark(0@0).reverse();
    selected := nil;

    # throw out nil values and generate iter|item|frag representation
    # for attributes
    selected := attr.select(nil,nil);
    var aiter := selected.mark(0@0).reverse()
                         .leftfetchjoin(iter)
                         # make it void
                         .reverse().mark(0@0).reverse();
    attr := selected.reverse().mark(0@0).reverse();
    afrag := afrag.select(nil,nil).reverse().mark(0@0).reverse();
    selected := nil;

    if (pre.count() != 0) {

        # use head to avoid elimination of duplicates
        # (this is additionally used in the content level determination
        var iter_unq := piter.mirror();
        # get all subtree copies
        var res_scj := loop_lifted_descendant_or_self_step 
                           (iter_unq, pre, pfrag, ws, 0);
        iter_unq := nil;

        # variables for the result of the scj 
        var res_iter := res_scj.fetch(0);
        var res_item := res_scj.fetch(1);
        # !be aware that res_frag is only a fake_project!
        var res_frag := res_scj.fetch(2);
        # !avoid being res_iter a fake_project!
        res_iter := de_NO_project (res_iter, res_item);
        res_scj := nil;
            
        # create content_iter as sorting argument for the merged union
        var content_iter := res_iter.leftfetchjoin(piter).chk_order();
        # create subtree copies for all bats
        var content_size := mposjoin(res_item, res_frag, ws.fetch(PRE_SIZE));
        var content_prop := mposjoin(res_item, res_frag, ws.fetch(PRE_PROP));
        var content_kind := mposjoin(res_item, res_frag, ws.fetch(PRE_KIND));
        var content_frag := mposjoin(res_item, res_frag, ws.fetch(PRE_FRAG));
        var content_level := mposjoin(res_item, res_frag, ws.fetch(PRE_LEVEL));
        # change the level of the subtree copies
        content_level := content_level.[+](chr(1));
        var contentRoot_level := mposjoin(pre, pfrag, ws.fetch(PRE_LEVEL));
        # map Root_level to the result of the scj 
        #using the faked iteration values
        contentRoot_level := res_iter.leftfetchjoin(contentRoot_level);
        content_level := content_level.[-](contentRoot_level);
        content_level := content_level.reverse().mark(0@0).reverse();
        contentRoot_level := nil;
            
        # attr
            # content_pre is needed for attribute subtree copies
            var content_pre := res_item;
            # as well as content_pre_frag
            var content_pre_frag := res_frag;
            
        # get the maximum level of the new constructed nodes
        # and set the maximum of the working set
        {
            var height := int(content_level.max()) + 1;
            ws.fetch(HEIGHT).replace(WS, 
                                     max(ws.fetch(HEIGHT).fetch(WS),
                                         height));
            height := nil;
        }
            
        root_iter := qn_iter.chk_order();
        # calculate the sizes for the root nodes
        root_size := {count}(content_iter.reverse(), qn_iter.reverse())
                             .reverse().mark(seqbase(qn_iter)).reverse();
        root_prop := qn_item;
        root_kind := fake_project(ELEMENT);
        root_frag := fake_project(WS);
        root_level := fake_project(chr(0));

        # attr
            # root_pre is a dummy needed for merge union with content_pre 
            root_pre := fake_project(oid(nil));
            # as well as root_frag_pre
            root_pre_frag := fake_project(oid(nil));

        # merge union root and nodes
        {
        var merged_result := merged_union (
                                 root_iter, content_iter,
                                 root_size, content_size,
                                 root_level, content_level,
                                 root_kind, content_kind,
                                 root_prop, content_prop,
                                 root_frag, content_frag,
        # attr
                                 root_pre, content_pre,
                                 root_pre_frag, content_pre_frag);
        root_iter := nil;
        content_iter := nil;
        content_size := nil;
        content_level := nil;
        content_kind := nil;
        content_prop := nil;
        content_frag := nil;
        # attr
            content_pre := nil;
            content_pre_frag := nil;
        root_size := merged_result.fetch(1);
        root_level := merged_result.fetch(2);
        root_kind := merged_result.fetch(3);
        root_prop := merged_result.fetch(4);
        root_frag := merged_result.fetch(5);
        # attr
            root_pre := merged_result.fetch(6);
            root_pre_frag := merged_result.fetch(7);

        merged_result := nil;

        # printing output for debugging purposes
            # print("merged (root & content)");
            # print(root_size, [int](root_level), [int](root_kind), root_prop);
        }

    } else { # end of ``if (pre.count() != 0)''

        root_size := qn_iter.project(0);
        root_prop := qn_item; # !the seqbase of qn_item is later modified
        root_kind := qn_iter.project(ELEMENT);
        root_frag := qn_iter.project(WS);
        root_level := qn_iter.project(chr(0));
        # attr
            root_pre := qn_iter.project(oid(nil));
            root_pre_frag := qn_iter.project(oid(nil));

    }  # end of else in ``if (pre.count() != 0)''
        
    # set the offset for the new created trees
    {
        var seqb := oid(count(ws.fetch(PRE_SIZE).fetch(WS))
                        + int(ws.fetch(PRE_SIZE).fetch(WS).seqbase()));
        root_size := root_size.seqbase(seqb);
        root_prop := root_prop.seqbase(seqb);
        root_kind := root_kind.seqbase(seqb);
        root_frag := root_frag.seqbase(seqb);
        root_level := root_level.seqbase(seqb);
        # attr
            # get the new pre values
            root_pre := root_pre.seqbase(seqb);
            root_pre_frag := root_pre_frag.seqbase(seqb);
        seqb := nil;
    }

    # insert the new trees into the working set
    ws.fetch(PRE_SIZE).fetch(WS).insert(root_size);
    ws.fetch(PRE_KIND).fetch(WS).insert(root_kind);
    ws.fetch(PRE_PROP).fetch(WS).insert(root_prop);
    ws.fetch(PRE_FRAG).fetch(WS).insert(root_frag);
    ws.fetch(PRE_LEVEL).fetch(WS).insert(root_level);

    # update indexes
    {
      var knd := ELEMENT;
      while ( knd <= DOCUMENT ) {
        var kind_root := root_kind.ord_uselect(knd).reverse().chk_order();
        ws.fetch(KIND_PRE + int(knd)).fetch(WS).insert(kind_root);
        if (knd = ELEMENT) {
          var prop_root := kind_root.reverse().mirror()
                                    .leftfetchjoin(root_prop)
                                    .reverse().chk_order();
          ws.fetch(PROP_PRE + int(knd)).fetch(WS).insert(prop_root);
        }
        if (knd = PI) {
          var prop_root := kind_root.reverse().mirror()
                                    .leftfetchjoin(root_prop)
                                    .reverse().chk_order();
          ws.fetch(PROP_PRE + 1).fetch(WS).insert(prop_root);
        }
        knd :+= chr(1);
      }
    }

    # save the new roots for creation of the intermediate result
    var roots := root_level.ord_uselect(chr(0));
    # (note that all operations are order preserving and ``mark''
    # aligns the key with the qn_iter input 
    roots := roots.mark(0@0).reverse();

    # resetting the temporary variables
    root_size := nil;
    root_prop := nil;
    root_kind := nil;
    root_frag := nil;
    root_level := nil;
        
    # adding the new constructed roots to the WS_FRAG bat of the
    # working set, that a following (preceding) step can check
    # the fragment boundaries
    {
        var seqb := oid(count(ws.fetch(WS_FRAG)));
        var new_pre_values := roots.reverse().mark(seqb).reverse();
        seqb := nil;
        ws.fetch(WS_FRAG).insert(new_pre_values);
        new_pre_values := nil;
    }

    # ----------------------------------
    # ----- ATTRIBUTE TRANSLATION ------
    # ----------------------------------
    # 1. step: add subtree copies of attributes
    if (pre.count() != 0) { # but only if there are any subtree nodes
        # lookup the affected attributes using the old pre values
        var preNew_attr := mvaljoin(root_pre, 
                                    root_pre_frag,
                                    ws.fetch(ATTR_OWN));
        # lookup the first free attr value
        var seqb := oid(ws.fetch(ATTR_QN).fetch(WS).count());
        # split up result of mvaljoin and mark them with the correct seqbase
        var attrNew_preNew := preNew_attr.mark(seqb).reverse();
        var attrNew_attrOld := preNew_attr.reverse().mark(seqb).reverse();
        preNew_attr := nil;
        var attrNew_pre_frag := attrNew_preNew.leftfetchjoin(root_pre_frag);
        # help MIL to keep head void
        attrNew_pre_frag := attrNew_pre_frag.reverse().mark(seqb).reverse();
        seqb := nil;

        # get the values of the QN/OID offsets for the reference to the
        # string values
        var attrNew_qn := mposjoin(attrNew_attrOld,
                                   attrNew_pre_frag,
                                   ws.fetch(ATTR_QN));
        var attrNew_prop := mposjoin(attrNew_attrOld,
                                     attrNew_pre_frag,
                                     ws.fetch(ATTR_PROP));
        # get fragment where values are stored (not where attribute is stored)
        var attrNew_frag := mposjoin(attrNew_attrOld,
                                     attrNew_pre_frag,
                                     ws.fetch(ATTR_FRAG));
        attrNew_attrOld := nil;
        attrNew_pre_frag := nil;

        ws.fetch(ATTR_QN).fetch(WS).insert(attrNew_qn);
        ws.fetch(ATTR_PROP).fetch(WS).insert(attrNew_prop);
        ws.fetch(ATTR_OWN).fetch(WS).insert(attrNew_preNew);
        ws.fetch(ATTR_FRAG).fetch(WS).insert(attrNew_frag);
        attrNew_qn := nil;
        attrNew_prop := nil;
        attrNew_preNew := nil;
        attrNew_frag := nil;
    }

    # 2. step: add attribute binding for new elements
    if (attr.count() != 0) { # but only if there are any top level attributes
        
        # use iter, qn and frag to find unique combinations
        var attr_qn := mposjoin(attr, afrag, ws.fetch(ATTR_QN));
        var attr_frag := mposjoin(attr, afrag, ws.fetch(ATTR_FRAG));
        var sorting := aiter.reverse().sort().reverse();
        sorting := sorting.CTrefine(mposjoin(attr_qn,
                                             attr_frag,
                                             ws.fetch(QN_LOC_URI)));
        var unq_attrs := sorting.tunique();
        sorting := nil;
        # test uniqueness
        if (unq_attrs.count() != aiter.count())
        {
           if (qn_item.count() > 0) {
               ERROR ("err:XQDY0025: attribute names are not unique in constructed element '%s'.",
                      qn_item.leftfetchjoin(ws.fetch(QN_LOC).fetch(WS))
                             .fetch(0));
           } else {
               ERROR ("err:XQDY0025: attribute names are not unique in constructed element.");
           }
        }
        unq_attrs := nil;

        var seqb := oid(ws.fetch(ATTR_QN).fetch(WS).count());
        attr_qn := attr_qn.seqbase(seqb);
        var attr_own := aiter.leftjoin(qn_iter.reverse())
                             .leftfetchjoin(roots)
                             .reverse().mark(seqb).reverse();
        var attr_prop := mposjoin(attr, afrag, ws.fetch(ATTR_PROP));
        attr_prop := attr_prop.seqbase(seqb);
        attr_frag := attr_frag.seqbase(seqb);
        seqb := nil;

        ws.fetch(ATTR_QN).fetch(WS).insert(attr_qn);
        ws.fetch(ATTR_PROP).fetch(WS).insert(attr_prop);
        ws.fetch(ATTR_OWN).fetch(WS).insert(attr_own);
        ws.fetch(ATTR_FRAG).fetch(WS).insert(attr_frag);
        attr_qn := nil;
        attr_prop := nil;
        attr_own := nil;
        attr_frag := nil;
    }

    # create result as a BAT of BATs
    var res := new (void, BAT).insert (nil, ws)
                              .insert (nil, roots)
                              .insert (nil, roots.project(WS))
                              .seqbase (0@0);
    roots := nil;

    return res;
}
ADDHELP("elem_constr", "tsheyar", "Oct 2005",
        "PARAMETERS:\n\
BAT[void, oid] qn_iter : iteration values of the qnames\n\
BAT[void, oid] qn_item : names of the elements\n\
BAT[void, oid] iter    : iteration values of the content\n\
BAT[oid, oid] pre      : pre values of the content (heads aligned with head of iter)\n\
BAT[oid, oid] pfrag    : node frag values of the content (heads aligned with head of iter)\n\
BAT[void, oid] attr    : attribute ids of the content (heads aligned with head of iter)\n\
BAT[void, oid] afrag   : attribute fragments of the content (heads aligned with head of iter)\n\
BAT[void, BAT] ws      : working set that stores the document representations\n\
DESCRIPTION:\n\
elem_constr is a full featured element constructor that requires\n\
qn|iter pairs for the name part and iteration, attribute, and node\n\
information for the content. These elements are added to the working\n\
set ws. Output is a BAT of BATs with the following components:\n\
(a) the modified working set,\n\
(b) the pre values, and\n\
(c) the frag values of the elements",
        "pathfinder");


PROC destroy_ws_locked(BAT[void,bat] ws) : void
{
    doc_query.select(int(ws))@batloop() {
        var doc_oid := $h;
        doc_query.delete(doc_oid, int(ws));
        if (not(doc_query.exist(doc_oid))) {
            if (doc_sema.exist(doc_oid)) {
                sema_up(doc_sema.find(doc_oid));
            }
        }
    }
    # determine whether a cache flush is desired (only count cached bats, those with a timestamp)
    var cursize := sum(bat("doc_timestamp").select(timestamp(nil),timestamp(nil)).mirror().join(bat("doc_size")));
    var maxsize := (1024LL * 1024LL) * lng(monet_environment.find("xquery_cacheMB"));

    if (cursize > maxsize) {
        delete_all_docs_locked(true);
    }
}

PROC destroy_ws(BAT[void,bat] ws) : void
{
    var err;

    if (type(doc_query) != bat) { return; }

    # always make sure not to cause errors while holding the lock
    lock_set(pf_lock);
    err := CATCH(destroy_ws_locked(ws));
    lock_unset(pf_lock);

    if (not(isnil(err))) { ERROR(err); }
}

PROC shred_doc_locked(BAT[str,bat] docBAT, str location, str name, timestamp ts, int ws, int height) : void 
{
    # get a new persistent doc id
    var doc_oid := 0@0;
    if (bat("doc_name").count() > 0) 
        doc_oid := oid(int(bat("doc_name").reverse().max()) + 1); 

    # rename all new bats with the doc id as suffix
    [rename](docBAT, [+](mirror(docBAT), str(int(doc_oid))));
    var totsize := sum([batsize](docBAT));

    # add to meta table
    bat("doc_name").insert(doc_oid, name);
    bat("doc_location").insert(doc_oid, location);
    bat("doc_timestamp").insert(doc_oid, ts);
    bat("doc_height").insert(doc_oid, height);
    bat("doc_size").insert(doc_oid, totsize);

    [persists](docBAT, true);

    # add the meta table to the commit set 
    #docBAT.insert(str(nil),bat("doc_name"));
    #docBAT.insert(str(nil),bat("doc_location"));
    #docBAT.insert(str(nil),bat("doc_timestamp"));
    #docBAT.insert(str(nil),bat("doc_height"));
    #docBAT.insert(str(nil),bat("doc_size"));
    #if (not(subcommit(docBAT))) 
    if (not(commit())) ERROR("shred_doc(%s) : commit failed\n");

    if (not(isnil(ws))) {
        pin_doc(doc_oid, ws);
    }
}

# HACK: overwrite 'height'
PROC shred_doc_impl(str location, str name, bit doCommit, str opt, timestamp ts, int ws) : BAT[void,bat]
{
    var err, exists := false;

    # always make sure not to cause errors while holding the lock
    lock_set(pf_lock);
    err := CATCH(exists := doc_check(name));
    lock_unset(pf_lock);

    if (not(isnil(err))) ERROR(err);
    else if (exists) ERROR("shred_doc(%s): already exists in database!\n", name);

    # shred it unlocked
    var docBAT := new(str,bat,WS_SIZE); # contains all new bats
    var us := usec();
    var shredRES := shred2bats(location,opt);

    # use the ws macro to get the the basic working set (excluding views, constants and indices)
@= shred_doc_ws
    if (and(=(@3, bat), <(@1, DOC_LOADED)))
        docBAT.insert(toLower("@1"), shredRES.fetch(@1));
@mil
    @:ws(shred_doc)@

    # these are simple concatenations (to accelerate combPRINT_WS_SZined lookup)
    docBAT.insert("qn_prefix_uri", docBAT.find("qn_prefix").[+](str('\1')).[+](docBAT.find("qn_uri")));
    docBAT.insert("qn_loc_uri",    docBAT.find("qn_loc").[+](str('\1')).[+](docBAT.find("qn_uri")));

    # create our indices
    var knd := ELEMENT;
    while ( knd <= DOCUMENT ) {
        docBAT.insert("kind_pre" + "_" + str(chr(48 + knd)),
                      docBAT.find("pre_kind").ord_uselect(knd).reverse().chk_order());
        knd :+= chr(1);
    }

    var knd_pre_bat := docBAT.find("kind_pre" + "_" + str(chr(48 + ELEMENT))).reverse().mirror();
    var prop_pre_bat := knd_pre_bat.leftfetchjoin(docBAT.find("pre_prop")).reverse().chk_order();
    if (not(ordered(prop_pre_bat))) {
        var _prop := prop_pre_bat.mark(0@0).reverse();
        var _pre  := prop_pre_bat.reverse().mark(0@0).reverse();
        var ord := _prop.reverse().sort().reverse();
            ord := ord.CTrefine(_pre).mark(nil);
        prop_pre_bat := prop_pre_bat.fetch(ord);
        prop_pre_bat := prop_pre_bat.chk_order();
    }
    docBAT.insert("prop_pre" + "_" + str(chr(48 + ELEMENT)), prop_pre_bat);

    knd_pre_bat := docBAT.find("kind_pre" + "_" + str(chr(48 + PI))).reverse().mirror();
    prop_pre_bat := knd_pre_bat.leftfetchjoin(docBAT.find("pre_prop")).reverse().chk_order();
    if (not(ordered(prop_pre_bat))) {
        var _prop := prop_pre_bat.mark(0@0).reverse();
        var _pre  := prop_pre_bat.reverse().mark(0@0).reverse();
        var ord := _prop.reverse().sort().reverse();
            ord := ord.CTrefine(_pre).mark(nil);
        prop_pre_bat := prop_pre_bat.fetch(ord);
        prop_pre_bat := prop_pre_bat.chk_order();
    }
    docBAT.insert("prop_pre" + "_" + str(chr(48 + PI)), prop_pre_bat);

    height := shredRES.fetch(PRE_LEVEL).max().int() + 1;

    docBAT := docBAT.access(BAT_READ);
    if (doCommit) {
        [mmap](docBAT, [ifthenelse]([<]([count](docBAT), 65536), STORE_MEM, STORE_MMAP));
        [save](docBAT);

        # add doc to the database locked
        lock_set(pf_lock);
        err := CATCH(shred_doc_locked(docBAT, location, name, ts, ws, height));
        lock_unset(pf_lock);

        if (not(isnil(err))) ERROR(err);
    }
    return docBAT;
}

PROC shred_doc_base(str location, str name, bit doCommit, str opt) : BAT[void,bat]
{
    var height, us := usec();
    var docBAT := shred_doc_impl(location, name, doCommit, opt, timestamp(nil), int(nil));
    # save all persistent BATs and print timings
    us := usec() - us;
    var ms := us/1000;
    if ( doCommit ) {
        printf("# Shredded XML doc(\"%s\"), total time after commit=%lld.%03llds\n", name, ms/1000,ms%1000);
    } else {
        printf("# Shredded XML doc(\"%s\") uncommitted, total time =%lld.%03llds\n", name, ms/1000,ms%1000);
    }
    return docBAT;
}

PROC shred_doc(str location, str name) : void
{
    shred_doc_base(location, name, true, "");
}
ADDHELP("shred_doc", "flokstra", "Dec 2004",
"PARAMETERS:\n\
- str URI containing the xml document to be shredded)\n\
- str document name ('alias') in database\n\
DESCRIPTION:\n\
Shred xml document to the internal Pathfinder format, (analog to import_doc()).",
"pathfinder");

PROC delete_doc_locked(oid doc_oid) : bit
{
    var docBAT := new(void,str,WS_SIZE);

    if (doc_sema.exist(doc_oid)) {
        # somebody else is deleting the same doc!
        while(doc_sema.exist(doc_oid)) {
            lock_unset(pf_lock);
            sleep(1);
            lock_set(pf_lock);
        }
        return; # now he is done
    }
    # we will delete it
    while (doc_query.exist(doc_oid)) {
        var sem := sema_create(0);
        doc_sema.insert(doc_oid, sem);

        lock_unset(pf_lock);
        sema_down(sem); # wait for the users to go away 
        lock_set(pf_lock);

        sema_destroy(sem);
        doc_sema.delete(doc_oid, sem);
    }

    # use the ws macro to get the names of all non-(view or constant) bats
@= getlower_ws
    if (@3 = bat)
        docBAT.insert(nil, toLower("@1"));
@mil
    @:ws(getlower)@
    docBAT := docBAT.access(BAT_READ);

    # docBAT becomes [name,bat]
    docBAT := [bat]([+](docBAT, str(int(doc_oid))).reverse().mirror());

    # rename the bats so even in case of failure they don't bother us directly anymore
    [rename](docBAT, [+]([+]("rm_", mirror(docBAT)), +("_" , str(lng(current_timestamp()))))).access(BAT_WRITE);

    # remove them from the repository
    [persists](docBAT, false);

    bat("doc_name").delete(doc_oid);
    bat("doc_location").delete(doc_oid);
    bat("doc_timestamp").delete(doc_oid);
    bat("doc_height").delete(doc_oid);
    bat("doc_size").delete(doc_oid);

    # also commit the meta bats
    #docBAT.insert(str(nil), bat("doc_name"));
    #docBAT.insert(str(nil), bat("doc_location"));
    #docBAT.insert(str(nil), bat("doc_timestamp"));
    #docBAT.insert(str(nil), bat("doc_height"));
    #docBAT.insert(str(nil), bat("doc_size"));
    #return subcommit(docBAT);
    return commit();
}

PROC delete_doc(str name) : void
{
    var err, exists := false;

    # always make sure not to cause errors while holding the lock
    lock_set(pf_lock);
    err := CATCH(exists := doc_check(name));
    lock_unset(pf_lock);

    if (not(isnil(err))) ERROR(err);
    else if (not(exists)) ERROR("delete_doc(%s): document not found in database!\n", name);

    # always make sure not to cause errors while holding the lock
    lock_set(pf_lock);
    err := CATCH(delete_doc_locked(bat("doc_name").reverse().find(name)));
    lock_unset(pf_lock);

    if (not(isnil(err))) ERROR(err);
}
ADDHELP("delete_doc", "tsheyar", "July 2004",
"PARAMETERS:\n\
str document name\n\
DESCRIPTION:\n\
delete the persistent BATS that store the document.",
"pathfinder");

PROC delete_all_docs_locked(bit cachedOnly) : void
{
    doc_init();
    bat("doc_timestamp").copy()@batloop() {
        if (not(and(cachedOnly, isnil($t))))
            delete_doc_locked($h);
    }
}

PROC delete_all_docs(bit cachedOnly) : void
{
    var err, b := bat(timestamp,str);

    # always make sure not to cause errors while holding the lock
    lock_set(pf_lock);
    err := CATCH(delete_all_docs_locked(cachedOnly));
    lock_unset(pf_lock);

    if (not(isnil(err))) ERROR(err);
}
ADDHELP("delete_all_docs", "tsheyar", "July 2004",
"DESCRIPTION:\n\
deletes all persistent document BATs that store xml documents;\n\
with parameter TRUE, only the (implicitely) cached documents are deleted,\n\
with parameter FALSE, also the (explicitely) shredded documents are deleted.",
"pathfinder");


PROC pin_doc(oid doc_oid, int ws) : void 
{
    # pin the document on this working set (must hold lock!)
    doc_query.insert(doc_oid, int(ws));
}

# HACK: this proc overwrites variables 'lifetime' and 'ts'
PROC add_doc_locked(str name, int ws) : oid
{
    var doc_oid := nil;
    var filename := name;
    var lim := timestamp(nil);

    # check if the document already existed
    if (doc_check(name)) {
        doc_oid := bat("doc_name").reverse().find(name);
    } else if (bat("doc_location").reverse().exist(name)) {
        # existed as cached location (URI): must check timestamp
        var doc_oid := bat("doc_location").reverse().find(name);
    }

    # do some first analysis on the URI
    if (not(isnil(doc_oid))) { 
        filename := bat("doc_location").find(doc_oid);
        lim := bat("doc_timestamp").find(doc_oid);
    }

    if (startsWith(filename, "file://") or startsWith(filename, "FILE://")) {
        filename := string(filename, 7);
    } else {
        idx := search(filename, "://");
    }
    if (idx >= 0) {
        # add uri-dependent lifetime
        var uri_lifetime := bat("uri_lifetime");
        var b := [startsWith](filename, mirror(uri_lifetime)).uselect(true);
        if (b.count() > 0) {
            var matchlen := [length](mirror(b));
            lifetime := *(1000LL, uri_lifetime.find(matchlen.reverse().find(matchlen.max())));
        } else {
            lifetime := lng(nil); # doc will *not* be cached 
        }
    } else {
        # a file: get lastmodification time
        var tts;
        var err := CATCH(tts := lastmod_time(filename));
        if (isnil(err)) {
            ts := tts; # timestamp was read succesfully
        } else if (not(isnil(lim))) {
            ts := add(lim, 1LL); # file-error on cached file: force remove
        }
    }

    if (isnil(doc_oid)) {
        return nil; # document not in database
    } else if (and(not(isnil(lim)), >(ts,lim))) { # if so, check if it still valid
        # remove outdated document from the cache
        delete_doc_locked(doc_oid); 
        return nil;
    }
    pin_doc(doc_oid, ws); # make sure nobody can delete it
    return doc_oid;
}

# if docid != "", it holds the str(id) of the persistent doc
# if not, the temporary document is in tmp_doc (by name without id)
PROC lookup_bat(BAT[str,bat] tmp_doc, str name, str docid) : bat[any,any]
{
    if (docid != "") return bat(name + docid);
    return tmp_doc.find(name);
}

PROC add_docbat(BAT[void,bat] ws, BAT[str,bat] docBAT, str name, str docid, int height) : oid
{
    # add it to the working set
    var frag := ws.fetch(DOC_LOADED).uselect(name);

    if (frag.count() = 0) {
        ws.fetch(DOC_LOADED).insert(nil,name);
        frag := oid(ws.fetch(DOC_LOADED).count());

        # use the ws macro to instantiate all non-(view or constant) bats
@= add_doc_bat_ws
        if (@3 = bat)
            ws.fetch(@1).insert(nil,docBAT.lookup_bat(toLower("@1"), docid));
@mil
        @:ws(add_doc_bat)@

        # create views
        ws.fetch(PRE_FRAG).insert(nil,fake_project(frag));
        ws.fetch(ATTR_FRAG).insert(nil,fake_project(frag));

        # adapt constants
        ws.fetch(FRAG).replace(0@0,frag);

        if (isnil(height))
          height := ws.fetch(PRE_LEVEL).max().int() + 1;
        ws.fetch(HEIGHT).insert(nil,height);
     } else { 
        ws.fetch(FRAG).replace(0@0,frag.reverse().fetch(0)); 
     }
     return frag;
}

var time_shred := 0; # dummy declarartion to avoid errors when timing is not activated in the script

# HACK: this proc overwrites variable 'time_shred' 
PROC add_doc(BAT[void,bat] ws, str name) : BAT[void,bat]
{
    var ts := current_timestamp();
    var lifetime := 0LL;
    var idx := -1;
    var doc_oid;
    var err, height;
    var t := time();

    # always make sure not to cause errors while holding the lock
    lock_set(pf_lock);
    err := CATCH(doc_oid := add_doc_locked(name, int(ws)));
    lock_unset(pf_lock);

    if (not(isnil(err))) ERROR(err);

    var docBAT, docid := "", doCommit := true;

    if (isnil(doc_oid)) {
        ts := add(ts, lifetime);
        if (isnil(ts)) doCommit := false;
        docBAT := shred_doc_impl(name, name, doCommit, "", ts, int(ws));
        # overwrites 'height'
        time_shred :+=  time() - t;
    } else {
        docBAT := new(str,bat);
    }
    if (doCommit) { 
        docid := str(int(bat("doc_name").reverse().find(name)));
        height := bat("doc_height").find(oid(int(docid)));
    }
    add_docbat(ws, docBAT, name, docid, height);  
    return ws;
}


@- xml document cache
When fn:doc() is used with a previously unseen URI, it is shredded on the fly and placed into 
the xml document cache (see text below). A number of procs are provided to monitor
and control the behavior of the cache.
@mil
const xmlcache_help := 
"The XML document cache keeps indexed copies of documents that where recently\n\
used in the fn:doc(URI) xquery function.\n\
\n\
The size of the cache is controlled using the 'xquery_cacheMB' setting in\n\
the 'MonetDB.conf' file.\n\
\n\
For file URIs, the cache looks at the last-modification-time of the file on disk\n\
to guarantee that the cached document is still up-to-date for answering queries from.\n\
\n\
For other URIs, *lifetime rules* determine how long documents can stay in the cache.\n\
Each lifetime rule consists of a URI prefix and the registered seconds of lifetime.\n\
\n\
The rule with longest prefix that matches an URI counts. Specifying a lifetime\n\
of 'int(nil)' seconds means that the URI will *not* be cached at all.\n\
This is also the default if no prefix matches an URI.\n\
\n\
The name of a cached document is the same as its location (URI). For explicitly\n\
shredded documents (with 'shred_doc(location,name)'), the name is an 'alias' and\n\
may differ from the URI. Explicitly shredded documents fall outside the XML document\n\
cache; documents are only removed at explicit user request (with 'delete_doc(name)').";

PROC xmlcache_add_rule(str uri, any lifetime) : void {
    xmlcache_add_rule(uri, lng(lifetime)); 
}
PROC xmlcache_add_rule(str uri, lng lifetime) : void {
    lock_set(pf_lock);
    var err := CATCH({ doc_init(); bat("uri_lifetime").delete(uri); bat("uri_lifetime").insert(uri, lifetime); });
    lock_unset(pf_lock);
    if (not(isnil(err))) ERROR(err);
}
ADDHELP("xmlcache_add_rule", "boncz", "May 2005",
"DESCRIPTION:\nadd a new URI lifetime rule.\n\n" + xmlcache_help,  "pathfinder");

PROC xmlcache_del_rule(str uri) : void {
    lock_set(pf_lock);
    var err := CATCH({doc_init(); bat("uri_lifetime").delete(uri);});
    lock_unset(pf_lock);
    if (not(isnil(err))) ERROR(err);
}
ADDHELP("xmlcache_del_rule", "boncz", "May 2005",
"DESCRIPTION:\ndeletes an existing URI lifetime rule.\n\n" + xmlcache_help,  "pathfinder");

PROC xmlcache_print_rules() : void {
    lock_set(pf_lock);
    var err := CATCH({ doc_init(); table(bat("uri_lifetime").mark(0@0).reverse().col_name("URI-prefix"), bat("uri_lifetime").reverse().mark(0@0).reverse().col_name("liftime-secs")); });
    lock_unset(pf_lock);
    if (not(isnil(err))) ERROR(err);
}
ADDHELP("xmlcache_print_rules", "boncz", "May 2005",
"DESCRIPTION:\nshows all URI lifetime rules.\n\n" + xmlcache_help,  "pathfinder");

PROC xmlcache_print() : void {
    lock_set(pf_lock);
    var err := CATCH({ doc_init(); table(bat("doc_name").col_name("alias"), bat("doc_location").col_name("URI"), bat("doc_size").col_name("size"), bat("doc_timestamp").select(timestamp(nil),timestamp(nil)).col_name("valid-thru")); });
    lock_unset(pf_lock);
    if (not(isnil(err))) ERROR(err);
}
ADDHELP("xmlcache_print", "boncz", "May 2005",
        "DESCRIPTION:\nshows the actual content of the XML document cache.\n\n" + xmlcache_help,  "pathfinder");

PROC xmldb_print() : void {
    lock_set(pf_lock);
    var err := CATCH({ doc_init(); table(mirror(bat("doc_timestamp").uselect(timestamp(nil))).join(bat("doc_name")).col_name("alias"), bat("doc_location").col_name("URI"), bat("doc_size").col_name("size")); });
    lock_unset(pf_lock);
    if (not(isnil(err))) ERROR(err);
}
ADDHELP("xmldb_print", "boncz", "May 2005",
"DESCRIPTION:\nshows the actual content of the persistent XML document database (not the XML document cache).\n\nThis consists of all documents explicitly shredded with shred_doc(URI, alias).",  "pathfinder");


@h
#ifndef PATHFINDER_H
#define PATHFINDER_H

#include <monettime.h>
#include <lock.h>
#include <monet.h>
#include <monet_interpreter.h>
#include <streams.h>
#include <pathfinder.proto.h>

pathfinder_export int CMDxquery(char** res, char *mode, char* xquery);

#endif

@c
#include "pathfinder.h"

MT_Lock pf_compiler_lock;

#include "compile_interface.h"

/*
 * translate xquery to MIL and execute
 */
int exec_xquery_mil(int debug, char *prologue, char *query, char *epilogue, char *err, char* mapi) {
    int ret = GDK_FAIL;
    if (debug) {
        stream_printf(GDKout,
                "%s#mil#########################################################\n",
                mapi);
        if (prologue) stream_write(GDKout, prologue, strlen(prologue), 1);
        else stream_printf(GDKout, "%s#null prologue\n", mapi);
        if (query) stream_write(GDKout, query, strlen(query), 1);
        else stream_printf(GDKout, "%s#null query\n", mapi);
        if (epilogue) stream_write(GDKout, epilogue, strlen(epilogue), 1);
        else stream_printf(GDKout, "%s#null epilogue\n", mapi);
    }
    if (err) {
        if (debug) {
            stream_printf(GDKout,
                    "%s#err#########################################################\n",
                    mapi);
        }
        stream_write(GDKout, err, strlen(err), 1);
    }
    if (prologue && query && epilogue) {
        if (debug) {
            stream_printf(GDKout, 
                    "%s#exec prologue...############################################\n",
                    mapi);
        }
        if (monet_exec(prologue) >= 0) {
            if (debug) {
                stream_printf(GDKout, 
                        "%s#exec query...###############################################\n",
                        mapi);
            }
            if (monet_exec(query) >= 0) {
                if (debug) {
                    stream_printf(GDKout, 
                            "%s#exec epilogue...############################################\n",
                            mapi);
                }
                if (monet_exec(epilogue) >= 0) {
                    if (debug) {
                        stream_printf(GDKout, 
                            "%s#ok##########################################################\n", 
                            mapi);
                    }
                    ret = GDK_SUCCEED;
                }
            }
        }
    }
    if (debug && ret != GDK_SUCCEED) {
        stream_printf(GDKout, 
                "%s#execution failed############################################\n", 
                mapi);
    }
    /* what comes out of PF is alloced differently */
    if (prologue) free(prologue);
    if (query) free(query);
    if (epilogue) free(epilogue);
    return ret;
}

static int
exec_xquery(char *mode, char* xquery) {
    char *prologue = NULL, *query = NULL, *epilogue = NULL, *err;
    int debug = strstr(mode,"debug") != NULL;
    char *mapi = strstr(mode, "mapi")?"=":"";

    if (debug) {
        stream_printf(GDKout,
                "%s#xquery######################################################\n",
                mapi);
        stream_write(GDKout, xquery, strlen(xquery), 1);
    }
    MT_set_lock(pf_compiler_lock, "CMDxquery");
    err = pf_compile_MonetDB(xquery, mode, &prologue, &query, &epilogue);
    if (err) {
        int len = strlen(err);
        if (*mapi) {
            /* put ! before error lines */
            char *p = err, *q = err;
            while (*p) {
		if (*p++ == '\n')
		    len++;
	    }
            err = (char*) alloca(len+3);
            *err = '!'; 
            for (p = err + 1; *q; q++) {
                *p++ = *q;
                if (*q == '\n')
		    *p++ = '!'; 
            }
            /* guard against errors that do not terminate in a newline */
            if (q > err && q[-1] != '\n')
		*p++ = '\n';
            else if (p[-1] == '!')
		p--;
            *p = 0;
            len = p - err;
        }
    }
    MT_unset_lock(pf_compiler_lock, "CMDxquery");
    return exec_xquery_mil(debug, prologue, query, epilogue, err, mapi);
}

int
CMDxquery(char** res, char *mode, char* xquery) {
    buffer *b = buffer_create(16384);
    stream *s = buffer_wastream(b, "CMDxquery");
    stream *o = GDKout;
    char *dst = NULL;
    int ret = 0;

    /* run the query, but collect all output in a buffer */
    THRsetdata(0,s);
    ret = exec_xquery(mode, xquery);
    THRsetdata(0,o);
    stream_close(s);

    if (ret == GDK_SUCCEED) {
        dst = GDKmalloc(b->pos+1);
        memcpy(dst, b->buf, b->pos);
        dst[b->pos] = 0;
    }
    buffer_destroy(b);
    stream_destroy(s);
    *res = dst;
    return ret;
}

@= builtin_operand
{
    int _k = interpret(stk, arg(lt, @1), res);
    if (_k < 0) {
        return _k;
    }
    @3 = VALconvert(@2, res);
    if ((@3 == ILLEGALVALUE)  || (@2 == TYPE_bat && !@3)) {
        return handle_paramerror(res,@1,res->vtype,@2);
    }
}
@c

inline int
xquery_shutdown(Client client, char* buf, int ret)
{
    GDKfree(buf);
    closeClient(client, 0);
    return(ret);
}

#define XQUERY_CHALLENGE "::mserver_xquery:4\n"

/*
 * read xquery from input stream; execute and print on output stream 
 */  
int
CMDxquery_server(Cntxt stk, YYSTREE lt, ValPtr res)
{
    stream **In, **Out;
    stream *in = NULL, *out = NULL;
    Thread XQthread = THRget(THRgettid());
    Client client;
    size_t curlen = 0, maxlen = BUFSIZ;
    /* one extra for terminator */
    char *xquery, *user, *buf = GDKmalloc(maxlen + 1);
    int i;

    if (buf == NULL)
        return GDK_FAIL;
    if (lt->cnt != 2)
        return handle_argerror(res, lt->cnt, 2);

    /* this is a BUILTIN because we must obtain the client context (and
     * thus the streams) */
    CNTXTclient((Cntxt) (ptrdiff_t) stk, &client);
    @:builtin_operand(0,TYPE_Stream,In)@
    in = *In;
    @:builtin_operand(1,TYPE_Stream,Out)@
    out = *Out;

    /* send challenge */
    snprintf(buf, 100, "%2u%s", (unsigned int)strlen(XQUERY_CHALLENGE), XQUERY_CHALLENGE);
    if (stream_write(out, buf, strlen(buf), 1) < 0) 
        return(xquery_shutdown(client, buf, GDK_FAIL));

    /* get user name (ignored later), and mapiclient mode */
    memset(buf, 0, BUFSIZ);
    user = buf;
    if (stream_read(in, user, 1, 1) < 0)
        return(xquery_shutdown(client, buf, GDK_FAIL));
    for (i = 0; i < BUFSIZ && *user != '\n'; i++) {
        user++;
        if (stream_read(in, user, 1, 1) < 0)
            return(xquery_shutdown(client, buf, GDK_FAIL));
    }
    *user = 0;

    /* check for blocked mode */
    if (strncmp(buf + i - 8, ":blocked", 8) == 0) {
        in = block_stream(stream_rstream(in));
        out = block_stream(stream_wstream(out));
        monetSetChannel(XQthread, in, out);
    }

    /* use the MAPI protocol to read as much xquery buffer as possible */
    while (1) {
        if (stream_write(out, PROMPT1, sizeof(PROMPT1) - 1, 1) < 0)
            return(xquery_shutdown(client, buf, GDK_FAIL));
        if (stream_flush(out))
            return(xquery_shutdown(client, buf, GDK_FAIL));

        curlen = 0;
        for ( ; buf; ) {
            ssize_t n = stream_read(in, buf + curlen, 1, maxlen - curlen);
            if (n < 0)
                return(xquery_shutdown(client, buf, GDK_FAIL));
            if (n == 0) {
                break;
            }
            curlen += n;
            if (curlen == maxlen) {
                maxlen += 1024;
                buf = GDKrealloc(buf, maxlen + 1);
            }
        }

        /* execute query */
        if (buf) {
            buf[curlen] = 0;	/* terminate (we know there is space) */

            /* first line of the query is the mode */ 
            for (xquery = buf; *xquery; xquery++)
                if (*xquery == '\n') {
                    *xquery++ = 0;
                    break;
                }

            /* we ignore the return value, since it doesn't make any
             * sense in a multi-query environment */
            exec_xquery(buf, xquery); /* execute, see above */
        }
    }
    return(xquery_shutdown(client, buf, GDK_SUCCEED));
}

bat *
pathfinder_prelude()
{
    pf_compiler_lock = MT_create_lock();
    return NULL;
}

void
pathfinder_epilogue()
{
    MT_destroy_lock(pf_compiler_lock);
}
/* vim:set shiftwidth=4 expandtab: */
