@' The contents of this file are subject to the Pathfinder Public License
@' Version 1.1 (the "License"); you may not use this file except in
@' compliance with the License.  You may obtain a copy of the License at
@' http://monetdb.cwi.nl/Legal/PathfinderLicense-1.1.html
@'
@' Software distributed under the License is distributed on an "AS IS"
@' basis, WITHOUT WARRANTY OF ANY KIND, either express or implied.  See
@' the License for the specific language governing rights and limitations
@' under the License.
@'
@' The Original Code is the Pathfinder system.
@'
@' The Original Code has initially been developed by the Database &
@' Information Systems Group at the University of Konstanz, Germany and
@' is now maintained by the Database Systems Group at the Technische
@' Universitaet Muenchen, Germany.  Portions created by the University of
@' Konstanz and the Technische Universitaet Muenchen are Copyright (C)
@' 2000-2005 University of Konstanz and (C) 2005-2007 Technische
@' Universitaet Muenchen, respectively.  All Rights Reserved.

@f pf_support
@a Stefan Manegold
@v 1.0
@t MIL primitives to support the XQuery front-end "Pathfinder"

@* Introduction
This module provides new MIL primitives to support the XQuery
implementation on top of MonetDB within the "Pathfinder" project.
@
@* Module Definition 
@m
.MODULE pf_support;

.USE malalgebra;
.USE lock;
.USE monettime;
.USE streams;

@- XML shredder (shredder.mx)
@m
.COMMAND shred_url(BAT[str,bat], str url, lng percentage, lock l, bit verbose) : void = CMDshred_url;
"PARAMETERS:
url - document locates at this uri is shredded. 
percentage - a number [0,100] indicating the amount of free space to reserve for updates. percentage=0 leads to a read-only document!" 

.COMMAND shred_str(BAT[str,bat], str buffer, lng percentage, lock l, bit verbose) : void = CMDshred_str;
"PARAMETERS:
buffer - the XML string to shred. 
percentage - a number [0,100] indicating the amount of free space to reserve for updates. percentage=0 leads to a read-only document!" 

.COMMAND shred_stream(BAT[str,bat], Stream s, lng percentage, lock l, bit verbose) : void = CMDshred_stream;
"PARAMETERS:
s - XML input stream. 
percentage - a number [0,100] indicating the amount of free space to reserve for updates. percentage=0 leads to a read-only document!" 


@- XML print functions (serialize.mx)
@m
.COMMAND print_doc(str,BAT[void,bat], str) : void = xquery_print_doc_main;
 "C interface to Workset print routine"

.COMMAND print_result(str mode, str moduleNS, str method,
                      BAT[oid,bat] ws,
                      BAT[oid,any] loop, BAT[void,oid] iter,
                      BAT[void,oid] item, BAT[void,int] kind, 
                      BAT[oid,lng] int_values, BAT[oid,dbl] dbl_values, 
                      BAT[oid,str] str_values) :
                            void = xquery_print_result_loop;
 "C interface to Workset result print routine, that can print multiple iters"

.COMMAND print_result(str mode, BAT[void,bat] ws,
                      BAT[void,oid] item, BAT[void,int] kind,
                      BAT[void,lng] int_values, BAT[void,dbl] dbl_values,
                      BAT[void,dbl] dec_values, BAT[void,str] str_values) :
                            void = xquery_print_result_main;
 "C interface to Workset result print routine"

.COMMAND print_result(str file, str mode, BAT[void,bat] ws,
                      oid item, int kind,
                      BAT[void,lng] int_values, BAT[void,dbl] dbl_values,
                      BAT[void,dbl] dec_values, BAT[void,str] str_values) :
                            void = xquery_print_result_file;
 "C interface to Workset result print routine, but print to a file"



@- Multi Join
@m
.COMMAND mposjoin( BAT[oid,oid] pre, BAT[oid,oid] cont, BAT[oid, bat] ws_item )
		: BAT[void,any] = CMDmposjoin;
"PARAMETERS:
BAT[oid,oid] - the values which have to be looked up
BAT[oid,oid] - the corresponding containers in which the values have to be looked up
BAT[oid,BAT[oid,any]] - the list of bats where the values are looked up
All BAT-heads must be dense.
DESCRIPTION:
looks up the values in a list of bats. The first argument specifies the
value to be looked up (joined) and the second one saves which bat contains
the value (see also 'mvaljoin').
The result is a bat with the tail values from the batlist (any) and the same
void column like the first two arguments"

.COMMAND mvaljoin( BAT[oid,oid] pre, BAT[oid,oid] cont, BAT[oid, bat] ws_item )
		: BAT[oid,oid] = CMDmvaljoin;
"PARAMETERS:
BAT[oid,oid] - the values which have to be joined
BAT[oid,oid] - the corresponding containers in which the values have to be looked up
BAT[oid,BAT[oid,oid]] - the list of bats where the tail values are joined
All BAT-heads must be dense.
DESCRIPTION:
joins the tail values of the first argument with the tail values of the bat
from one bat of the third argument. Which bat from the list is chosen is
specified by the second argument (see also 'mposjoin').
The result is a bat with the head values of the first two arguments in the head
and the head values from the batlist"


@- new algebraic primitives
@m
.COMMAND merged_union( any left, any right, ..any.. )
		: BAT[void,BAT] = CMDmerged_union;
"PARAMETERS:
Even number of BAT[oid,any] with dense heads and pairs of equal tail type;
all odd BATs must be head-aligned and all even BATs must be head-aligned;
first two BATs must be sorted on tail values.
DESCRIPTION:
Merges pairs of bats according to the order as defined by the first pair's tails."
@(
"Returns the union of two *tail-sorted* BATs. All BUNs of both BATs appear
 in the result, i.e., duplicates are not eliminated. As opposed to standard
 "union", the sortedness on the tail column is maintained in the result and
 all BUNs in the result appear in the same order as in their respective
 input."
@)

.COMMAND ll_tokenize( BAT[void,str] strs, BAT[void,str] seps )
		: BAT[oid,str] = CMDll_strSplit;
"PARAMETERS:
BAT[void,str] - the strings which are going to be tokenized
BAT[void,str] - separators for the strings
DESCRIPTION:
ll_tokenize is a special version of [split](bat(void,str),bat(void,str)), which
tokenizes a string using the aligned separator string and adds a row for
each substring, with the oid of the input head identifying the original boundaries."

.COMMAND round_up(dbl x) : dbl = math_unary_up_ROUND;
"PARAMETERS:
dbl - the argument to round
DESCRIPTION:
round_up rounds to 0 digits after the point. The only difference to
round(dbl, 0) is that negative numbers with .5 are rounded up 
(e.g., round(-1.5) = -1.0)."

.COMMAND normSpace(str string)
		: str = CMDnormSpace;
"PARAMETERS:
BAT[void,str] - the strings which are going to be normalized
DESCRIPTION:
normSpace normalizes the whitespaces to a single space."

.COMMAND combine_text_string( BAT[void,oid] iter, BAT[void,oid] kind, BAT[void,str] str_value, int result_size )
		: BAT[oid,str] = CMDcombine_text_string;
"PARAMETERS:
BAT[void,oid] - iter values which have to be ordered
BAT[void,oid] - one of three different kinds (strings '2@0'; text-nodes '1@0' and other nodes '0@0')
BAT[void,str] - string values of the strings and text-nodes; an empty string for each other node
DESCRIPTION:
very specialized helper function for the translation of the item-sequence-to-node-sequence function in XQuery.
It expects three aligned columns and creates one string out of adjacent strings and text-nodes,
which can be translated into a text-node again. Every other node and every new iter divide the strings.
A space is inserted if two strings are adjacent, and no space is inserted if a text-node is in between.
Empty strings are not added to the output."

.COMMAND string_join( BAT[oid,str] iter_str, BAT[oid,str] separator )
		: BAT[oid,str] = CMDstring_join;
"PARAMETERS:
BAT[oid,str] - sorted iters in the head, string values in the tail
BAT[oid,str] - separator which is added between strings, within an iter
DESCRIPTION:
string_join constructs for each iter one strings by appending the strings
with the separator for this iter in between."

.COMMAND enumerate( BAT[void,lng] startval, BAT[void,lng] length) : BAT[oid,lng] = CMDenumerate;
"PARAMETERS:
BAT[void,oid] - list of start values
BAT[void,oid] - list of length values
DESCRIPTION:
enumerate creates for each input row #length rows with the values beginning
at startval and increasing by one with every row."

.COMMAND ebv(BAT[oid, bit] b) : BAT[oid, bit] = CMDebv;
"PARAMETERS
BAT[oid, bit], sorted on head values.
DESCRIPTION:
This is a helper function to implement the XQuery ``effective boolean\n
value.'' Grouped by the head values, it will look at the tail values.\n
If for a head value there is exactly one BUN whose tail equals `true',\n
the result for this group will be `true' as well. In any other case\n
(a single `false' BUN, or multiple BUNs for this group), a `false'\n
tuple will be in the result for this group."

.COMMAND invalid_qname(BAT[any,str]) : str = CMDinvalid_qname;
"PARAMETERS
BAT[any, str] 
DESCRIPTION:
This is a helper function that tries to find an invalid qname
string. It returns the first invalid one it sees or str(nil) 
if all are ok." 

.COMMAND lastmod_time(str filename) : timestamp = CMDlastmod_time;
 "return the last modification time of a file"

.COMMAND correct_sizes(bat[void,oid] iter, bat[void,oid] item, bat[void,int] size)
                      : bat[void,int] = CMDcorrect_sizes;
 "correct subtree sizes when copying subtrees;
  silently modifies third argument (size-BAT) in place."

@- Constant Columns

In XQuery, we often have the case that BAT[void,T] all contain the same tail value
'constant columns'. In this case, we represent the column variable by a MIL
constant. The new 'constant' MIL module mimics the BAT algebra on such constants.

Regrettably, we cannot always represent constants as single MIL (non-bat) values.
Constants sometimes *need* to go into bats-of-bats. Particular cases in XQuery are 
the working set BAT[void,BAT] that contains a set of columns, and the result of staircase 
join (ditto). 

Our solution here is to represent constant columns inside a bat of bats as a 'fake' 
BAT[void,T] with a single entry [nil,c].  The routines constant2bat/bat2constant switch
between the MIL constant and the 'fake' BAT[nil,c] representation (obviously, non-constant
bats are unaffected by these calls). The switching is done automatically when 
bats fetched from a bat-of-bats (using an overloaded fetch) and put into them (with
an overloaded insert).
@m
.COMMAND is_constant(any) : bit = CMDisFakeProject;
"checks whether the parameter is a BAT[nil,c] constant column"

.COMMAND constant2bat(any col) : bat[any,any] = CMDfakeProject;
"if a column is a MIL constant c, change it into a BAT[nil,c] constant column."

.COMMAND bat2constant(any col) : any = CMDdeFakeProject;
"convert BAT[nil,c] constant column into a simple MIL constant 'c'"

.COMMAND fetch(BAT[any,bat] b, int pos) : any = CMDfetchConvert;
"fetches a bat from a bat-of-bats, and if it is a fake-bat converts it to a MIL constant"

.COMMAND insert(BAT[any::1,bat] b, any::1 head, any tail) : BAT[any::1,bat] = CMDinsertConvert;
"insert into a bat-of-bats; if a non-bat tail is inserted, it is inserted as a fake-bat."

.COMMAND append(BAT[any::1,bat] b, any tail) : BAT[any::1,bat] = CMDappendConvert;
"append into a bat-of-bats; if a non-bat tail is appended, it is appended as a fake-bat."

@- Path steps (staircase join) 
@m
@:prec_foll(following)@
@:prec_foll(preceding)@

@= prec_foll
.COMMAND @1_void (BAT[void,int] pre_size,
               BAT[oid,any] ctx,
               BAT[oid,oid] doc_pre,
               int upperbound) : BAT[oid,void] = PF@1_void;
"PARAMETERS
  pre_size: the complete size BAT (preorder rank, size)
  ctx: context node sequence (preorder rank, *)
  doc_pre: table of document containers (doc id, preorder start value)
  upperbound: upperbound for size of result context node sequence
DESCRIPTION
axis step evaluation on the @1 axis from the given context."
@m
@:scj_cmd(descorself,descendant-or-self)@
@:scj_cmd(desc,descendant)@
@:scj_cmd(ancorself,ancestor-or-self)@
@:scj_cmd(anc,ancestor)@

@= scj_cmd
.COMMAND scj_@1(BAT[void,int] pre_size,
                BAT[oid,any] ctx,
                int upperbound): BAT[oid,void] = PFscj_@1_void;
"PARAMETERS
  pre_size: the complete size BAT (preorder rank, size)
  ctx: context node sequence (preorder rank, *)
  upperbound: upperbound for size of result context node sequence
DESCRIPTION
axis step evaluation on the @2 axis from the given context."
@m
@:ll_cmd(descendant)@
@:ll_cmd(descendant_or_self)@
@:ll_cmd(child)@

@= ll_cmd
.COMMAND ll_@1(BAT[oid,oid] iter,
                   BAT[oid,oid] ctx,
                   BAT[oid,int] pre_size,
                   BAT[void,any] cands,
                   bit one_iter, bit one_ctx,
                   oid min_iter, oid max_iter,
                   bit no_iter_order, chr kind_test): BAT[oid,oid] = PFll_@1;
"PARAMETERS:
BAT[void,oid] iter (grouping relation; sorted on tail within each ctx group)
BAT[void,oid] ctx (context set; sorted on tail)
BAT[void,int] pre_size (from the working set)
BAT[oid,oid]  cands (sorted list of result candidate OIDs in the tail)
bit           one_iter (only one iter?)
bit           one_ctx  (only one ctx node, i.e., the same for all iters?)
oid           min_iter,max_iter (smallest and largest iter id)
bit           no_iter_order (descendant & descendant_or_self, only: 
               result will be ordered on item, but not sub-ordered on iter)
DESCRIPTION:
returns all nodes on the @1 axis of the ctx-nodes duplicate free for each group."
@m
@:ll_upwards(parent,parent)@
@:ll_upwards(ancestor,ancestor)@
@:ll_upwards(ancestor_or_self,ancestor-or-self)@

@= ll_upwards
.COMMAND ll_@1(BAT[oid,oid] iter,
                   BAT[oid,oid] ctx,
                   BAT[oid,int] pre_size, BAT[oid,chr] pre_level): BAT[oid,oid] = PFll_@1;
"PARAMETERS:
BAT[void,oid] iter (grouping relation; sorted on tail within each ctx group)
BAT[void,oid] ctx (context set; sorted on tail)
BAT[void,int] pre_size (from the working set)
BAT[void,int] pre_level (from the working set)
DESCRIPTION:
returns all nodes on the @2 axis of the ctx-nodes duplicate free for each group."
@m
@:lev_cmd(child,child)@
@:lev_cmd(parent,parent)@
@:lev_cmd(fs,following-sibling)@
@:lev_cmd(ps,preceding-sibling)@

@= lev_cmd
.COMMAND lev_@1(BAT[void,chr] pre_level,
                BAT[oid,any] ctx,
                int upperbound) : BAT[oid,void] = PFlev_@1;
"PARAMETERS
  pre_level: the complete level BAT (preorder rank, level)
  ctx: context node sequence (preorder rank, *)
  upperbound: upperbound for size of result context node sequence
DESCRIPTION
axis step evaluation on the @2 axis from the given context."

@- update primitives
@m
.COMMAND delete_nodes_prepare_pre_size (BAT[void,int] pre_size, BAT[oid,void] pre_)
	: BAT[oid,int] = CMDdelete_nodes_prepare_pre_size;
"PARAMETERS:
 BAT[void,int] pre_size
 BAT[oid,void] pre_ (pre-order ranks of nodes to be deleted)
RESULT:
 BAT[oid,int] pre-order ranks and 'fake-size' of nodes to be deleted
DESCRIPTION:
 Calculates new 'fake-sizes' of nodes to be deleted;
 optimizes 'holes' in pre_size."

@- string primitives
@m
.COMMAND splitbat(BAT[oid,str], str sep) : BAT[oid,BAT] = CMDSplitBat;
"Split the strings in the BAT on the separator sep which may not be empty.
Returns a BAT with as many BATs as the maximum number of substrings.
Each of those BATs is the same size as the input BAT and contains either
the next substring or str(nil) if there are no more substrings."

.COMMAND splitkind(BAT[oid,chr]) : BAT[oid,BAT] = CMDSplitKind;
"Split a \"kind\" BAT into separate BATs.
Returns a BAT of 5 BATs where the ith BAT is like the result of a uselect(i),
but does this in a single scan."

.COMMAND cleantmpdir(lng lim) : void = CMDcleantmpdir;
"quietly delete all files older than 'lim' (unix time) from the tmp dir (dbfarm/dbname/tmp)"

@- bit manipulations
@= nilarithMIL
.COMMAND nil@1(int v1, int v2) : int = CMDnil@1;
"Compute @1 on the two arguments, viewing nil as just another bit pattern."
@m
@:nilarithMIL(and)@
@:nilarithMIL(or)@
@:nilarithMIL(plus)@

@- transaction locking 
@m
.COMMAND pflock_get(int i) : lock = CMDpflock_get;
 "provide a pointer to a global lock"
.COMMAND pflock_begin(lng wsid) : void = CMDpflock_begin;
 "let a query (rdonly,docmgmt,update) enter the system"
.COMMAND pflock_end(lng wsid) : void = CMDpflock_end;
 "a query (rdonly,docmgmt,update) leaves the system.
  NOTE: you *must* have the short lock when executing this!!"
.COMMAND pflock_meta(lng wsid) : void = CMDpflock_meta;
 "alert that query (rdonly,docmgmt,update) reads meta information"
.COMMAND pflock_free(bit persistent) : bit = CMDpflock_free;
 "return false if there is an active document mgt query.
  NOTE: you *must* have the short lock when executing this!!"

.PRELUDE = pf_support_prelude;
.EPILOGUE = pf_support_epilogue;

.END pf_support;

@mil
module("xtables");
module("aggrX3");
module("bat_arith");
module("mmath");
module("pcre");
module("malalgebra");
module("monettime");
module("lock");
module("logger");

# global constants using in the MIL translation
const empty_kind_bat := bat(void,int,0).seqbase(0@0).rename("empty_kind_bat").access(BAT_READ);
const EMPTY_STRING := 0@0;
const OID_PAGE_BITS := isnil(lng(1LL << 32).oid()).ifthenelse(10,9);
const empty_dbl__bat := bat(void,dbl,0).seqbase(0@0).rename("empty_dbl__bat").access(BAT_READ);
const empty_dec__bat := empty_dbl__bat;
const empty_str__bat := bat(void,str,0).seqbase(0@0).rename("empty_str__bat").access(BAT_READ);
const empty_int__bat := bat(void,lng,0).seqbase(0@0).rename("empty_int__bat").access(BAT_READ);
const bool_not := bat(void,oid,2).append(1@0).append(0@0).rename("bool_not").seqbase(0@0).access(BAT_READ);
const bool_str := bat(void,str,2).append("false").append("true").rename("bool_str").seqbase(0@0).access(BAT_READ);

# nil constants (saves some run-time casting)
const bit_nil       := bit(nil);
const chr_nil       := chr(nil);
const int_nil       := int(nil);
const lng_nil       := lng(nil);
const dbl_nil       := dbl(nil);
const oid_nil       := oid(nil);
const str_nil       := str(nil);
const stream_nil    := Stream(nil);
const lock_nil      := lock(nil);
const sema_nil      := sema(nil);
const timestamp_nil := timestamp(nil);

PROC log2(any i) : int { return int(log(dbl(i))/M_LN2) + 1; }

PROC addValues(bat[void,any::1] container, any::1 delta) : oid
{
    container.append(delta);
    return container.reverse().find(delta);
}

PROC addValues(bat[void,any::1] container, bat[oid,any::1] delta) : bat[oid,oid]
{
    container.append(delta);
    return delta.leftjoin(container.reverse());
}

PROC mposjoin( any pre, oid cont, BAT[oid, bat] ws_item ) : BAT[void,any] 
{
    return pre.leftfetchjoin(bat2constant(ws_item.find(cont)));
}

PROC mvaljoin( any pre, oid cont, BAT[oid, bat] ws_item ) : BAT[oid,oid] 
{
    return pre.leftjoin(ws_item.find(cont).reverse());
}

PROC tmark_unique( any::1 col, BAT[void,any] ipik) : BAT[oid,oid]
{
    return bat(void,oid).append(reverse(ipik).fetch(0)).seqbase(0@0).access(BAT_READ);
}

PROC tmark_unique( BAT[oid,any] col, BAT[oid,any] ipik) : BAT[oid,oid]
{
    return tmark(kunique(reverse(col)),0@0);
}

PROC tmark_grp_unique( any iter, BAT[oid,any] ipik) : BAT[oid,oid]
{
    return ipik.mark(1@0);
}

PROC tmark_grp_unique( BAT[any,any] iter, BAT[oid,any] ipik) : BAT[oid,oid]
{
    if (is_constant(iter)) return ipik.mark(1@0);
    return iter.mark_grp(iter.tunique(), 1@0);
}

PROC is_type (int kind, int type_) : bit
{
        return kind.and(63) = type_;
}

PROC get_type (bat[void,int] kind, int type_) : bat[oid,void]
{
        return kind.[and](63).ord_uselect(type_); # 63 = 2^6 - 1
}

PROC get_type_node (bat[void,int] kind) : bat[oid,void]
{
        return kind.ord_uselect(NODE, int_nil);
}

PROC get_type_atomic (bat[void,int] kind) : bat[oid,void]
{
        return kind.ord_uselect(int_nil, ATOMIC);
}

PROC get_container (int kind) : oid
{
        return kind.>>(6).oid();
}

PROC get_container (bat[oid,int] kind) : bat[oid,oid]
{
        return kind.[>>](6).[oid]();
}

PROC get_container (bat[void,int] kind) : bat[void,oid]
{
        return kind.[>>](6).[oid]();
}

PROC set_kind (oid cont, int type_) : int
{
        return cont.int().<<(6).or(type_);
}

PROC set_kind (bat[void,oid] cont, int type_) : bat[void,int]
{
        return cont.[int]().[<<](6).[or](type_);
}

PROC get_types (bat[void,int] kind) : bat[void,int]
{
        return kind.[and](63); # 63 = 2^6 - 1
}

PROC correct_sizes (oid iter, bat[void,oid] item, bat[void,int] size) : bat[void,int]
{
	return correct_sizes (constant2bat(iter), item, size);
}

@- get_root
We now have frag_root populated for all XML document collections (previously only
the transient document container). Finding the root of an item entails establishing
the largest root PRE that is smaller than that item. In the general case, we need
a thetajoin for that (root < item) with a per-item {max} aggregate.

The thetajoin can get quickly out of hand in MonetDB, so we actually do a 
chunked thetajoin A.K.A. blocked nested-loops (by hand, by taking slices).
@mil
PROC replace_root(BAT[void,oid] result, BAT[oid,oid] frag_item, BAT[oid,oid] frag_frag) : void
{
        # MonetDB bogus: avoid materialized cartesian product in thetajoin using blocked nested loop
        var delta := 100000 / count(frag_frag); # generate 100K intermediate tuples max
        var cur := 0, end := count(frag_item);
        while(cur < end) {
                var batch := frag_item.slice(cur, cur+delta);
                result.replace(leftthetajoin(batch, frag_frag, GE).{max}(), true);
                result.replace(batch.ord_select(0@0), true); # get_root() on collection super-root (0@0) returns self
                cur :+= delta + 1;
        }
}

PROC get_root(BAT[oid,bat] ws, oid item, int kind, oid cont) : oid
{
        if (item = 0@0) return 0@0; # get_root() on collection super-root (0@0) returns self
        var nid_rid   := ws.fetch(NID_RID).fetch(cont);
        var map_pid   := ws.fetch(MAP_PID).fetch(cont);
        var frag_root := ws.fetch(FRAG_ROOT).fetch(cont).tmark(0@0).leftfetchjoin(nid_rid).[swizzle](map_pid);

        if (kind.is_type(ATTR)) { # convert attributes to a pre
                item := ws.fetch(ATTR_OWN).fetch(cont).find(item);
                item := nid_rid.find(item).swizzle(map_pid);
        }
        item := max(frag_root.select(oid_nil,item));
        return item;
}

PROC get_root(BAT[oid,bat] ws, BAT[void,oid] item, int kind, oid cont) : BAT[void,oid]
{
        if ((count(item) = 1)) {
        	# short-cut to avoid some extra work...
		var root := bat(void,oid).seqbase(seqbase(item));
		root.append(get_root(ws, item.fetch(0), kind, cont));
                return root.access(BAT_READ);
        }

        var seqbase := item.seqbase();
        var nid_rid   := ws.fetch(NID_RID).fetch(cont);
        var map_pid   := ws.fetch(MAP_PID).fetch(cont);
        var frag_root := ws.fetch(FRAG_ROOT).fetch(cont).tmark(0@0).leftfetchjoin(nid_rid).[swizzle](map_pid);

        if (kind.is_type(ATTR)) { # convert attributes to a pre
                item := item.leftfetchjoin(ws.fetch(ATTR_OWN).fetch(cont));
                item := item.leftfetchjoin(nid_rid).[swizzle](map_pid);
                item := item.tmark(seqbase);
        }

        var result := item.copy().access(BAT_WRITE);
        replace_root(result, item, mirror(reverse(frag_root)));
        return result.access(BAT_READ).tmark(seqbase);
}

PROC get_root(BAT[oid,bat] ws, BAT[void,oid] item, BAT[void,int] kind, BAT[void,oid] cont) : BAT[void,oid] 
{
        if ((count(kind) = 1) and (count(cont) = 1)) {
        	# short-cut avoid some extra work...
                return get_root(ws, item, kind.fetch(0), cont.fetch(0));
        }

        var seqbase := item.seqbase();
        var result := item.copy().access(BAT_WRITE);

        var attr_cont := kind.get_type(ATTR).mirror().leftfetchjoin(cont);

        # look up root fragment
        var conts := cont.tunique();
        conts@batloop() {
                var nid_rid   := ws.fetch(NID_RID).fetch($h);
                var map_pid   := ws.fetch(MAP_PID).fetch($h);
                var frag_root := ws.fetch(FRAG_ROOT).fetch($h).tmark(0@0).leftfetchjoin(nid_rid).[swizzle](map_pid);
                var frag_item := cont.ord_uselect($h).mirror().leftfetchjoin(result);
                var attr_item := attr_cont.ord_select($h);
                if (count(attr_item) > 0) {
                        attr_item := attr_item.mirror().leftfetchjoin(item);
                        attr_item := attr_item.leftfetchjoin(ws.fetch(ATTR_OWN).fetch($h));
                        attr_item := attr_item.leftfetchjoin(nid_rid).[swizzle](map_pid);
                        result.replace(attr_item, true);
                }
                var frag_item := cont.ord_uselect($h).mirror().leftfetchjoin(result);
                replace_root(result, frag_item, mirror(reverse(frag_root)));
                var frag_item := cont.ord_uselect($h).mirror().leftfetchjoin(result);
        }
        return result.access(BAT_READ).tmark(seqbase);
}

PROC get_attr_own(BAT[void,bat] ws, any nids, any cont) : BAT[oid,oid]
{
    var private := mvaljoin(nids, cont, ws.fetch(ATTR_OWN_PRIVATE));
    var shared := mvaljoin(nids, cont, ws.fetch(ATTR_OWN_SHARED));
    var cnt_p := count(int(private));
    var cnt_s := count(int(shared));
    if (cnt_s = 0LL) return private; # private is all we got.

    var hshared := shared.hmark(0@0);
    var tshared := shared.tmark(0@0);
    shared := shared.fetch(mposjoin(tshared, hshared.leftfetchjoin(cont), ws.fetch(ATTR_OWN)).ord_uselect(oid_nil,oid_nil));
    cnt_s := count(int(shared));

    if (cnt_s = 0LL) return private; # private is all we got.
    if (cnt_p = 0LL) return shared; # shared is all we got.

    # merge-union shared and private
    var res := merged_union(shared.hmark(0@0), private.hmark(0@0), shared.tmark(0@0), private.tmark(0@0));
    return reverse(res.fetch(0)).leftfetchjoin(res.fetch(1));
}


PROC fn_put(BAT[oid,bat] ws, BAT[void,str] uri, BAT[void,oid] item, any kind,
            BAT[void,lng] int_values, BAT[void,dbl] dbl_values,
            BAT[void,dbl] dec_values, BAT[void,str] str_values) : void
{
    var protocol := [search](uri, "://");
    var sel := protocol.uselect(0,int_nil).hmark(0@0).leftfetchjoin(uri);
    var isfile := [or]([startsWith](sel, "file://"), [startsWith](sel, "FILE://"));
    var filepath := protocol.uselect(-1).hmark(0@0).leftfetchjoin(uri).access(BAT_WRITE);

    isfile := isfile.uselect(true).hmark(0@0).leftfetchjoin(sel);
    sel := isfile.copy().access(BAT_WRITE).append(filepath).access(BAT_READ);
    filepath.append([string](isfile,7));

    if (count(sel) != count(uri)) { # we only support file url's for the moment
       var nonfileurl := tdiff(uri, sel);
       ERROR("fn_put: URI '%s' not recognized (only file URLs are supported; %d such errors).", wrong.fetch(0), count(nonfileurl));
    }
    var absolute := [or](filepath.[startsWith]("/"), filepath.[startsWith]("\\")).uselect(true).mirror().leftfetchjoin(sel);
    if (count(absolute) > 0) {
       ERROR("fn_put: file URI '%s' must be a relative path (%d such errors).", backwards.fetch(0), count(backwards));
    }
    var backwards := filepath.[search]("..").uselect(0,int_nil).mirror().leftfetchjoin(sel);
    if (count(backwards) > 0) {
       ERROR("fn_put: file URI '%s' must is not allowed to contain '..' (%d such errors).", backwards.fetch(0), count(backwards));
    }
    [print_result](sel, "xml-noroot", 
                   const ws, item, kind, 
                   const int_values, 
                   const dbl_values, 
                   const dec_values, 
                   const str_values);
}

# print whitespaces (used for indenting)
PROC pws (int shift) : void
{
    while (shift > 0) {
        printf("  ");
        shift :-= 1;
    }
}

# main trace routine (returns an XML document representing the trace)
#
# The generated XML document should be aligned to the following DTD
# <!DOCTYPE scope [
# 
# <!ELEMENT scope (iteration*)>
# <!ELEMENT iteration (scope*, trace*)>
# <!ELEMENT trace (item*)>
# <!ELEMENT item ANY>
# 
# <!ATTLIST scope id CDATA #REQUIRED>
# 
# <!ATTLIST iteration no CDATA #REQUIRED>
# 
# <!ATTLIST trace msg CDATA #REQUIRED>
# <!ATTLIST trace id CDATA #REQUIRED>
# 
# <!ATTLIST item type (integer|string|decimal|
#                      double|boolean|qname|
#                      node|attribute) #REQUIRED>
# <!ATTLIST item pos CDATA #REQUIRED>
# <!ATTLIST item id CDATA #IMPLIED>
# <!ATTLIST item fragment CDATA #IMPLIED>
# <!ATTLIST item name CDATA #IMPLIED>
# <!ATTLIST item value CDATA #IMPLIED>
# 
# ]>
PROC trace (BAT[void,any] ws,
            BAT[oid,bat] trace_outer,
            BAT[oid,bat] trace_inner,
            BAT[oid,bat] trace_iter,
            BAT[oid,bat] trace_msg,
            BAT[oid,bat] trace_item,
            BAT[oid,int] trace_type,
            BAT[oid,oid] trace_rel) : void
{
    # shift/indent counter
    var s := 0;
    # initial trace id
    var init_id := 0@0;
    # initial outer relation
    var init_outer_val := 1@0;
    
    pws (s); printf("<scope id=\"%i\">\n", init_id); s :+= 1;
    pws (s); printf("<iteration no=\"%i\">\n", init_outer_val);  s :+= 1;

    # first print all trace information in nested scopes ...
    trace_scopes (ws,
                  trace_outer,
                  trace_inner,
                  trace_iter,
                  trace_msg,
                  trace_item,
                  trace_type,
                  trace_rel,
                  s,
                  init_id,
                  init_outer_val);
    # ... and then print the trace information for this scope.
    trace_ops (ws,
               trace_iter,
               trace_msg,
               trace_item,
               trace_type,
               trace_rel,
               s,
               init_id,
               init_outer_val);
           
    s :-= 1; pws (s); printf("</iteration>\n");
    s :-= 1; pws (s); printf("</scope>\n");
}

# worker for PROC trace. It recursively prints the structure of
# the scopes and initiates the printing of trace information (trace_ops).
PROC trace_scopes (BAT[void,any] ws,
                   BAT[oid,bat] trace_outer,
                   BAT[oid,bat] trace_inner,
                   BAT[oid,bat] trace_iter,
                   BAT[oid,bat] trace_msg,
                   BAT[oid,bat] trace_item,
                   BAT[oid,int] trace_type,
                   BAT[oid,oid] trace_rel,
                   int s, oid id, oid cur_outer) : void
{
    var rels := trace_rel.reverse().select(id).mirror();
    var outer_rels := trace_outer.reverse().leftjoin(rels).reverse();
    var inner_rels := trace_inner.reverse().leftjoin(rels).reverse();

    var index1 := 0;
    outer_rels@batloop () {
        pws (s); printf("<scope id=\"%i\">\n", $h); s :+= 1;

        var next_id := $h;
        var inner_rel := inner_rels.fetch(index1);
        var outer := $t.select(cur_outer);
        var inner := outer.mirror().leftjoin(inner_rel);
        
        # iterate over the tuples of the outer|inner relation
        inner@batloop () {
            pws (s); printf("<iteration no=\"%i\">\n", $t);  s :+= 1;
            
            # first print all trace information in nested scopes ...
            trace_scopes (ws,
                          trace_outer,
                          trace_inner,
                          trace_iter,
                          trace_msg,
                          trace_item,
                          trace_type,
                          trace_rel,
                          s,
                          next_id,
                          $t);
            # ... and then print the trace information for this scope.
            trace_ops (ws,
                       trace_iter,
                       trace_msg,
                       trace_item,
                       trace_type,
                       trace_rel,
                       s,
                       next_id,
                       $t);
                   
            s :-= 1; pws (s); printf("</iteration>\n");
        }
        
        s :-= 1; pws (s); printf("</scope>\n");
        index1 :+= 1;
    }
}

# worker for PROC trace. It prints all trace operators. A worker
# (trace_item) prints the item sequences that correspond to the trace
# operators and the current iteration.
PROC trace_ops (BAT[void,any] ws,
                BAT[oid,bat] trace_iter,
                BAT[oid,bat] trace_msg,
                BAT[oid,bat] trace_item,
                BAT[oid,int] trace_type,
                BAT[oid,oid] trace_rel,
                int s, oid id, oid cur_inner) : void
{
    var rels := trace_rel.reverse().select(id).mirror();
    var msg_rels := trace_msg.reverse().leftjoin(rels).reverse();

    # iterate over the available trace operators ...
    msg_rels@batloop () {
        # ... and pick the current iteration
        var msg_row := $t.reverse().select(cur_inner).reverse();
        if (msg_row.count() != 0) {
            pws (s); printf("<trace msg=\"%s\" id=\"%i\">\n",
                            msg_row.fetch(0),
                            $h); s :+= 1;
            
            var iter := trace_iter.reverse().select($h).reverse().fetch(0);
            if (iter.seqbase() != 0@0) {
                printf ("\n");
                ERROR ("Trace printing failed: wrong seqbase in trace_iter");
            }
            # Also make sure that we only use values from the current iteration
            var iter_sel := iter.select(cur_inner);
            var iter_count := iter_sel.count();
            if (iter_count != 0) {
                var item_set := trace_item.reverse().select($h).reverse();
                var types := trace_type.reverse().select($h).reverse();
                var iter_offset := iter_sel.reverse().min().int();
                # generate an item sequence
                trace_items (ws, item_set, types, iter_offset, iter_count);
            }

            s :-= 1; pws (s); printf("</trace>\n");
        }
    }
}

# worker for PROC trace_ops. It prints an item sequence starting
# from ``offset'' until ``offset + count''. For nodes, attributes,
# and qnames special printing is performed.
PROC trace_items (BAT[void,any] ws,
                  BAT[oid,bat] item_set,
                  BAT[oid,int] types,
                  int offset,
                  int count) : void
{
    var item_set_count := item_set.count();
    var i := 0;
    # iterate over the tuples that are in the current iteration
    while (i < count) {
        pws (s); printf("<item pos=\"%i\"", i + 1);

        # print item value
        var j := 0;
        var node_id := oid(nil);
        var frag := oid(nil);
        var kind;
        # iterate over the list of item relations
        # (to cope with polymorphic sequences)
        while (j < item_set_count) {
            var type_id := types.fetch(j);
            var item := item_set.fetch(j).fetch(i + offset);
            var type_str;
            # the types are aligned with the aat_* types in
            # struct PFalg_simple_type_t (compiler/include/algebra.h)
            if (type_id = 2) {
                type_str := "integer";
            } else if (type_id = 4) {
                type_str := "string";
            } else if (type_id = 8) {
                type_str := "decimal";
            } else if (type_id = 16) {
                type_str := "double";
            } else if (type_id = 32) {
                type_str := "boolean";
                item := (item = 1@0);
            } else if (type_id = 64) {
                type_str := "QName";
            } else if (type_id = 128) {
                type_str := "untypedAtomic";
            } else if (type_id = 256) {
                type_str := "node";
                frag := item;
            } else if (type_id = 512) {
                type_str := "node";
                node_id := item;
                kind := ELEM;
            } else if (type_id = 1024) {
                type_str := "attribute";
                frag := item;
            } else if (type_id = 2048) {
                type_str := "attribute";
                node_id := item;
                kind := ATTR;
            } else {
                printf("\n");
                ERROR ("unknown type (%i) appears in "
                       + "trace output generation",
                       type_id);
            }
            # we have found the correct type -- so generate output for it
            if (not(isnil (item))) {
                # qnames
                if (type_id = 64) {
                    printf (" type=\"%s\"", type_str);
                    var name := trace_qname_uri (item, WS);
                    printf (">%s", name);
                    break;
                # all primitive types
                } else if (and (isnil (node_id), isnil (frag))) {
                    printf (" type=\"%s\">", type_str);
                    printf ("%s", item.str());
                    break;
                # nodes
                } else if (and (not (isnil (node_id)),
                                not (isnil (frag)))) {
                    printf (" type=\"%s\"", type_str);
                    printf (" id=\"%i\"", node_id);
                    printf (" fragment=\"%i\"", frag);
                    if (kind = ELEM) {
                        # element, text, comment, pi, and doc nodes
                        printf (">");
                        print_result ("xml-noroot-noheader",
                                      ws,
                                      new (void,oid).seqbase(0@0)
                                                    .append(node_id),
                                      new (void,oid).seqbase(0@0)
                                                    .append(frag)
                                                    .set_kind(kind),
                                      new (void,lng).seqbase(0@0),
                                      new (void,dbl).seqbase(0@0),
                                      new (void,dbl).seqbase(0@0),
                                      new (void,str).seqbase(0@0));
                    } else {
                        # attribute nodes
                        var qn_id := ws.fetch(ATTR_QN)
                                       .fetch(frag)
                                       .reverse()
                                       .select(node_id)
                                       .reverse()
                                       .fetch(0);
                        var qn_frag := ws.fetch(ATTR_CONT)
                                         .fetch(frag)
                                         .reverse()
                                         .select(node_id)
                                         .reverse()
                                         .fetch(0);
                        var prop := ws.fetch(ATTR_PROP)
                                      .fetch(frag)
                                      .reverse()
                                      .select(node_id)
                                      .reverse()
                                      .fetch(0);
                        var cont := ws.fetch(ATTR_CONT)
                                      .fetch(frag)
                                      .reverse()
                                      .select(node_id)
                                      .reverse()
                                      .fetch(0);
                        var val := ws.fetch(PROP_VAL)
                                     .fetch(cont)
                                     .reverse()
                                     .select(prop)
                                     .reverse()
                                     .fetch(0);
                        var name := trace_qname_uri (qn_id, qn_frag);
                        printf (" name=\"%s\" value=\"%s\">", name, val);
                    }
                    break;
                }
            }
            j :+= 1;
        }
            
        printf("</item>\n");
        i :+= 1;
    }
}

# worker for PROC trace_ops. It looks up the qname values
# and prints a namespace attribute (as side effect) if a prefix
# is used. The return value is the string representation of the
# qname.
PROC trace_qname_uri (oid item, oid frag) : str
{
    var local := ws.fetch(QN_LOC)
                   .fetch(frag)
                   .reverse()
                   .select(item)
                   .reverse()
                   .fetch(0);
    var prefix := ws.fetch(QN_PREFIX)
                    .fetch(frag)
                    .reverse()
                    .select(item)
                    .reverse()
                    .fetch(0);
    var uri := ws.fetch(QN_URI)
                 .fetch(frag)
                 .reverse()
                 .select(item)
                 .reverse()
                 .fetch(0);
    if (and (prefix = "", uri = "")) {
        return local;
    } else if (prefix = "") {
        printf (" xmlns:prefix=\"%s\"",
                uri);
        return "prefix:" + local;
    } else {
        printf (" xmlns:%s=\"%s\"",
                prefix, uri);
        return prefix + ":" + local;
    }
    break;
}


@- order-preserving xquery join with existential semantics

The below proc handles xquery theta-joins between l[oid,any::1] and r[oid,any::1]
(iter,value) combinations with a join predicate PRED = { LT, LE, EQ, GE, GT }.

The result are the [oid,oid] iter numbers that should be ordered on head, and within 
head on tail (hence the term 'htordered') to preserve proper xquery sequence order.

We may also get optional lx[void,any] and rx[void,any] bats that substitute the
head and tail result oid-s for something else (backmapping). These lx/rx are not always 
present though.  They are pushed inside the thetajoin when sampling indicates the result 
will become big, or when the any types are smaller than oid-s (thus always make the 
result smaller).  In this case it is better to go into the join with the substitution 
already performed.  In other cases, the  substitution is done after computing the join.

The join is *existential*, that is, each [iter_l,iter_r] is unique in the result,
and should be there iff there exists any [iter_l,v_l] and [iter_r,v_r] for which 
(v_l PRED v_r) holds. 

For equi-joins, we compute the full join and then eliminate
duplicate [oid,oid] combinations. Given the htordered result characteristic, this will 
use a merge-algorithm. However for <,>,<=,>= we even *avoid* duplicates by joining {min}(l) 
with {max}(r) for <,<= (and vice versa for >,>=). Note  that these {min}() and {max}() 
are also efficient as they also can use a merge-algorithm for aggregation.

For non-equi-joins, use use either sort-merge or nested-loop join. Note that the latter join 
does not require any reordering to achieve htordered-ness. This is a plus, but nested-loop is 
of course quadratic in complexity. On the other hand, non-equi-joins tend to be quadratic 
in their result size anyway. We check this using run-time sampling. Nested-loop is chosen if 
sampling indicates a large join result.

Finally, the theta-join is loop-lifted, in that it may be executed between two loop-lifted
expressions. In that case, only matching outer iterations (lo and ro) should be joined on value. 
In effect, it requires a join condition "(l_val PRED r_val) and (lo = ro)". As MonetDB cannot 
handle such multi-column theta-joins efficiently, the current solution exploits the fact
that the loop-lifted iterations appear contiguous in both inputs. Thus, each iteration
is a slice of both input BATs. We just go through all outer iterations, take slices, and
execute the join for corresponding iter slices on l and r; concatenating all results.
@mil
var pf_sample256 := bat(void,lng,256LL).rename("pf_sample256").seqbase(0@0); 
{ var i := 0LL; while(i < 256LL) { pf_sample256.append(i); i :+= 1LL; } } 

PROC htordered_unique_thetajoin( int mode,
                                 bat[oid,any::1] l, bat[oid,any::1] r,
                                 any lx, any rx) : bat[any,any]
{
    var lcnt := int(l).count();
    var rcnt := int(r).count();
    if ((lcnt = 0LL) or (rcnt = 0LL))
        return bat(oid,oid,1LL).access(BAT_READ);

    if (not(ordered(l)) or not(ordered(r)))
        ERROR("htordered_unique_thetajoin(): ordered left and right head columns (iters) expected.\n");

    if (rcnt = 1LL) {
        # it is a selection; not a join
        var v := r.fetch(0);
        var t := r.reverse().fetch(0);
        var lo := (mode >= EQ).ifthenelse(v,cast(nil,ttype(l)));
        var hi := (mode <= EQ).ifthenelse(v,cast(nil,ttype(l)));
        var sel;
        if ((mode = GT) or (mode = LT)) {
            sel := l.ord_select(lo,hi).[!=](v).ord_uselect(true);
        } else {
            sel := l.ord_uselect(lo,hi);
        }
        if (type(lx) = bat) sel := reverse(reverse(sel).leftfetchjoin(lx));
        if (type(rx) = bat) t := rx.find(t);
        return sel.project(t);
    }
    if (lcnt = 1LL) {
        return reverse(htordered_unique_thetajoin(-(mode), r, l, rx, lx));
    }
    if (mode != EQ) {
        # try to reduce footprint, first:
        # join with lx/rx only if smaller than l/r
        if (type(lx) = bat) {
            if (htype(lx) <= sht) { l := reverse(leftfetchjoin(reverse(l),lx)); lx := nil; }
        }
        if (type(rx) = bat) {
            if (htype(rx) <= sht) { r := reverse(leftfetchjoin(reverse(r),rx)); rx := nil; }
        }
        # trick: as we eliminate double matches anyway, let's not generate them to start with
        # pumps are efficient because merge-based
        if ((mode = GT) or (mode = GE)) {
            l := {max}(l);
            r := {min}(r);
        } else {
            l := {min}(l);
            r := {max}(r);
        }
        var samp := [oid]([*](pf_sample256, int(l).count()/256LL)).leftfetchjoin(l.reverse().mark(0@0).reverse());
        var res := nlthetajoin(samp, reverse(r), mode, int(r).count() * 64LL);
        if (((2LL * batsize(res) * int(l).count()) / (1LL + int(samp).count())) > mem_maxsize()) {
            var cnt := int(res).count(); res := nil;
            # if not done, yet, join with lx/rx
            if (type(lx) = bat) l := reverse(leftfetchjoin(reverse(l),lx));
            if (type(rx) = bat) r := reverse(leftfetchjoin(reverse(r),rx));
            # a large intermediate result is better handled with nested loop (no reordering necessary)
            return nlthetajoin(l, reverse(r), mode, ((cnt+cnt) / int(samp).count())*int(r).count());
        }
    }
    var join_order := leftthetajoin(l, reverse(r), mode);

    # avoid error (lng(max(bat(oid,oid))) = nil)
    if (join_order.count() = 0) { return join_order; }

    var snd_iter := join_order.reverse().mark(0@0).reverse();
    var fst_iter := join_order.mark(0@0).reverse();
    var sorting := fst_iter.CTrefine(snd_iter); # this may hurt
    if (lng(max(sorting)) != int(sorting).count()) {
        # the output of CTrefine allows to easily check if it is kunique
        sorting := sorting.reverse().kunique().reverse(); # merge-based kunique
    }
    join_order := join_order.fetch(sorting); # this may hurt as well
    # joins with lx/rx can only be done after the CTrefine() and kunique()
    if (type(lx) = bat) join_order := reverse(leftfetchjoin(reverse(join_order),lx));
    if (type(rx) = bat) join_order := leftfetchjoin(join_order,rx);
    return join_order;
}

var lng_oid := ifthenelse(isnil(oid(LNG_MAX)),lng,oid);

# loop-lifted variant; lo and ro are the outer iteration numbers that should match
PROC ll_htordered_unique_thetajoin( int mode,
                                    bat[oid,any::2] l, bat[oid,any::2] r,
                                    bat[void,oid] lo, bat[void,oid] ro,
                                    any lx, any rx) : bat[any,any]
{
    var lo_histo := histogram(lo), lr_histo := reverse(lo_histo);
    var ro_histo := histogram(ro), rr_histo := reverse(ro_histo);
    var li := 0LL, lp := 0LL, lc := count(int(lo_histo));
    var ri := 0LL, rp := 0LL, rc := count(int(ro_histo));
    var b := new(void,bat,min(lc,rc));
    var tpe := ttype(l);

    if (not(reverse(lo).ordered()) or not(reverse(ro).ordered()))
        ERROR("htordered_unique_thetajoin(): ordered left and right outer columns (iters) expected.\n");

    # trivial case; not handled below as log2() cannot cope with max() of an empty BAT being NIL
    if ((lc = 0LL) or (rc = 0LL)) {
        return bat(oid, oid, 0).access(BAT_READ);
    }

    # in case of integer equi-join, we shift iter and value together in a lng and do a single join on that
    if ((mode = EQ) and or(or((tpe = int), (tpe = oid)), (tpe = lng)))  {
        var iter_max := 32;
        var combine := true;
        if ((tpe = lng) or (lng_oid = lng)) {
            iter_max := log2(lng(max(max(lr_histo),max(rr_histo))));
            combine := (iter_max + log2(lng(max(max(l),max(r))))) < 64;
        }
        if (combine) {
            lo := [lng](lo).access(BAT_WRITE); 
            ro := [lng](ro).access(BAT_WRITE); 
	    [:+=](lo, [<<]([lng](l.tmark(0@0)), iter_max)).access(BAT_READ);
	    [:+=](ro, [<<]([lng](r.tmark(0@0)), iter_max)).access(BAT_READ);
	    l := l.mark(0@0).leftfetchjoin(lo); 
	    r := r.mark(0@0).leftfetchjoin(ro); 
            return htordered_unique_thetajoin(EQ, l, r, lx, rx);
        }
    }

    # otherwise iterate over the outer scopes; execute the join only for a single outer iter at a time
    while((lp < lc) and (rp < rc)) {
        var lv := lr_histo.fetch(lp);
        var rv := rr_histo.fetch(rp);
        var ln := lng(lo_histo.fetch(lp));
        var rn := lng(ro_histo.fetch(rp));
        if (lv = rv) {
            # only join corresponding slices of l an r
            var bn := htordered_unique_thetajoin(mode, l.slice(li,li+ln-1), r.slice(ri,ri+rn-1), lx, rx);
            if (count(int(bn)) > 0LL) b.append(bn);
        }
        if (lv <= rv) { lp :+= 1; li :+= ln; }
        if (rv <= lv) { rp :+= 1; ri :+= rn; }
    }

    # concatenate all results
    if (count(int(b)) = 1LL) return b.fetch(0);
    var bn := bat(oid, oid, sum_lng([count]([int](b)))).access(BAT_WRITE);
    if (count(int(b)) > 0LL) [insert](const bn,b);
    return bn.access(BAT_READ);
}

@- loop-lifted staircase join
@mil
#############################################
# MIL WRAPPER for AXIS STEPS 
#
# In order to simplify the invocation of the axis steps functions, this
# interface provides...
# 

@(
@:step(descendant,descendant,scj_desc,,@:sizes_code@,,)@
@:step(descendant_or_self,descendant-or-self,scj_descorself,,@:sizes_code@,,)@
@)

@:wrap(descendant)@
@:wrap(descendant_or_self)@

@(
@:step(parent,parent,lev_parent,@:level_intro@,@:level_code@,,)@
@:step(ancestor,ancestor,scj_anc,,@:sizes_code@,,)@
@:step(ancestor_or_self,ancestor-or-self,scj_ancorself,,@:sizes_code@,,)@
@)

@:upwards(parent)@
@:upwards(ancestor)@
@:upwards(ancestor_or_self)@

@(
@:step(child,child,lev_child,@:level_intro@,@:level_code@,,)@
@)

@:wrap(child)@

@:step(following_sibling,following-sibling,lev_fs,@:level_intro@,@:level_code@,,)@
@:step(preceding_sibling,preceding-sibling,lev_ps,@:level_intro@,@:level_code@,,)@
@:step(following,following,following_void,,@:sizes_code@, @:foll_prec_code@,@:doc_pre@)@
@:step(preceding,preceding,preceding_void,,@:sizes_code@, @:foll_prec_code@,@:doc_pre@)@

@= chk_order
	if ( and(order,1) = @2 ) {
		@3 := @3.chk_order(); # just in case...
	}
@= one_iter_many_items
	# 1 iter, n items
	one_iter := TRUE;
	@:chk_order(@1@2,0,item)@
	# =>  we do not need to sort the input
	order := or(order,1);
@= many_iters_one_item
	# n iters, 1 item 
	one_item := TRUE;
	@:chk_order(@1@2,1,iter)@
	# =>  we do not need to sort the input
	order := or(order,1);
@= unique_iters
		# unique iters
                var unq_items;
                unq_items := item.reverse().kunique();
                if ((item.count() - unq_items.count()) > 1000) {
                        unq_iter := TRUE;
                        unq_iters := iter.reverse()
                                         .leftfetchjoin(item)
                                         .leftjoin(unq_items.mark(0@0));
                        item := unq_items.mark(0@0).reverse();
                        iter := item.mirror();
                        # we now certainly have iter|item ordering
                        order := and(order,2);
                }
@= resolve_unique_iters
                # undo memoization and remap iter values
                if (unq_iter) {
                    # we assume descendant and child steps ALWAYS
                    # return their result in iter|item order
                    #
                    # Otherwise we need to correct the ordering if not already present
                    # if (not (ordered (res))) {
                    #     var ord := res.mark(0@0).sort().reverse();
                    #         ord := ord.CTrefine(res.reverse().mark(0@0).reverse())
                    #                   .mark(0@0).reverse();
                    #     iter := ord.leftfetchjoin(res.mark(0@0).reverse());
                    #     item := ord.leftfetchjoin(res.reverse().mark(0@0).reverse());
                    #     res := iter.reverse().leftfetchjoin(item);
                    # }
                    res := unq_iters.lefthashjoin(res.access(BAT_WRITE).revert());
                    res.chk_order();
                }
@= wrap
PROC @1 (BAT[oid,oid] iter, BAT[oid,oid] item, oid cont, BAT[oid,bat] ws, int order, BAT[void,any] cands, chr kind_test) : BAT[void,bat]
{
	var result := nil;
	var one_iter := FALSE;
	var one_item := FALSE;
	var unq_iter := FALSE;
        var unq_iters := nil;
	var min_iter := oid_nil;
	var max_iter := oid_nil;
	var skip_self := 1;
	if ( "@1" = "descendant_or_self" ) {
		skip_self := 0;
	}

	# check consistency
	if ( isnil(seqbase(iter)) or isnil(seqbase(item)) ) {
		ERROR("@1(0): heads of iter & item/ctx must not be NIL!");
	}
	if ( (count(iter) != count(item)) or (seqbase(iter) != seqbase(item)) ) {
		ERROR("@1(1): heads of iter & item/ctx must be aligned (count(iter)="+str(count(iter))+", count(item)="+str(count(item))+", seqbase(iter)="+str(seqbase(iter))+", seqbase(item)="+str(seqbase(item))+") !");
	}
	@:chk_order(@1(3),0,iter)@
	@:chk_order(@1(4),1,item)@
	cands := cands.chk_order(); # just in case...

	var pre_size := ws.fetch(PRE_SIZE).fetch(cont);

	# trivial cases	
	if ( (count(item) = 0) or (count(cands) = 0) or (count(pre_size) = 0) ) {
		result := new(void,bat,2).seqbase(0@0)
		          .append(bat(void,oid,0).seqbase(0@0).access(BAT_READ))
		          .append(bat(void,oid,0).seqbase(0@0).access(BAT_READ))
		          .access(BAT_READ);
		return result;
	}
	
	# special cases
	if ( count(item) = 1 ) {
		# 1 iter, 1 item
		one_iter := TRUE;
		one_item := TRUE;
		# =>  we do not need to sort neither input nor output
		order := 3;
		iter := iter.chk_order(); # just in case...
		item := item.chk_order(); # just in case...
	} else {
	# first: try cheap min==max checks on ordered columns
	if ( ordered(reverse(iter)) and (min(iter) = max(iter)) ) {
		@:one_iter_many_items(@1,(6))@
	} else {
	if ( ordered(reverse(item)) and (min(item) = max(item)) ) {
		@:many_iters_one_item(@1,(7))@
	} else {
	# then: invest in one scan to check order to save two scans for min/max
	iter := iter.chk_order(); # just in case...
	if ( ordered(reverse(iter)) and (min(iter) = max(iter)) ) {
		@:one_iter_many_items(@1,(8))@
	} else {
	item := item.chk_order(); # just in case...
	if ( ordered(reverse(item)) and (min(item) = max(item)) ) {
		@:many_iters_one_item(@1,(9))@
	} else {
	if ( ordered(reverse(iter)) and (key(reverse(iter)))) {
		@:unique_iters()@
	}}}}}}

	min_iter := min(iter);
	max_iter := max(iter);

	# pre-sort input
	if ( and(order,1) = 0 ) {
		var ord := item.tsort();
		    ord := ord.CTrefine(iter).mark(0@0).reverse();
		iter := ord.leftfetchjoin(iter);
		item := ord.leftfetchjoin(item);
		iter := iter.chk_order();
		item := item.chk_order();
	}

	# the actual location step
	if ( isnil(result) ) {
		var res := ll_@1 (iter, item, pre_size, cands, one_iter, one_item, 
		                   min_iter, max_iter, (and(order,2) = 0), kind_test);
                               
                @:resolve_unique_iters()@

		result := new(void,bat,2).seqbase(0@0)
		          .append(res.mark(0@0).reverse())
		          .append(res.reverse().mark(0@0).reverse())
		          .access(BAT_READ);
	}

	# post-sort output
	if ( (and(order,2) = 2) and not(ordered(reverse(result.fetch(1)))) ) {
		iter := result.fetch(0);
		item := result.fetch(1);
		var ord := item.tsort();
		    ord := ord.CTrefine(iter).mark(0@0).reverse();
		result := new(void,bat,2).seqbase(0@0)
		          .append(ord.leftfetchjoin(iter).chk_order())
		          .append(ord.leftfetchjoin(item).chk_order())
		          .access(BAT_READ);
	}
	
	# post-sort output
	if ( (and(order,2) = 0) and not(ordered(reverse(result.fetch(0)))) ) {
		iter := result.fetch(0);
		item := result.fetch(1);
		var ord := iter.tsort();
		    ord := ord.CTrefine(item).mark(0@0).reverse();
		result := new(void,bat,2).seqbase(0@0)
		          .append(ord.leftfetchjoin(iter).chk_order())
		          .append(ord.leftfetchjoin(item).chk_order())
		          .access(BAT_READ);
	}
	
	return result;
}
ADDHELP("@1", "tsheyar", "Sep 2004",
"PARAMETERS:\n\
BAT[void,oid] iter (grouping relation)\n\
BAT[void,oid] item (context set)\n\
oid cont (the current container of the ws)\n\
BAT[void,bat] ws (working set)\n\
int order (input & output order properties:\n\
           bit 0: input is sorted on iter(0) or item(1)\n\
           bit 1: output must be sorted on iter(0) or item(1))\n\
BAT[oid,oid] cands (sorted list of result candidate OIDs in the tail)\n\
DESCRIPTION:\n\
returns all nodes on the @1 axis of the ctx-nodes duplicate free for each group.",
"pf_support");
@
# use size concept
@= sizes_code
pre_sizes
@
# use level concept
@= level_intro
    var pre_levels := ws.fetch(PRE_LEVEL).fetch(cont);
@
@= level_code
pre_levels
@
# code for following and preceding steps
@= foll_prec_code
    # find the document root-pre's for prec_foll
    var nid_rid := ws.fetch(NID_RID).fetch(cont);
    var map_pid := ws.fetch(MAP_PID).fetch(cont);
    var doc_pre := ws.fetch(FRAG_ROOT).fetch(cont).tmark(0@0).leftfetchjoin(nid_rid).[swizzle](map_pid);
@
@= doc_pre
doc_pre,
@

@= step
PROC @1(BAT[void,oid] iter, BAT[void,oid] ctx, oid cont, BAT[void,bat] ws, int order ) : BAT[void,bat]
{
    # "order" is not (yet?) used, here.

    var pre_sizes := ws.fetch(PRE_SIZE).fetch(cont);
    @4

    var unq := {count}(iter.reverse(), iter.tunique(),FALSE);

    var offset := 0;
    var ctx_slice;

    @6

    var res_iter, res_item;
    if (count(unq) = 1) {
        res_iter := constant2bat(unq.reverse().fetch(0));
	    ctx_slice := ctx.reverse().project(nil);
	    ctx_slice := ctx_slice.chk_order(); # have to check because step expects it sorted
        res_item := @3( @5, ctx_slice, @7 pre_sizes.count()).hmark(0@0);
        res_item := res_item.chk_order();
    } else {
        res_iter := bat(void,oid);
        res_item := bat(void,oid);
        unq@batloop () {
            ctx_slice := ctx.slice(offset, offset + $t - 1).reverse().project(nil);
            ctx_slice := ctx_slice.chk_order(); # have to check because step expects it sorted
            var res := @3( @5, ctx_slice, @7 pre_sizes.count()).reverse();
            res := res.chk_order();
            res_item.append(res);
            var l := count(res);
            if (l <= $t) {
                res_iter.append(iter.slice(offset, offset + l - 1));
            } else {
                res_iter.append(res.project($h));
            }
            offset := offset + $t;
        }
        res_iter := res_iter.seqbase(0@0).access(BAT_READ).chk_order();
        res_item := res_item.seqbase(0@0).access(BAT_READ).chk_order();
    }
    return new(void,bat,2).seqbase(0@0)
           .append(res_iter)
           .append(res_item)
           .access(BAT_READ);
}
ADDHELP("@1", "tsheyar", "Sep 2004",
"PARAMETERS:\n\
BAT[void,any] iter (grouping relation)\n\
BAT[void,any] ctx (context set)\n\
oid cont (the current container of the ws)\n\
BAT[void,bat] ws (working set)\n\
DESCRIPTION:\n\
returns all nodes on the @2 axis of the ctx-nodes duplicate free for each group.",
"pf_support");

@= upwards
PROC @1(BAT[void,oid] iter, BAT[void,oid] ctx, oid cont, BAT[void,bat] ws, int order) : BAT[void,bat]
{
    # "order" is not (yet?) used, here.
    var pre_sizes := ws.fetch(PRE_SIZE).fetch(cont);
    var pre_levels := ws.fetch(PRE_LEVEL).fetch(cont);
    var res := ll_@1(iter.chk_order(), ctx.chk_order(), pre_sizes, pre_levels);
    return new(void,bat,2).seqbase(0@0).append(hmark(res,0@0)).append(tmark(res,0@0)).access(BAT_READ);
}
@

@mil
@:loop_lifted_scj_step1(ancestor)@
@:loop_lifted_scj_step1(ancestor_or_self)@

@:loop_lifted_scj_wrap1(child)@
@:loop_lifted_scj_wrap1(descendant)@
@:loop_lifted_scj_wrap1(descendant_or_self)@

@:loop_lifted_scj_step1(parent)@
@:loop_lifted_scj_step1(following)@
@:loop_lifted_scj_step1(following_sibling)@
@:loop_lifted_scj_step1(preceding)@
@:loop_lifted_scj_step1(preceding_sibling)@
@
#==================================================================
# expansions of the loop lifted scj
# kind argument
@= kind_args
, chr kind
@
@= kind_params
, kind
@
# tagname argument
@= ns_args
, str ns
@
@= ns_params
, ns
@
@= loc_args
, str loc
@
@= loc_params
, loc
@
@= tgt_args
, str tgt
@
@= tgt_params
, tgt
@
@= nsloc_args
, str ns, str loc
@
@= nsloc_params
, ns, loc
@
@= params2
, cands, kind_test
@= postfilter
if (postfilter) {
	var pre_cont := ws.fetch(PRE_CONT).fetch(contID);
	var pre_kind := ws.fetch(PRE_KIND).fetch(contID);
	var pre_prop := ws.fetch(PRE_PROP).fetch(contID);
	if (is_constant(pre_cont)) {
                # determine a sel [qnid,qnid] of qualifying nslocs (i.e. prop IDs)
		var sel := ws.fetch(@1).fetch(bat2constant(pre_cont)).ord_uselect(@2);

                # first join on prop, getting some false hits for non ELEMENT nodes
                tmp_res := result_part_item.leftfetchjoin(pre_prop).leftjoin(sel).hmark(0@0);

                # remove false hits
                tmp_res := tmp_res.leftfetchjoin(result_part_item).leftfetchjoin(pre_kind).ord_uselect(@3).hmark(0@0).leftfetchjoin(tmp_res);
        } else {
                # the cont value refers back to multiple different containers (XPath step in transient doc container)

                # first select the element nodes
                var X := result_part_item.leftfetchjoin(pre_kind).ord_uselect(@3).hmark(0@0);

                # fetch cont and prop values
                tmp_res := X.leftfetchjoin(result_part_item);
                var X_cont  := tmp_res.leftfetchjoin(pre_cont);
                var X_prop  := tmp_res.leftfetchjoin(pre_prop);

                # get qnames using mposjoin from the source containers
		var X_nsloc := mposjoin(X_prop, X_cont, ws.fetch(@1));

                # final select
		tmp_res := X_nsloc.ord_uselect(@2).hmark(0@0).leftfetchjoin(X);
        } 
	@:mapping_code@
}
@= mapping_code
	result_part_iter := tmp_res.leftfetchjoin(result_part_iter);
	result_part_item := tmp_res.leftfetchjoin(result_part_item);
	tmp_res := nil;
@= nsloc_post
      { var nsloc := ns + NS_ACCEL_SEP + loc; @:postfilter(QN_URI_LOC,nsloc,ELEMENT)@ }
@= loc_post
        @:postfilter(QN_LOC,loc,ELEMENT)@
@= ns_post
        @:postfilter(QN_URI,ns,ELEMENT)@
@= target_post
        @:postfilter(PROP_TGT,tgt,PI)@
@= kind_post
        if (postfilter) {
      		var pre_kind := ws.fetch(PRE_KIND).find(contID);
               	tmp_res := result_part_item.leftfetchjoin(pre_kind).ord_uselect(@1).hmark(0@0);
	       	@:mapping_code@
        }
@= no_pre
	cands := ws.fetch(PRE_SIZE).fetch(contID).mirror(); # no selection: cands is everything
@= kind_pre
      { cands := ws.fetch(PRE_KIND).fetch(contID); kind_test := kind; }
@= nsloc_pre
      { var nsloc := ns + NS_ACCEL_SEP + loc; @:prefilter(nsloc,QN_URI_LOC)@ }
@= loc_pre
      { @:prefilter(loc,QN_LOC)@ }
@= ns_pre
      { @:prefilter(ns,QN_URI)@ }
@= prefilter
	       cands := ws.fetch(PRE_SIZE).fetch(contID).mirror();
	var pre_cont := ws.fetch(PRE_CONT).fetch(contID);
	if (is_constant(pre_cont) and (count(result_part_item) > 2048)) {
print(count(result_part_item));
print(count(cands));
		var qn_sel := ws.fetch(@2).fetch(bat2constant(pre_cont)).ord_uselect(@1);
                if (isnil(CATCH(cands := ws_lookup(ws, contID, qn_sel.mirror())))) {
	        	var map_pid  := ws.fetch(MAP_PID).fetch(contID);
                        var isolate := (ttype(map_pid) = oid);
                        if (isolate) {
	        	    var nid_rid  := ws.fetch(NID_RID).fetch(contID);
                	    cands := cands.leftfetchjoin(nid_rid);
                	    cands := cands.[swizzle](map_pid); 
                	    cands := cands.tsort(); 
                        }
	                if ( (count(cands) > 0) and (count(result_part_item) > 0) ) {
                                var min_cand := min(cands);
                                var min_item := min(result_part_item);
                                if ( min_cand < min_item ) {
                                        cands := cands.ord_select(min_item,oid_nil);
                                }
                        }
                        if ( (count(cands) > 0) and (count(result_part_item) > 0) ) {
                                var max_cand := max(cands);
                                var max_item := max(result_part_item);
                                if ( max_cand < max_item ) {
                                        tmp_res := result_part_item.ord_uselect(oid_nil,max_cand).hmark(0@0);
                                        @:mapping_code@
                                }
                        }
                	postfilter := false; # we have a true candidate list
                }
       } # else we use postfilter (after SCJ)

# expanding the scj for the different tests
@= loop_lifted_scj_wrap1
@:loop_lifted_scj_wrap_pre(@1)@
@
@= loop_lifted_scj_wrap_pre
@:loop_lifted_scj_step2(@1,,,,,                                                               @:params2@,@:no_pre@)@
@:loop_lifted_scj_step2(@1,_with_kind_test,  @:kind_args@, @:kind_params@,,                   @:params2@,@:kind_pre(kind)@)@
@:loop_lifted_scj_step2(@1,_with_nsloc_test, @:nsloc_args@,@:nsloc_params@,@:nsloc_post@,     @:params2@,@:nsloc_pre@)@
@:loop_lifted_scj_step2(@1,_with_ns_test,    @:ns_args@,   @:ns_params@,   @:ns_post@,        @:params2@,@:ns_pre@)@
@:loop_lifted_scj_step2(@1,_with_loc_test,   @:loc_args@,  @:loc_params@,  @:loc_post@,       @:params2@,@:loc_pre@)@
@:loop_lifted_scj_step2(@1,_with_target_test,@:tgt_args@,  @:tgt_params@,  @:target_post@,    @:params2@,@:no_pre@)@
@
@= loop_lifted_scj_wrap_post
@:loop_lifted_scj_step2(@1,,,,,                                                               @:params2@,@:no_pre@)@
@:loop_lifted_scj_step2(@1,_with_kind_test,  @:kind_args@, @:kind_params@, @:kind_post(kind)@,@:params2@,@:no_pre@)@
@:loop_lifted_scj_step2(@1,_with_nsloc_test, @:nsloc_args@,@:nsloc_params@,@:nsloc_post@,     @:params2@,@:no_pre@)@
@:loop_lifted_scj_step2(@1,_with_ns_test,    @:ns_args@,   @:ns_params@,   @:ns_post@,        @:params2@,@:no_pre@)@
@:loop_lifted_scj_step2(@1,_with_loc_test,   @:loc_args@,  @:loc_params@,  @:loc_post@,       @:params2@,@:no_pre@)@
@:loop_lifted_scj_step2(@1,_with_target_test,@:tgt_args@,  @:tgt_params@,  @:target_post@,    @:params2@,@:no_pre@)@
@
@= loop_lifted_scj_step1
@:loop_lifted_scj_step2(@1,,,,,,                                                               )@
@:loop_lifted_scj_step2(@1,_with_kind_test,  @:kind_args@, @:kind_params@, @:kind_post(kind)@,,)@
@:loop_lifted_scj_step2(@1,_with_nsloc_test, @:nsloc_args@,@:nsloc_params@,@:nsloc_post@     ,,)@
@:loop_lifted_scj_step2(@1,_with_ns_test,    @:ns_args@,   @:ns_params@,   @:ns_post@        ,,)@
@:loop_lifted_scj_step2(@1,_with_loc_test,   @:loc_args@,  @:loc_params@,  @:loc_post@       ,,)@
@:loop_lifted_scj_step2(@1,_with_target_test,@:tgt_args@,  @:tgt_params@,  @:target_post@    ,,)@
@
#==================================================================
# actual definition of the scj proc
@= loop_lifted_scj_per_cont
	result_part_iter := result_part_iter.chk_order();
	result_part_item := result_part_item.chk_order();
        result_part_cont := nil;

	# pre-test
	@4

        var result := @1 (result_part_iter, result_part_item, contID, ws, order @3);
	result_part_iter := result.fetch(0);
	result_part_item := result.fetch(1);
        result_part_cont := constant2bat(contID);
	cands := nil;

	# post-test
	@2
@
@= loop_lifted_scj_step2
PROC loop_lifted_@1_step@2 (bat[void, oid] iter, bat[void, oid] item, bat[void, oid] cont, bat[void, bat] ws @3) : bat[void,bat]
{
     return loop_lifted_@1_step@2 (iter, item, cont, ws, 0 @4);
}
PROC loop_lifted_@1_step@2 (bat[void, oid] iter, bat[void, oid] item, bat[void, oid] cont, bat[void, bat] ws, int order @3) : bat[void,bat]
{
    # handle empty results correctly
    if (iter.count() = 0) {
	var empty := bat(void,oid,0).seqbase(0@0).access(BAT_READ);
	return bat(void,bat,3).seqbase(0@0)
			    .append(empty)
			    .append(empty)
			    .append(empty)
			    .access(BAT_READ);
    }

    var result;
    var result_iter;
    var result_item;
    var result_cont;
    var tmp_res;
    var cands;
    var kind_test := chr_nil;
    var postfilter := true;

    var uniqueCont := cont.tunique().sort();
    var contID := uniqueCont.reverse().fetch(0);
    if (uniqueCont.count() = 1) {
        var result_part_cont := oid_nil;
        var result_part_iter := iter;
        var result_part_item := item;

        @:loop_lifted_scj_per_cont(@1,@5,@6,@7)@

        result_iter := result_part_iter;
        result_item := result_part_item;
        result_cont := result_part_cont;
        result_part_iter := nil;
        result_part_item := nil;
        result_part_cont := nil;
    } else {
        var result_part_cont := cont.ord_uselect(contID).hmark(0@0);
        var result_part_iter := result_part_cont.leftfetchjoin(iter);
        var result_part_item := result_part_cont.leftfetchjoin(item);

        @:loop_lifted_scj_per_cont(@1,@5,@6,@7)@

        result_iter := result_part_iter;
        result_item := result_part_item;
        result_cont := result_part_cont;
        result_part_iter := nil;
        result_part_item := nil;
        result_part_cont := nil;

        var res_mu;
	uniqueCont.slice(1,uniqueCont.count() - 1)@batloop () {
	    contID := $h;
            result_part_cont := cont.ord_uselect(contID).hmark(0@0);
            result_part_iter := result_part_cont.leftfetchjoin(iter);
            result_part_item := result_part_cont.leftfetchjoin(item);

            @:loop_lifted_scj_per_cont(@1,@5,@6,@7)@

            if ( and(order,2) = 2 )
	    {
	         res_mu := merged_union(result_item, result_part_item,
	             		   result_iter, result_part_iter,
	             		   result_cont, result_part_cont);
                 result_part_iter := nil;
                 result_part_item := nil;
                 result_part_cont := nil;
                 result_item := res_mu.fetch(0);
                 result_iter := res_mu.fetch(1);
                 result_cont := res_mu.fetch(2);
                 res_mu := nil;
	    }
	    else
	    {
	         res_mu := merged_union(result_iter, result_part_iter,
	             		   result_item, result_part_item,
	             		   result_cont, result_part_cont);
                 result_part_iter := nil;
                 result_part_item := nil;
                 result_part_cont := nil;
                 result_iter := res_mu.fetch(0);
                 result_item := res_mu.fetch(1);
                 result_cont := res_mu.fetch(2);
                 res_mu := nil;
	    }
        }
    }
    
    result_iter.access(BAT_READ);
    result_item.access(BAT_READ);
    result_cont.access(BAT_READ);
    var result_scj := bat(void,bat,3).seqbase(0@0);
    result_scj.append(result_iter);
    result_scj.append(result_item);
    result_scj.append(result_cont);

    return result_scj.access(BAT_READ);
}
@


@- update primitives
@mil

# the UPDATE_INSERT_* and UPDATE_REPLACECONTENT commands MUST be
# consecutive, the UPDATE_INSERT_LAST and UPDATE_INSERT_BEFORE
# commands also MUST be consecutive
const UPDATE_INSERT_FIRST := 1LL;
const UPDATE_INSERT_LAST := 2LL;
const UPDATE_INSERT_BEFORE := 3LL;
const UPDATE_INSERT_AFTER := 4LL;
const UPDATE_REPLACECONTENT := 5LL;
const UPDATE_DELETE := 6LL;
const UPDATE_RENAME := 7LL;
const UPDATE_REPLACE := 8LL;

PROC myupdate(bat[any::1,any::2] oldbat, bat[any::1,any::2] newbat) : bat[any::1,any::2]
{
  if (newbat.count() = 0) {
    return oldbat;
  }
  if (oldbat.htype() = void) {
    var mx := newbat.reverse().max().wrd();
    var x;
    if (oldbat.count() = 0) {
      x := wrd(-1);
    } else {
      x := oldbat.reverse().max().wrd();
    }
    var n := cast(nil, oldbat.ttype());
    while (x < mx) {
      oldbat.append(n, true);
      x :+= 1;
    }
    oldbat := oldbat.replace(newbat, true);
  } else {
    if (ordered(oldbat)) newbat := newbat.chk_order().sort();
    var newvals := kdiff(newbat, oldbat);
    var replvals := kdiff(newbat, newvals);
    oldbat := oldbat.replace(replvals, true).insert(newvals);
  }
  return oldbat;
}
ADDHELP("myupdate", "sjoerd", "Apr 13 2006",
"Combination of insert and replace: insert new values and replace existing ones.",
"pf_support");

PROC myupdate(bat[any::1,any::2] oldbat, any::1 key, any::2 val) : bat[any::1,any::2]
{
  if (oldbat.htype() = void) {
    # we want to maintain the void head if at all possible
    # therefore we check whether the key already exists and do a
    # replace if it does
    if (not(oldbat.exist(key))) {
      var x;
      if (oldbat.count() = 0) {
        x := wrd(-1);
      } else {
        x := oldbat.reverse().max().wrd();
      }
      var xkey := wrd(key);
      var n := cast(nil, oldbat.ttype());
      while (x < xkey) {
        oldbat.append(n, true);
        x :+= 1;
      }
    }
    oldbat.replace(key, val, true);
  } else {
    oldbat.insert(key, val);
  }
  return oldbat;
}
ADDHELP("myupdate", "sjoerd", "Apr 13 2006",
"Combination of insert and replace: insert new values and replace existing ones.",
"pf_support");

PROC findupdate(BAT[any::1,any::2] oldbat, BAT[any::1,any::2] newbat, any::1 key): any::2
{
  if (newbat.exist(key)) {
    return newbat.find(key);
  } else {
    return oldbat.find(key);
  }
}

# convert a PRE value to a RID value using the given map
PROC antiswizzle(oid o, BAT[oid,oid] map) : oid
{
  var pid := oid(lng(o) >> REMAP_PAGE_BITS);
  var revmap := map.reverse();
  if (revmap.exist(pid))
    return oid(<<(lng(revmap.find(pid)),REMAP_PAGE_BITS) + and(lng(o),REMAP_PAGE_MASK));
  return o;
}

# Find the ancestors of a given node in a document that is being modified.
PROC mil_ancestor(bat[void,bat] ws, oid cont, oid newpre) : bat[void,oid]
{
  var ancestors := new(void, oid).seqbase(0@0); # the result
  var pre_size := ws.fetch(PRE_SIZE).find(cont);
  var rid_size := ws.fetch(_RID_SIZE).find(cont);
  var rid_size_update := ws.fetch(RID_SIZE_UPDATE).find(cont);
  var map_pid := ws.fetch(MAP_PID).find(cont);
  var map_pid_update := ws.fetch(MAP_PID_UPDATE).find(cont);
  var pid_map_update := map_pid_update.select(oid_nil, oid_nil).reverse().sort().tmark(0@0);

  var p := 0@0;
  extend_protect(ws, cont);
  while (p < newpre) {
    var pageid := pid_map_update.find(oid(lng(p) >> REMAP_PAGE_BITS));
    var isoldpage := false;
    if (map_pid.exist(pageid)) {
      isoldpage := not(isnil(map_pid.find(pageid)));
    }
    var r := swizzle(p, pid_map_update);
    var size;
    if (isoldpage) {
      if (rid_size_update.exist(r)) {
        size := rid_size_update.find(r);
      } else {
        size := pre_size.find(swizzle(r, map_pid));
      }
    } else {
      size := rid_size.find(r); # PETER: looks like this code path was untested
    }
    size := niland(size, INT_MAX);
    var e := oid(lng(p) + size);
    if (e >= newpre) {
      ancestors.append(p);
      p := oid(lng(p) + 1);
    } else {
      p := oid(lng(e) + 1);
    }
  }
  extend_unprotect(ws, cont);
  return ancestors;
}

# # Find the parent of a given node in a document that is being modified.
# # this function is called with extend_protect(ws, cont) set.
# PROC mil_parent(bat[void,bat] ws, oid cont, oid newpre) : oid
# {
#   var parent := oid_nil;	# the result
#   var pre_size := ws.fetch(PRE_SIZE).find(cont);
#   var rid_size := ws.fetch(_RID_SIZE).find(cont);
#   var rid_size_update := ws.fetch(RID_SIZE_UPDATE).find(cont);
#   var map_pid := ws.fetch(MAP_PID).find(cont);
#   var map_pid_update := ws.fetch(MAP_PID_UPDATE).find(cont);
#   var pid_map_update := map_pid_update.select(oid_nil, oid_nil).reverse().sort().tmark(0@0);

#   var p := 0@0;
#   while (p < newpre) {
#     var pageid := pid_map_update.find(oid(lng(p) >> REMAP_PAGE_BITS));
#     var isoldpage := false;
#     if (map_pid.exist(pageid)) {
#       isoldpage := not(isnil(map_pid.find(pageid)));
#     }
#     var r := swizzle(p, pid_map_update);
#     var size;
#     if (isoldpage) {
#       if (rid_size_update.exist(r)) {
#         size := rid_size_update.find(r);
#       } else {
#         size := pre_size.find(swizzle(r, map_pid));
#       }
#     } else {
#       size := rid_size.find(r); # PETER: looks like this code path was untested
#     }
#     size := niland(size, INT_MAX);
#     var e := oid(lng(p) + size);
#     if (e >= newpre) {
#       parent := p;
#       p := oid(lng(p) + 1);
#     } else {
#       p := oid(lng(e) + 1);
#     }
#   }
#   return parent;
# }

# Move data starting at "from" of length "size" by "delta".  The
# destination *must* be a hole (XXX check this).
# Delta can be positive (move data down) or negative (move data up).
PROC movedata(bat[void,bat] ws, oid cont, oid from, int size, int delta): void
{
  if (delta = 0)
    return;                     # not moving, so nothing to do

# The picture shows the layout of the tables and what the important
# variables represent.  When delta > 0, the data is moved down (and
# thus the hole is moved up).
#
#       delta > 0 (moving down)                delta < 0 (moving up)
#             |         |                            |         |
#             |         |                            |         |
# before ---->|         |                before ---->|         |
#             |---------|                            |---------|
# from ------>| ^       |<- newholefirst holefirst ->| ^       |
#             | |       |                            | |       |
#             | | size  |                            | | delta |
#             | |       |<- newholelast  holelast -->| |       |
#             | | - - - |                            |---------|
# last ------>| v       |                from ------>| ^       |
#             |---------|                            | | - - - |
# holefirst ->| |       |                            | |       |<- newholefirst
#             | | delta |                            | | size  |
#             | |       |                            | |       |
# holelast -->| v       |                last ------>| v       |<- newholelast
#             |---------|                            |---------|
# after ----->|         |                after ----->|         |
#             |         |                            |         |
#             |         |                            |         |
#
# The following areas are of interest:
# - data to be moved, this is the data with PRE values "from" to "last"
#   and given by the parameters "from" and "size";
# - hole to be overwritten, this is the hole after (if delta > 0) or
#   before (if delta < 0) the data to be moved with PRE values
#   "holefirst" to "holelast" and size "abs(delta)";
# - affected area, this is the combination of the above two areas;
# - the area before the affected area;
# - the area after the affected area.
#
# We distinguish the following nodes that each get their own treatment:
# - nodes in the area before the affected area that end inside the data
#   to be moved;
# - nodes in the area before the affected area that end inside the hole
#   to be overwritten;
# - nodes in the data to be moved that end in the hole to be overwritten
#   (only applicable if delta > 0);
# - nodes in the data to be moved that end in the area after the
#   affected area.

  var map_pid := ws.fetch(MAP_PID).find(cont);
  var map_pid_update := ws.fetch(MAP_PID_UPDATE).find(cont);

  # some helpful values
  var pageno := oid(lng(from) >> REMAP_PAGE_BITS); # page number where all this takes place
  var pageid := map_pid_update.reverse().find(pageno);
  ws.fetch(MODIFIED_PAGE).insert(cont, pageid); # we're modifying this page
  var isoldpage := false;
  if (map_pid.exist(pageid)) {
    isoldpage := not(isnil(map_pid.find(pageid)));
  }
  # we're using RID values exclusively in this function
  var from_rid := antiswizzle(from, map_pid_update);

# var delta;                    # displacement (< 0: up, > 0: down) (parameter)
  var holefirst;                # ID of start of hole
  var holelast;                 # ID of end of hole
  var before;                   # ID of last node before area being moved and hole
# var from;                     # ID of first node being moved (parameter)
  var last;                     # ID of last node being moved
# var size;                     # size of area being moved (parameter)
  var after;                    # ID of first node after area being moved and hole
  var newholefirst;             # ID if start of to-be-created hole
  var newholelast;              # ID if end of to-be-created hole

  last := oid((lng(from_rid) + size) - 1);
  if (delta < 0) {
    holefirst := oid(lng(from_rid) + delta);
    holelast := oid(lng(from_rid) - 1);
    before := oid(lng(holefirst) - 1);
    after := oid(lng(last) + 1);
    newholefirst := oid(lng(holefirst) + size);
    newholelast := oid(lng(holelast) + size);
  } else {
    holefirst := oid(lng(from_rid) + size);
    after := oid(lng(holefirst) + delta);
    holelast := oid(lng(after) - 1);
    before := oid(lng(from_rid) - 1);
    newholefirst := from_rid;
    newholelast := oid(lng(from_rid) + delta - 1);
  }

  var pre_size := ws.fetch(PRE_SIZE).find(cont);
  var rid_size := ws.fetch(_RID_SIZE).find(cont);
  var rid_size_update := ws.fetch(RID_SIZE_UPDATE).find(cont);
  var pre_level := ws.fetch(PRE_LEVEL).find(cont);
  var rid_level := ws.fetch(_RID_LEVEL).find(cont);
  var rid_level_update := ws.fetch(RID_LEVEL_UPDATE).find(cont);
  var pre_kind := ws.fetch(PRE_KIND).find(cont);
  var rid_kind := ws.fetch(_RID_KIND).find(cont);
  var rid_kind_update := ws.fetch(RID_KIND_UPDATE).find(cont);
  var pre_prop := ws.fetch(PRE_PROP).find(cont);
  var rid_prop := ws.fetch(_RID_PROP).find(cont);
  var rid_prop_update := ws.fetch(RID_PROP_UPDATE).find(cont);
  var pre_nid := ws.fetch(PRE_NID).find(cont);
  var rid_nid := ws.fetch(_RID_NID).find(cont);
  var rid_nid_update := ws.fetch(RID_NID_UPDATE).find(cont);
  var modified_nid := ws.fetch(MODIFIED_NID);
  var ancestor_nid := ws.fetch(ANCESTOR_NID);
  var pid_map_update := map_pid_update.select(oid_nil, oid_nil).reverse().sort().tmark(0@0);

  extend_protect(ws, cont);

  # for each page prior to the one we're actually moving in, and for
  # all nodes in the page we're moving in before the affected area,
  # find the nodes (holes or real nodes) that end inside the affected
  # area (i.e. inside the data being moved or the hole that is going
  # to be overwritten) and fix up the size

  # In this section of the code we're only dealing with "ancestor"
  # nodes.  Note that nodes that are on new pages are by definition
  # not ancestral and must already be in the MODIFIED_NID table.  Also
  # nodes that are already in an UPDATES bat must already be in the
  # MODIFIED_NID table.
  {
    var ancestors_newpre;
    var ancestors_newrid;
    var ancestors_isnewpage;
    var ancestors_nid;
    var ancestors_oldpre;
    {
      var from_nid;
      if (isoldpage) {
        if (rid_nid_update.exist(from_rid)) {
          from_nid := rid_nid_update.find(from_rid);
        } else {
          from_nid := pre_nid.find(swizzle(from_rid, map_pid_update));
        }
      } else {
        from_nid := rid_nid.find(from_rid);
      }
      var oldpre := oid_nil;
      if (nid_rid.exist(from_nid))
         oldpre := swizzle(nid_rid.find(from_nid), map_pid);
      if (not(isnil(oldpre))) {
        # it's an old node, we can use ll_ancestor to find ancestors
        ancestors_oldpre := new(void,oid).seqbase(0@0).append(0@0).append(ll_ancestor(new(void,oid).seqbase(0@0).append(0@0), new(void,oid).seqbase(0@0).append(oldpre), pre_size, pre_level));
        ancestors_nid := ancestors_oldpre.join(pre_nid);
        ancestors_newrid := ancestors_nid.join(nid_rid).access(BAT_WRITE).myupdate(ancestors_nid.join(nid_rid_update));
        ancestors_newpre := [swizzle](ancestors_newrid, map_pid_update);
        ancestors_isnewpage := [isnil](outerjoin([oid]([>>]([lng](ancestors_newrid), REMAP_PAGE_BITS)), map_pid));
      } else {
        ancestors_newpre := mil_ancestor(ws, cont, swizzle(from_rid, map_pid_update));
        ancestors_newrid := [swizzle](ancestors_newpre, pid_map_update);
        ancestors_isnewpage := [isnil](outerjoin([oid]([>>]([lng](ancestors_newrid), REMAP_PAGE_BITS)), map_pid));
        {
          # distinguish the three cases:
          # rid is on new page
          var a := ancestors_isnewpage.uselect(true).mirror().join(ancestors_newrid).join(rid_nid);
          # rid is on old page and was modified
          var b := ancestors_isnewpage.uselect(false).mirror().join(ancestors_newrid).join(rid_nid_update);
          # rid is on old page and was not modified
          var c := ancestors_isnewpage.uselect(false).kdiff(b).mirror().join(ancestors_newrid).[swizzle](map_pid).join(pre_nid);
          # combine
          ancestors_nid := a.access(BAT_WRITE).insert(b).insert(c).order().tmark(0@0);
        }
        ancestors_oldpre := ancestors_nid.join(nid_rid).[swizzle](map_pid);
      }
    }
    var ancestors_size := ancestors_oldpre.join(pre_size).access(BAT_WRITE).myupdate(ancestors_newrid.join(rid_size_update));
    # now figure out which ancestors *end* at the hole
    var ancestors_end := [oid]([+]([lng](ancestors_newpre), ancestors_size));
    # select those that end in the data being moved
    var cands := ancestors_end.uselect(from_rid, last);
    if (cands.count() > 0) {
      var new_size := [+](cands.mirror().join(ancestors_size), delta);
      var old_page := ancestors_isnewpage.uselect(false);
      if (old_page.count() > 0) {
         rid_size_update.myupdate(ancestors_newrid.reverse().join(old_page.mirror().join(new_size)));
      }
      var new_page := ancestors_isnewpage.uselect(true);
      if (new_page.count() > 0) {
        rid_size.myupdate(ancestors_newrid.reverse().join(new_page.mirror().join(new_size)));
      }
      # remember which ancestors were changed
      ancestor_nid.insert(cands.project(cont).reverse().join(ancestors_nid));
    }
    # select those that end in the hole being overwritten
    var cands := ancestors_end.uselect(holefirst, holelast);
    if (cands.count() > 0) {
      # clamp to edge
      var new_size := [int](cands.mirror().join(ancestors_newrid)); # not really, yet
      if (delta < 0) {
        new_size := [int]([-](int(before), new_size));
      } else {
        new_size := [int]([-](int(last), new_size));
      }
      var old_page := ancestors_isnewpage.uselect(false);
      if (old_page.count() > 0) {
         rid_size_update.myupdate(ancestors_newrid.reverse().join(old_page.mirror().join(new_size)));
      }
      var new_page := ancestors_isnewpage.uselect(true);
      if (new_page.count() > 0) {
         rid_size.myupdate(ancestors_newrid.reverse().join(new_page.mirror().join(new_size)));
      }
      # remember which ancestors were changed
      ancestor_nid.insert(cands.project(cont).reverse().join(ancestors_nid));
    }
  }

  # the PRE_SIZE table has the correct sizes, but we now need to
  # actually move the data in all PRE tables
  var update_hole;
  var from_pre := swizzle(from_rid, map_pid);
  var last_pre := swizzle(last, map_pid);

  {
    var data;
    if (isoldpage) {
      data := pre_size.reverse().select(from_pre, last_pre).reverse().copy().access(BAT_WRITE).seqbase(from_rid).key(true);
      data.myupdate(rid_size_update.reverse().select(from_rid, last).reverse());
    } else {
      data := rid_size.reverse().select(from_rid, last).reverse().copy().access(BAT_WRITE).key(true);
    }

    # update the sizes of the moved data

    # table with int(nil) for all RID values that represent holes and 0 otherwise
    var ishole := [niland](data, int_nil);
    var end_data := [oid]([+]([int](data.mirror()), [niland](data, INT_MAX)));
    # find the nodes that end inside the hole that is to disappear
    if (delta > 0) {
      var holeend := end_data.uselect(holefirst, holelast);
      var newsizes := [-](int(last), [int](holeend.mirror()));
      newsizes := [nilor](newsizes.mirror().join(ishole), newsizes);
      data.replace(newsizes);
    } # else
      # if moving backward (delta < 0) no nodes that are to be moved end
      # in the hole
    # find the nodes that end past the hole and compensate for the move
    var pastend := end_data.uselect(after, oid_nil);
    if (pastend.count() > 0) {
      var newsizes := [-](pastend.mirror().join(data), delta);
      data.replace(newsizes);
    }

    # now fix the head values (i.e. move the data)
    var update_data := data.reverse().mark(oid(lng(from_rid) + delta)).reverse();
    # we're not actually using the tail value here...
    update_hole := rid_size.reverse().select(newholefirst, newholelast).reverse();
    if (update_hole.count() < delta) {
      var i := update_hole.count();
      update_hole := update_hole.copy().access(BAT_WRITE).key(true);
      while (i < delta) {
        update_hole := update_hole.append(int_nil);
        i :+= 1;
      }
    }
    update_hole := [nilor]([-](update_hole.project(int(newholelast)), [int](update_hole.mirror())), int_nil);

    if (isoldpage) {
      rid_size_update.myupdate(update_hole).myupdate(update_data);
    } else {
      rid_size.replace(update_hole, true).replace(update_data, true);
    }
  }

  {
    var update_data;
    if (isoldpage) {
      update_data := pre_level.reverse().select(from_pre, last_pre).reverse().copy().seqbase(from_rid);
      update_data := update_data.access(BAT_WRITE).key(true).myupdate(rid_level_update.reverse().select(from_rid, last).reverse());
    } else {
      update_data := rid_level.reverse().select(from_rid, last).reverse();
    }
    update_data := update_data.reverse().mark(oid(lng(from_rid) + delta)).reverse();
    update_hole := update_hole.project(cast(nil, rid_level.ttype()));
    if (isoldpage) {
      rid_level_update.myupdate(update_hole).myupdate(update_data);
    } else {
      rid_level.replace(update_hole, true).replace(update_data, true);
    }
  }

  {
    var update_data;
    if (isoldpage) {
      update_data := pre_kind.reverse().select(from_pre, last_pre).reverse().copy().seqbase(from_rid);
      update_data := update_data.access(BAT_WRITE).key(true).myupdate(rid_kind_update.reverse().select(from_rid, last).reverse());
    } else {
      update_data := rid_kind.reverse().select(from_rid, last).reverse();
    }
    update_data := update_data.reverse().mark(oid(lng(from_rid) + delta)).reverse();
    update_hole := update_hole.project(cast(nil, rid_kind.ttype()));
    if (isoldpage) {
      rid_kind_update.myupdate(update_hole).myupdate(update_data);
    } else {
      rid_kind.replace(update_hole, true).replace(update_data, true);
    }
  }

  {
    var update_data;
    if (isoldpage) {
      update_data := pre_prop.reverse().select(from_pre, last_pre).reverse().copy().seqbase(from_rid);
      update_data := update_data.access(BAT_WRITE).key(true).myupdate(rid_prop_update.reverse().select(from_rid, last).reverse());
    } else {
      update_data := rid_prop.reverse().select(from_rid, last).reverse();
    }
    update_data := update_data.reverse().mark(oid(lng(from_rid) + delta)).reverse();
    update_hole := update_hole.project(oid_nil);
    if (isoldpage) {
      rid_prop_update.myupdate(update_hole).myupdate(update_data);
    } else {
      rid_prop.replace(update_hole, true).replace(update_data, true);
    }
  }

  {
    var update_data;
    if (isoldpage) {
      update_data := pre_nid.reverse().select(from_pre, last_pre).reverse().copy().seqbase(from_rid);
      update_data := update_data.access(BAT_WRITE).key(true).myupdate(rid_nid_update.reverse().select(from_rid, last).reverse());
    } else {
      update_data := rid_nid.reverse().select(from_rid, last).reverse();
    }
    update_data := update_data.reverse().mark(oid(lng(from_rid) + delta)).reverse();
    update_hole := update_hole.project(oid_nil);
    if (isoldpage) {
      rid_nid_update.myupdate(update_hole).myupdate(update_data);
    } else {
      rid_nid.replace(update_hole, true).replace(update_data, true);
    }
    var update_nid := update_data.select(oid_nil, oid_nil).reverse();
    var nid_rid_update := ws.fetch(NID_RID_UPDATE).find(cont);
    nid_rid_update.myupdate(update_nid);
    modified_nid.reverse().accbuild("hash");
    modified_nid.insert([lng](update_nid.project(cont).reverse()).[>>](OID_PAGE_BITS));
  }
  extend_unprotect(ws, cont);
}
ADDHELP("movedata", "sjoerd", "Oct 4 2005",
"Move data starting in WS for document CONT, starting at FROM of size
SIZE by DELTA. Delta may be positive (move data down) or
negative. Data should only be moved over an existing hole (not
currently checked). Returns the first argument which details the
changed already made to the BATs in the working set.",
"pf_support");

PROC play_update_tape(bat[void, bat] ws, bat[void, oid] item, bat[void, int] kind, bat[void,lng] int_values, bat[void,str] str_values) : void
{
  # [void,oid] list of all conts of affected documents
  var affected_conts := [and]([lng](item.mirror()), 3LL).ord_uselect(1LL).mirror().leftfetchjoin(kind).get_container().reverse().kunique().hmark(0@0);  # [i,CONT]

  if (affected_conts.count() = 0) {
    # nothing to do
    # this is actually not expected to happen, but it doesn't do any harm
    return;
  }

  # check that we're not trying to update the transient container
  if (affected_conts.uselect(0@0).count() > 0) {
    ERROR("updating transient container.\n");
  }

  # check that all containers are updatable (i.e. that none are read-only)
  if ([ttype](affected_conts.join(ws.fetch(MAP_PID))).uselect(void).count() > 0) {
    ERROR("updating read-only document.\n");
  }
  if (ws_log_active)
    ws_log(ws, "===================== START OF UPDATE");

  # set-wise unique bats, but better build the hash table on tail (head=cont != selective)
  ws.fetch(MODIFIED_NID).reverse().accbuild("hash");
  ws.fetch(ANCESTOR_NID).reverse().accbuild("hash");
  ws.fetch(MODIFIED_ATTR).reverse().accbuild("hash");
  ws.fetch(MODIFIED_PAGE).reverse().accbuild("hash");
  ws.fetch(NEW_PAGE).reverse().accbuild("hash");

  affected_conts@batloop() {
    ws.fetch(MAP_PID_UPDATE).insert($t, ws.fetch(MAP_PID).find($t).copy().access(BAT_WRITE).key(true));
    ws.fetch(RID_SIZE_UPDATE).insert($t, new(oid,int).key(true));
    ws.fetch(RID_LEVEL_UPDATE).insert($t, new(oid,chr).key(true));
    ws.fetch(RID_KIND_UPDATE).insert($t, new(oid,chr).key(true));
    ws.fetch(RID_PROP_UPDATE).insert($t, new(oid,oid).key(true));
    ws.fetch(RID_NID_UPDATE).insert($t, new(oid,oid).key(true));
    ws.fetch(NID_RID_UPDATE).insert($t, new(oid,oid).key(true));
    ws.fetch(ATTR_QN_UPDATE).insert($t, new(oid,oid).key(true));
    ws.fetch(ATTR_PROP_UPDATE).insert($t, new(oid,oid).key(true));
    ws.fetch(ATTR_OWN_UPDATE).insert($t, new(oid,oid).key(true));
    ws.fetch(PROP_VAL_UPDATE).insert($t, new(oid,str).key(true));
    ws.fetch(PROP_TEXT_UPDATE).insert($t, new(oid,str).key(true));
    ws.fetch(PROP_COM_UPDATE).insert($t, new(oid,str).key(true));
    ws.fetch(PROP_INS_UPDATE).insert($t, new(oid,str).key(true));
    ws.fetch(PROP_TGT_UPDATE).insert($t, new(oid,str).key(true));
    ws.fetch(QN_LOC_UPDATE).insert($t, new(oid,str).key(true));
    ws.fetch(QN_URI_UPDATE).insert($t, new(oid,str).key(true));
    ws.fetch(QN_PREFIX_UPDATE).insert($t, new(oid,str).key(true));
    ws.fetch(QN_URI_LOC_UPDATE).insert($t, new(oid,str).key(true));
    ws.fetch(QN_PREFIX_URI_LOC_UPDATE).insert($t, new(oid,str).key(true));
    ws.fetch(QN_HISTOGRAM_UPDATE).insert($t, new(oid,lng).key(true));
    ws.fetch(NID_QN_INS_UPDATE).insert($t, new(oid,oid).key(true));
    ws.fetch(NID_QN_DEL_UPDATE).insert($t, new(oid,oid).key(true));
  }

  # extract the commands from the update tape 
  var update_cmd := [and]([lng](item.mirror()), 3LL).ord_uselect(0LL).mirror().leftfetchjoin(item).leftfetchjoin(int_values);

  {
    var rename_update := update_cmd.ord_uselect(UPDATE_RENAME);
    if (rename_update.count() > 0) {
      var update_node_item := [oid]([+]([lng](rename_update.mirror()), 1)).leftfetchjoin(item);
      var update_node_kind := [oid]([+]([lng](rename_update.mirror()), 1)).leftfetchjoin(kind);
      var update_rename := [oid]([+]([lng](rename_update.mirror()), 2)).leftfetchjoin(item).leftfetchjoin(str_values);

      # now select all the unique nodes (both item and kind must be
      # the same), and when there are multiple occurences, select the
      # *last*.
      # note that order doesn't matter once this is done.
      if (update_node_item.count() > 1) {
        var update_node_unique := update_node_item.CTgroup().CTderive(update_node_kind).CTmap().reverse().{max}().reverse().mirror();
        update_node_item := update_node_unique.leftjoin(update_node_item);
        update_node_kind := update_node_unique.leftjoin(update_node_kind);
        update_rename := update_node_unique.leftjoin(update_rename);
      }

      # now make the three BATs void-headed
      update_node_item := update_node_item.tmark(0@0);
      update_node_kind := update_node_kind.tmark(0@0);
      update_rename := update_rename.tmark(0@0);

      # do the work
      do_update_rename(ws, update_node_item, update_node_kind, update_rename);
    }
  }

  {
    var replace_update := update_cmd.ord_uselect(UPDATE_REPLACE);
    if (replace_update.count() > 0) {
      var update_node_item := [oid]([+]([lng](replace_update.mirror()), 1)).leftfetchjoin(item);
      var update_node_kind := [oid]([+]([lng](replace_update.mirror()), 1)).leftfetchjoin(kind);
      var update_replace := [oid]([+]([lng](replace_update.mirror()), 2)).leftfetchjoin(item).leftfetchjoin(str_values);

      # now select all the unique nodes (both item and kind must be
      # the same), and when there are multiple occurences, select the
      # *last*.
      # note that order doesn't matter once this is done.
      if (update_node_item.count() > 1) {
        var update_node_unique := update_node_item.CTgroup().CTderive(update_node_kind).CTmap().reverse().{max}().reverse().mirror();
        update_node_item := update_node_unique.leftjoin(update_node_item);
        update_node_kind := update_node_unique.leftjoin(update_node_kind);
        update_replace := update_node_unique.leftjoin(update_replace);
      }

      # now make the three BATs void-headed
      update_node_item := update_node_item.tmark(0@0);
      update_node_kind := update_node_kind.tmark(0@0);
      update_replace := update_replace.tmark(0@0);

      # do the work
      do_update_replace(ws, update_node_item, update_node_kind, update_replace);
    }
  }

  {
    var insert_update := update_cmd.ord_select(UPDATE_INSERT_FIRST, UPDATE_REPLACECONTENT);
    if (insert_update.count() > 0) {
      var update_node_item := [oid]([+]([lng](insert_update.mirror()), 1)).leftfetchjoin(item);
      var update_node_kind := [oid]([+]([lng](insert_update.mirror()), 1)).leftfetchjoin(kind);
      var update_insert_node_item := [oid]([+]([lng](insert_update.mirror()), 2)).leftfetchjoin(item);
      var update_insert_node_kind := [oid]([+]([lng](insert_update.mirror()), 2)).leftfetchjoin(kind);

      # UPDATE_INSERT_LAST and UPDATE_INSERT_BEFORE commands must be executed in reverse order
      var order := insert_update.ord_uselect(UPDATE_INSERT_LAST, UPDATE_INSERT_BEFORE).copy().mark(0@0);
      if (order.count() > 1) {
        update_node_item.access(BAT_WRITE).replace(order.leftjoin(order.mirror().leftjoin(update_node_item).access(BAT_WRITE).revert().tmark(0@0)));
        update_node_kind.access(BAT_WRITE).replace(order.leftjoin(order.mirror().leftjoin(update_node_kind).access(BAT_WRITE).revert().tmark(0@0)));
        update_insert_node_item.access(BAT_WRITE).replace(order.leftjoin(order.mirror().leftjoin(update_insert_node_item).access(BAT_WRITE).revert().tmark(0@0)));
        update_insert_node_kind.access(BAT_WRITE).replace(order.leftjoin(order.mirror().leftjoin(update_insert_node_kind).access(BAT_WRITE).revert().tmark(0@0)));
        insert_update.access(BAT_WRITE).replace(order.leftjoin(order.mirror().leftjoin(insert_update).access(BAT_WRITE).revert().tmark(0@0)));
      }

      # do double elimination on UPDATE_REPLACECONTENT commands
      # this is done by writing nil values in the insert_update column for eliminated rows.
      var replace := insert_update.ord_uselect(UPDATE_REPLACECONTENT).mirror();
      if (replace.count() > 1) {
        insert_update.access(BAT_WRITE).replace(replace.kdiff(replace.leftjoin(update_node_item).CTgroup().CTderive(update_node_kind).CTmap().reverse().{max}().reverse()).project(lng_nil));
      }

      # now make the five BATs void-headed
      update_node_item := update_node_item.tmark(0@0);
      update_node_kind := update_node_kind.tmark(0@0);
      update_insert_node_item := update_insert_node_item.tmark(0@0);
      update_insert_node_kind := update_insert_node_kind.tmark(0@0);
      insert_update := insert_update.tmark(0@0);

      # do the work
      do_update_insert(ws, insert_update, update_node_item, update_node_kind, update_insert_node_item, update_insert_node_kind);
    }
  }

  {
    var delete_update := update_cmd.ord_uselect(UPDATE_DELETE);
    if (delete_update.count() > 0) {
      var update_node_item := [oid]([+]([lng](delete_update.mirror()), 1)).leftfetchjoin(item);
      var update_node_kind := [oid]([+]([lng](delete_update.mirror()), 1)).leftfetchjoin(kind);

      # now select all the unique nodes (both item and kind must be
      # the same).
      if (update_node_item.count() > 1) {
        var update_node_unique := update_node_item.CTgroup().CTderive(update_node_kind).CTmap().tunique().mirror();
        update_node_item := update_node_unique.leftjoin(update_node_item);
        update_node_kind := update_node_unique.leftjoin(update_node_kind);
      }

      # now make the two BATs void-headed
      update_node_item := update_node_item.tmark(0@0);
      update_node_kind := update_node_kind.tmark(0@0);

      # do the work
      do_update_delete(ws, update_node_item, update_node_kind);
    }
  }

  var texts := ws.fetch(UPDATED_TEXT);
  #fix_consecutive_texts(ws, texts);

  # prepare for postcommit
  var old_attr := new(oid,bat);
  var old_text := new(oid,bat);
  var new_attr := new(oid,bat);
  var new_text := new(oid,bat);

  affected_conts@batloop() {
    var cont := $t;

    # first do attr
    extend_protect(ws, cont);
    var attr_prop_update := ws.fetch(ATTR_PROP_UPDATE).find(cont); # [ATID,PROPID]
    # delattr: attributes of deleted nodes
    var delattr := get_attr_own(ws, ws.fetch(NID_RID_UPDATE).find(cont).uselect(oid_nil).hmark(0@0), cont);
    if ((attr_prop_update.count() > 0) or (delattr.count() > 0)) {
      # combine attrs from deleted nodes (delattr) with updated
      # attributes (attr_prop_update) and figure out old values by
      # joining with ATTR_PROP
      var oldprop := kunion(delattr.reverse().mirror(), attr_prop_update.mirror()).leftjoin(ws.fetch(ATTR_PROP).find(cont)).ord_select(oid_nil, oid_nil); # [ATID,PROPID]
      var oldvx;
      if (oldprop.count() > 0) {
        var oldval := oldprop.leftfetchjoin(ws.fetch(PROP_VAL).find(cont)); # [ATID,str]
        var map_new_old := oldprop.hmark(0@0); # [i,ATID]
        oldval := oldval.tmark(0@0);  # [i,str]
        var oldqn := map_new_old.leftfetchjoin(ws.fetch(ATTR_QN).find(cont));
        var oldown := map_new_old.leftfetchjoin(ws.fetch(ATTR_OWN).find(cont));
        oldvx := vx_maintain(oldown, oldqn, oldval);
      } else {
        oldvx := new(int,oid);
      }
      # only keep attributes that are set to new values (i.e. not deleted)
      attr_prop_update := attr_prop_update.ord_select(oid_nil, oid_nil).sort();	 # [ATID,PROP]
      var map_new_old := attr_prop_update.hmark(0@0); # [i,ATID]
      var newval := attr_prop_update.leftjoin(ws.fetch(PROP_VAL).find(cont)).access(BAT_WRITE); # [i,str]
      newval := newval.insert(attr_prop_update.leftjoin(ws.fetch(PROP_VAL_UPDATE).find(cont))).order().tmark(0@0);

      var newqn := map_new_old.join(ws.fetch(ATTR_QN).find(cont)).copy().access(BAT_WRITE).key(true).myupdate(map_new_old.join(ws.fetch(ATTR_QN_UPDATE).find(cont))).order();  # [i,QNID]
      var newown := map_new_old.join(ws.fetch(ATTR_OWN).find(cont)).copy().access(BAT_WRITE).key(true).myupdate(map_new_old.join(ws.fetch(ATTR_OWN_UPDATE).find(cont))).order();  # [i,OWN]
      var newvx := vx_maintain(newown, newqn, newval);
      old_attr.insert(cont, sdiff(oldvx, newvx));
      new_attr.insert(cont, sdiff(newvx, oldvx));
    } else {
      old_attr.insert(cont, new(int,oid));
      new_attr.insert(cont, new(int,oid));
    }

    # then do text
    var rid_prop_update := ws.fetch(RID_PROP_UPDATE).find(cont); # [RID,PROP] (updated properties)
    if (rid_prop_update.count() > 0) {
      var rids := rid_prop_update.hmark(0@0);  # [i,RID]
      var map_pid := ws.fetch(MAP_PID).find(cont);
      var oldpres := [swizzle](rids, map_pid);  # [i,PRE]
      oldpres := oldpres.join(ws.fetch(PRE_KIND).find(cont)).uselect(TEXT).mirror().join(oldpres).tmark(0@0);  # [j,PRE] (original text nodes)
      var oldvx;
      if (oldpres.count() > 0) {
        var oldtext := oldpres.leftjoin(ws.fetch(PRE_PROP).find(cont)).leftjoin(ws.fetch(PROP_TEXT).find(cont)); # [j,str]
        var oldnids := oldpres.leftjoin(ws.fetch(PRE_NID).find(cont));
        oldvx := vx_maintain(oldnids, oldtext);
      } else {
        oldvx := new(int,oid);
      }
      var nids;
      {
        # figure out where to get new value
        var isnewpage := [isnil](outerjoin([oid]([>>]([lng](rids), REMAP_PAGE_BITS)), map_pid)); # [i,bit]
        # rid is on new page
        var a := isnewpage.uselect(true).mirror().join(rids).join(ws.fetch(_RID_KIND).find(cont));
        # rid is on old page and was modified
        var b := isnewpage.uselect(false).mirror().join(rids).join(ws.fetch(RID_KIND_UPDATE).find(cont));
        # rid is on old page and was unmodified
        var c := isnewpage.uselect(false).kdiff(b).mirror().join(rids).[swizzle](map_pid).join(ws.fetch(PRE_KIND).find(cont));
        # select text elements and combine
        var t := a.access(BAT_WRITE).insert(b).insert(c).uselect(TEXT).sort().mirror(); # [i,i] (text elems only)
        rids := t.leftjoin(rids).tmark(0@0); # [i,RID] (only text elements, new numbering)
        isnewpage := t.leftjoin(isnewpage).tmark(0@0);	 # [i,bit] (only text elements, new numbering)
        a := isnewpage.uselect(true).mirror().join(rids).join(ws.fetch(_RID_NID).find(cont));
        # rid is on old page and was modified
        b := isnewpage.uselect(false).mirror().join(rids).join(ws.fetch(RID_NID_UPDATE).find(cont));
        # rid is on old page and was unmodified
        c := isnewpage.uselect(false).kdiff(b).mirror().join(rids).[swizzle](map_pid).join(ws.fetch(PRE_NID).find(cont));
        # select text elements and combine
        nids := a.access(BAT_WRITE).insert(b).insert(c).order().tmark(0@0); # [i,NID]
      }
      if (rids.count() > 0) {
        var pres := [swizzle](rids, ws.fetch(MAP_PID_UPDATE).find(cont));  # [i,newPRE]
        rids := rids.leftjoin(rid_prop_update);
        var vals := rids.leftjoin(ws.fetch(PROP_TEXT).find(cont)).access(BAT_WRITE);
        vals.insert(rids.leftjoin(ws.fetch(PROP_TEXT_UPDATE).find(cont))).order().tmark(0@0); 
        var newvx := vx_maintain(nids, vals);
        old_text.insert(cont, sdiff(oldvx, newvx));
        new_text.insert(cont, sdiff(newvx, oldvx));
      } else {
        old_text.insert(cont, oldvx);
        new_text.insert(cont, new(int,oid));
      }
    } else {
      old_text.insert(cont, new(int,oid));
      new_text.insert(cont, new(int,oid));
    }
    extend_unprotect(ws, cont);
  }

  # get containers in the order in which we must commit them ([COLL_ID,CONT])
  var cont_order := affected_conts.reverse().mirror().leftfetchjoin(ws.fetch(CONT_COLL)).reverse().sort();

  ws.seqbase(oid_nil); # indicate that we are done reading our snapshot

  var ws_logstarttime := usec(); # this timer is used assuming a single collection per query

  # get the collection locks, re-read the master ancestor sizes, and check for conflicts (ws_precommit)
  cont_order@batloop() {
    var cont := $t;

    coll_lock_set(ws, cont, COLL_LONGLOCK, "===================== COMMIT START"); 
    var ws_logtime := usec();

    # add ancestor nids on modified pages to modified nids list and remove them from ancestor nids list
    var modified_nid := ws.fetch(MODIFIED_NID); # [cont,NID]
    var ancestor_nid := ws.fetch(ANCESTOR_NID); # [cont,NID]
    var sel_modified_page := ws.fetch(MODIFIED_PAGE).reverse().uselect(cont).hmark(0@0); # [i,PGID]
    var sel_modified_attr := ws.fetch(MODIFIED_ATTR).reverse().uselect(cont).hmark(0@0); # [i,ATID]
    var sel_ancestor_nid := ancestor_nid.reverse().uselect(cont).hmark(0@0); # [i,NID]
    var sel_ancestor_rid := sel_ancestor_nid.join(ws.fetch(NID_RID).find(cont)); # [i,RID]
    var del_ancestor_nid := new(void, oid);
    sel_modified_page@batloop() {
      var start := oid(lng($t) << REMAP_PAGE_BITS);
      var end := oid(lng(start) + REMAP_PAGE_MASK);
      var rids := sel_ancestor_rid.reverse().select(start, end).reverse();
      var nids := reverse([lng](rids.mirror().join(sel_ancestor_nid)).[>>](OID_PAGE_BITS));
      modified_nid.reverse().insert(nids.project(cont));
      del_ancestor_nid.append(sel_ancestor_nid.tintersect(rids.mirror().join(sel_ancestor_nid)));
    }
    # we change modified_nid + ancestor_nid here; this info is later needed by ws_isolate()
    reverse(ancestor_nid).deleteBuns(del_ancestor_nid.tunique().project(cont));

    if (ws_log_active)
      ws_log(ws, "commit-reread exec" + str(ws_logtime - usec())); 

    # ws_precommit gives an ERROR if a conflicting transaction has committed already
    ws_precommit(ws, cont, sel_modified_page, sel_modified_attr);
  }

  var ws_logtime := usec();

  # WAL all changes: no error means a succesful commit
  lock_set(pf_wal);
  if (ws_log_active) 
    ws_log(ws, "===================== WAL LOCKED wal_lock" + str(ws_logtime - (ws_logtime := usec()))); 
  log_trans_start(pf_logger);
  var err := CATCH(do_log_updates(ws, cont_order));
  lock_unset(pf_wal);
  if (not(isnil(err))) ERROR(err);

  if (ws_log_active)
    ws_log(ws, "===================== WAL RELEASED exec" + str(ws_logtime - (ws_logtime := usec()))); 


  # upgrade locks, isolate the masters, apply deltas to them, maintain indices, and release all coll-lock(s)
  cont_order@batloop() {
    var cont := $t;

    coll_lock_set(ws, cont, COLL_SHORTLOCK, "===================== MASTER UPDATES"); 

    # isolation: which attrs, nids and pages were modified?
    var modified_page := ws.fetch(MODIFIED_PAGE).reverse().uselect(cont).hmark(0@0); # [i,PGID]
    var modified_attr := ws.fetch(MODIFIED_ATTR).reverse().uselect(cont).hmark(0@0); # [i,ATID]
    var modified_nid := ws.fetch(MODIFIED_NID).reverse().uselect(cont).hmark(0@0); # [i,NID]
    var ancestor_nid := ws.fetch(ANCESTOR_NID).reverse().uselect(cont).hmark(0@0); # [i,NID]

    # ws_isolate isolates concurrent snapshots, before we modify the masters 
    __ws_isolate(ws, cont, modified_page, ancestor_nid, [oid]([<<](modified_nid, OID_PAGE_BITS)), modified_attr);

    if (ws_log_active)
      ws_log(ws, "commit-isolate exec" + str(ws_logtime - (ws_logtime := usec()))); 

    # apply the logged changes to the masters
    pf_assert(CATCH(do_commit_updates(ws, cont)), "master update failed (commit_updates)");

    if (ws_log_active)
      ws_log(ws, "commit-apply exec" + str(ws_logtime - (ws_logtime := usec()))); 

    # ws_postcommit maintains the indices and releases the collection lock
    __ws_postcommit(ws, cont,
                    new_attr.find(cont),
                    old_attr.find(cont),
                    new_text.find(cont),
                    old_text.find(cont),
                    ws.fetch(NID_QN_INS_UPDATE).find(cont).reverse(), 
                    ws.fetch(NID_QN_DEL_UPDATE).find(cont).reverse(), 
                    ws.fetch(QN_URI_UPDATE).find(cont).hmark(0@0),
                    ws.fetch(DEL_PAGE).reverse().uselect(cont).hmark(0@0));

    if (ws_log_active)
      ws_log(ws, "commit-idx exec" + str(ws_logtime - usec())); 

    coll_lock_unset(ws, cont, COLL_BOTHLOCK, "===================== COMMIT END", ws_logstarttime); 
  }
}

PROC do_commit_updates(BAT[void,bat] ws, oid cont) : void
{
  ws.fetch(_MAP_PID).find(cont).myupdate(ws.fetch(MAP_PID_UPDATE).find(cont));
  ws.fetch(_RID_SIZE).find(cont).myupdate(ws.fetch(RID_SIZE_UPDATE).find(cont));
  ws.fetch(_RID_LEVEL).find(cont).myupdate(ws.fetch(RID_LEVEL_UPDATE).find(cont));
  ws.fetch(_RID_KIND).find(cont).myupdate(ws.fetch(RID_KIND_UPDATE).find(cont));
  ws.fetch(_RID_PROP).find(cont).myupdate(ws.fetch(RID_PROP_UPDATE).find(cont));
  ws.fetch(_RID_NID).find(cont).myupdate(ws.fetch(RID_NID_UPDATE).find(cont));
  ws.fetch(_NID_RID).find(cont).myupdate(ws.fetch(NID_RID_UPDATE).find(cont));
  ws.fetch(_ATTR_QN).find(cont).myupdate(ws.fetch(ATTR_QN_UPDATE).find(cont));
  ws.fetch(_ATTR_PROP).find(cont).myupdate(ws.fetch(ATTR_PROP_UPDATE).find(cont));
  ws.fetch(_ATTR_OWN).find(cont).myupdate(ws.fetch(ATTR_OWN_UPDATE).find(cont));
}

PROC mylog_delta(logger l, BAT[any,any] b, str nme) : void
{
    log_delta(l, b, nme);
b.col_name(nme).print();
}

PROC do_log_updates(BAT[void,bat] ws, BAT[any,any] cont_order) : void
{
  var ws_logtime := usec(); 

  # first do the main logging work without the collection locks
  cont_order@batloop() {
    var cont := $t;
    var rid_size := ws.fetch(_RID_SIZE).find(cont);
    var rid_level := ws.fetch(_RID_LEVEL).find(cont);
    var rid_kind := ws.fetch(_RID_KIND).find(cont);
    var rid_prop := ws.fetch(_RID_PROP).find(cont);
    var rid_nid := ws.fetch(_RID_NID).find(cont);
    log_delta(pf_logger, ws.fetch(MAP_PID_UPDATE).find(cont), ws.fetch(_MAP_PID).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(RID_SIZE_UPDATE).find(cont), rid_size.bbpname());
    log_delta(pf_logger, ws.fetch(RID_LEVEL_UPDATE).find(cont), rid_level.bbpname());
    log_delta(pf_logger, ws.fetch(RID_KIND_UPDATE).find(cont), rid_kind.bbpname());
    log_delta(pf_logger, ws.fetch(RID_PROP_UPDATE).find(cont), rid_prop.bbpname());
    log_delta(pf_logger, ws.fetch(RID_NID_UPDATE).find(cont), rid_nid.bbpname());
    log_delta(pf_logger, ws.fetch(NID_RID_UPDATE).find(cont), ws.fetch(_NID_RID).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(ATTR_QN_UPDATE).find(cont), ws.fetch(_ATTR_QN).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(ATTR_PROP_UPDATE).find(cont), ws.fetch(_ATTR_PROP).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(ATTR_OWN_UPDATE).find(cont), ws.fetch(_ATTR_OWN).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(PROP_VAL_UPDATE).find(cont), ws.fetch(_PROP_VAL).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(PROP_TEXT_UPDATE).find(cont), ws.fetch(_PROP_TEXT).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(PROP_COM_UPDATE).find(cont), ws.fetch(_PROP_COM).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(PROP_INS_UPDATE).find(cont), ws.fetch(_PROP_INS).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(PROP_TGT_UPDATE).find(cont), ws.fetch(_PROP_TGT).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(QN_LOC_UPDATE).find(cont), ws.fetch(_QN_LOC).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(QN_URI_UPDATE).find(cont), ws.fetch(_QN_URI).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(QN_PREFIX_UPDATE).find(cont), ws.fetch(_QN_PREFIX).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(QN_URI_LOC_UPDATE).find(cont), ws.fetch(_QN_URI_LOC).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(QN_PREFIX_URI_LOC_UPDATE).find(cont), ws.fetch(_QN_PREFIX_URI_LOC).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(QN_HISTOGRAM_UPDATE).find(cont), ws.fetch(_QN_HISTOGRAM).find(cont).bbpname());
    var new_page := ws.fetch(NEW_PAGE).reverse().ord_uselect(cont).sort();
    new_page@batloop() {
      var start := oid(lng($h) << REMAP_PAGE_BITS);
      var end := oid(lng(start) + REMAP_PAGE_MASK);
      log_delta(pf_logger, rid_size.reverse().select(start, end).reverse(), rid_size.bbpname());
      log_delta(pf_logger, rid_level.reverse().select(start, end).reverse(), rid_level.bbpname());
      log_delta(pf_logger, rid_kind.reverse().select(start, end).reverse(), rid_kind.bbpname());
      log_delta(pf_logger, rid_prop.reverse().select(start, end).reverse(), rid_prop.bbpname());
      log_delta(pf_logger, rid_nid.reverse().select(start, end).reverse(), rid_nid.bbpname());
    }

    if (ws_log_active)
      ws_log(ws, "commit-LOG_DELTAS exec" + str(ws_logtime - (ws_logtime := usec()))); 

    # compute the size-delta on the ancestors of modified nodes, and apply those to the recent masters (and LOG it) 
    var sel_ancestor_nid := ws.fetch(ANCESTOR_NID).reverse().uselect(cont).hmark(0@0); # [i,NID]
    var anc_rid := sel_ancestor_nid.leftjoin(ws.fetch(NID_RID).find(cont));
    var anc_oldsize := [swizzle](anc_rid, ws.fetch(MAP_PID).find(cont)).join(ws.fetch(PRE_SIZE).find(cont));
        anc_rid.access(BAT_WRITE).replace(sel_ancestor_nid.join(ws.fetch(NID_RID_UPDATE).find(cont)));
    var anc_newsize := anc_rid.leftjoin(ws.fetch(RID_SIZE_UPDATE).find(cont));
    var anc_mstsize := anc_rid.leftjoin(ws.fetch(_RID_SIZE).find(cont));
    var anc_updsize := [+]([-](anc_newsize, anc_oldsize), anc_mstsize);
    var rid_updsize := reverse(anc_rid).leftfetchjoin(anc_updsize);
    log_delta(pf_logger, rid_updsize, ws.fetch(_RID_SIZE).find(cont).bbpname()); 
    ws.fetch(RID_SIZE_UPDATE).find(cont).myupdate(rid_updsize);

    if (ws_log_active)
      ws_log(ws, "commit-LOG_SIZE exec" + str(ws_logtime - (ws_logtime := usec()))); 
  }
  log_trans_end(pf_logger); # write commit record in WAL: ==> THIS IS THE COMMIT POINT <==

  if (ws_log_active)
    ws_log(ws, "commit-LOG_TRANS_END exec" + str(ws_logtime - usec()));
}

# due to the order in which updates are executed, do_update_rename and
# do_update_replace don't have to contend with inserted pages or with
# nodes having changed their PRE value
PROC do_update_rename(bat[void, bat] ws, bat[void,oid] update_node_item, bat[void,int] update_node_kind, bat[void,str] update_rename) : void
{
  var conts := update_node_kind.get_container();
  var types := update_node_kind.get_types();
  var attrs := types.ord_uselect(ATTR);
  var elems := types.ord_uselect(ELEM);

  if (attrs.count() > 0) {
    attrs := attrs.mirror();
    var aitem := attrs.leftjoin(update_node_item); # [i,ATID]
    var aconts := attrs.leftjoin(conts); # [i,CONT]
    {
      var modified_attr := ws.fetch(MODIFIED_ATTR);
      modified_attr.insert(aconts.reverse().join(aitem));
    }
    aconts.tunique()@batloop() {
      var cont := $h;
      var list := aconts.ord_uselect(cont).mirror(); # [i,i] (attributes in current container)
      var aqn := list.leftjoin(update_rename); # [i,newQName]
      var aqnid := find_qn_bulk(ws, cont, [+](NS_ACCEL_SEP + NS_ACCEL_SEP, aqn), bit_nil); # [i,newQNID]
      var attr_qn_updates := ws.fetch(ATTR_QN_UPDATE).find(cont);
      # list.leftjoin(aitem) [i,ATID]
      # list.leftjoin(aitem).reverse() [ATID,i]
      # list.leftjoin(aitem).reverse().leftjoin(aqnid) [ATID,QNID]
      attr_qn_updates.insert(list.leftjoin(aitem).reverse().leftjoin(aqnid));
    }
  }

  if (elems.count() > 0) {
    elems := elems.mirror();
    var eitem := elems.leftjoin(update_node_item); # [i,PRE]
    var econts := elems.leftjoin(conts); # [i,CONT]
    econts.tunique()@batloop() {
      var cont := $h;

      var pre_kind := ws.fetch(PRE_KIND).find(cont);
      var rid_prop_update := ws.fetch(RID_PROP_UPDATE).find(cont);
      var pid_map := ws.fetch(PID_MAP).find(cont);

      var list := econts.ord_uselect(cont).mirror(); # [i,i] (nodes in current container)
      var epres := list.leftjoin(eitem); # [i,PRE]
      var ekinds := epres.leftfetchjoin(pre_kind); # [i,KIND]
      var esplit := ekinds.splitkind();
      var eelem := esplit.fetch(int(ELEMENT)).mirror(); # [i,i] element nodes in current container
      var epi := esplit.fetch(int(PI)).mirror(); # [i,i] processing instruction nodes in current container
      if ((esplit.fetch(int(TEXT)).count() > 0) or (esplit.fetch(int(COMMENT)).count() > 0) or (esplit.fetch(int(DOCUMENT)).count() > 0)) {
        ERROR("node must be of type ELEMENT, PI, or ATTRIBUTE\n");
      }
      var erids := [swizzle](epres, pid_map);
      var pre_nid := ws.fetch(PRE_NID).find(cont);
      {
        var modified_nid := ws.fetch(MODIFIED_NID);
        modified_nid.reverse().accbuild("hash");
        modified_nid.insert([lng](epres.join(pre_nid).reverse().project(cont).reverse()).[>>](OID_PAGE_BITS));
      }
      {
        var modified_page := ws.fetch(MODIFIED_PAGE);
        modified_page.insert(reverse(project(tunique([oid]([>>]([lng](erids), REMAP_PAGE_BITS))), cont)));
      }
      if (eelem.count() > 0) {
        var eqn := eelem.leftjoin(update_rename); # [i,newQName]
        var eqnid := find_qn_bulk(ws, cont, [+](NS_ACCEL_SEP + NS_ACCEL_SEP, eqn), bit_nil); # [i,newQNID]
        {
          var eelempre := eelem.leftjoin(epres); # [i,PRE]
          var eelemnid := eelempre.leftjoin(pre_nid); # [i,NID]
          var pre_prop := ws.fetch(PRE_PROP).find(cont);
          var eelemqnold := eelempre.leftjoin(pre_prop); # [i,oldQNID]
          var eoldnidqn := eelemnid.reverse().join(eelemqnold); # [NID,oldQNID]
          var enewnidqn := eelemnid.reverse().join(eqnid); # [NID,newQNID]
          ws.fetch(NID_QN_INS_UPDATE).find(cont).insert(enewnidqn);
          ws.fetch(NID_QN_DEL_UPDATE).find(cont).insert(eoldnidqn);
        }
        var upd := eelem.leftjoin(erids).reverse().join(eqnid); # [RID,QNID]
        rid_prop_update.insert(upd);
      }
      if (epi.count() > 0) { 
        var prop_ins := ws.fetch(_PROP_INS).find(cont);
        # rename comes before replace, so we don't have to deal with updated properties
        var pre_prop := ws.fetch(PRE_PROP).find(cont);
        var etgt := epi.leftjoin(update_rename); # [i,tgt]
        var eins := epi.leftjoin(epres).leftjoin(pre_prop);
        var evalid := add_pi_bulk(ws, cont, etgt, eins.leftjoin(prop_ins)); # [i,PROPID]
        var upd := epi.leftjoin(erids).reverse().join(evalid); # [RID,PROPID]
        rid_prop_update.insert(upd);
      }
    }
  }
}

PROC do_update_replace(bat[void, bat] ws, bat[void,oid] update_node_item, bat[void,int] update_node_kind, bat[void,str] update_replace) : void
{
  var conts := update_node_kind.get_container();
  var types := update_node_kind.get_types();
  var attrs := types.ord_uselect(ATTR);
  var elems := types.ord_uselect(ELEM);
  if ((attrs.count() + elems.count()) != types.count()) {
    ERROR("node must be of type ELEMENT or ATTRIBUTE\n");
  }

  if (attrs.count() > 0) {
    attrs := attrs.mirror(); # [i,i]
    var aitem := attrs.leftjoin(update_node_item); # [i,ATID]
    var aconts := attrs.leftjoin(conts); # [i,CONT]
    {
      var modified_attr := ws.fetch(MODIFIED_ATTR);
      modified_attr.insert(aconts.reverse().join(aitem));
    }
    aconts.tunique()@batloop() {
      var cont := $h;
      var list := aconts.ord_uselect(cont).mirror(); # [i,i]
      var aval := list.leftjoin(update_replace); # [i,update_replace]
      var avalid := add_string_bulk(ws, cont, _PROP_VAL, PROP_VAL_UPDATE, aval, true);
      var attr_prop_updates := ws.fetch(ATTR_PROP_UPDATE).find(cont);
      attr_prop_updates.insert(list.leftjoin(aitem).reverse().leftjoin(avalid));
    }
  }

  if (elems.count() > 0) {
    elems := elems.mirror(); # [i,i]
    var eitem := elems.leftjoin(update_node_item); # [i.PRE]
    var econts := elems.leftjoin(conts); # [i,CONT]
    econts.tunique()@batloop() {
      var cont := $h;

      var pre_kind := ws.fetch(PRE_KIND).find(cont);
      var rid_prop_update := ws.fetch(RID_PROP_UPDATE).find(cont);
      var pre_nid := ws.fetch(PRE_NID).find(cont);
      var pid_map := ws.fetch(PID_MAP).find(cont);

      var list := econts.ord_uselect(cont).mirror(); # [i,i] current container
      var epres := list.leftjoin(eitem); # [i,PRE]
      var ekinds := epres.leftfetchjoin(pre_kind); # [i,KIND]
      var esplit := ekinds.splitkind();
      var etext := esplit.fetch(int(TEXT)).mirror(); # [i,i] text node in current container
      var ecomment := esplit.fetch(int(COMMENT)).mirror(); # [i,i] comment node in current container
      var epi := esplit.fetch(int(PI)).mirror(); # [i,i] processing instruction node in current container
      if ((esplit.fetch(int(ELEMENT)).count() > 0) or (esplit.fetch(int(DOCUMENT)).count() > 0)) {
        ERROR("node must be of type TEXT, COMMENT, PI, or ATTRIBUTE\n");
      }
      {
        var modified_nid := ws.fetch(MODIFIED_NID);
        modified_nid.reverse().accbuild("hash");
        modified_nid.insert([lng](epres.join(pre_nid).reverse().project(cont).reverse()).[>>](OID_PAGE_BITS));
      }
      var erids := [swizzle](epres, pid_map); # [i,RID]
      {
        var modified_page := ws.fetch(MODIFIED_PAGE);
        modified_page.insert(reverse(project(tunique([oid]([>>]([lng](erids), REMAP_PAGE_BITS))), cont)));
      }
      if (etext.count() > 0) {
        var elval := etext.leftjoin(update_replace); # [i,update_replace]
        var elvalid := add_string_bulk(ws, cont, _PROP_TEXT, PROP_TEXT_UPDATE, elval, true); # [i,PROPID]
        var upd := etext.leftjoin(erids).reverse().join(elvalid); # [RID,PROPID]
        rid_prop_update.insert(upd);
      }
      if (ecomment.count() > 0) {
        var elval := ecomment.leftjoin(update_replace); # [i,update_replace]
        var elvalid := add_string_bulk(ws, cont, _PROP_COM, PROP_COM_UPDATE, elval, true); # [i,PROPID]
        var upd := ecomment.leftjoin(erids).reverse().join(elvalid); # [RID,PROPID]
        rid_prop_update.insert(upd);
      }
      if (epi.count() > 0) {
        var prop_tgt := ws.fetch(_PROP_TGT).find(cont); 
        # replace comes after rename: deal with renamed processing instruction targets
        var props := epi.leftjoin(erids).leftjoin(rid_prop_update); # [i,old PROPID]
        if (props.count() > 0) {
          var eins := props.mirror().leftjoin(update_replace); # [i,ins]
          var elvalid := add_pi_bulk(ws, cont, props.leftjoin(prop_tgt), eins); # [i,new PROPID]
          var upd := epi.leftjoin(erids).reverse().join(elvalid); # [RID,PROPID]
          rid_prop_update.replace(upd, true);
          epi := epi.kdiff(props); # only do the remaining ones
        }
        if (epi.count() > 0) {
          var pre_prop := ws.fetch(PRE_PROP).find(cont);
          var eins := epi.leftjoin(update_replace); # [i,ins]
          var etgt := epi.leftjoin(epres).leftjoin(pre_prop);
          var elvalid := add_pi_bulk(ws, cont, etgt.leftjoin(prop_tgt), eins); # [i,PROPID]
          var upd := epi.leftjoin(erids).reverse().join(elvalid); # [RID,PROPID]
          rid_prop_update.insert(upd);
        }
      }
    }
  }
}

PROC do_update_insert(bat[void, bat] ws, bat[void,lng] insert_update, bat[void,oid] update_node_item, bat[void,int] update_node_kind, bat[void,oid] update_insert_node_item, bat[void,int] update_insert_node_kind) : void
{
  # extract container ID from update_insert_node_kind
  var batdoccont := update_node_kind.get_container();
  var batcont := update_insert_node_kind.get_container();

  # separate out attribute
  var types := update_insert_node_kind.get_types();
  var attrs := types.ord_uselect(ATTR);
  var elems := types.ord_uselect(ELEM);

  if ((attrs.count() + elems.count()) != types.count()) {
    ERROR("node must be of type ELEMENT or ATTRIBUTE\n");
  }

  if (attrs.count() > 0) {
    # insert attributes into node
    attrs := attrs.mirror(); # [i.i]
    var attrconts := attrs.leftjoin(batdoccont);
    attrconts.tunique()@batloop() {
      var cont := $h;

      var attr_prop_update := ws.fetch(ATTR_PROP_UPDATE).find(cont);
      var attr_own_update := ws.fetch(ATTR_OWN_UPDATE).find(cont);
      var attr_qn_update := ws.fetch(ATTR_QN_UPDATE).find(cont);
      var attr_qn := ws.fetch(ATTR_QN).find(cont);

      var list := attrconts.ord_uselect(cont).mirror(); # [i,i]
      var aconts := list.leftjoin(batcont); # [i,ATTR_CONT]
      var aitems := list.leftjoin(update_insert_node_item); # [i,ATTR_ATTRID]

      var mapping := aitems.mark(0@0); # [i,j] mposjoin wants dense heads, so add a mapping
      var taitems := aitems.tmark(0@0); # [j,ATTR_ATTRID]
      var taconts := aconts.tmark(0@0); # [j,ATTR_CONT]

      var aqns := mposjoin(taitems, taconts, ws.fetch(ATTR_QN)); # [j,ATTR_QNID]
      var aqncont := mposjoin(taitems, taconts, ws.fetch(ATTR_CONT)); # [j,ATTR_QN_CONT]
      var aprefuriloc := mposjoin(aqns, aqncont, ws.fetch(QN_PREFIX_URI_LOC)); # [j,ATTR_PREFIX_URI_LOC]
      var aqnid := find_qn_bulk(ws, cont, aprefuriloc, bit_nil); # [j,DOC_QNID]
      aqnid := mapping.leftjoin(aqnid); # [i,DOC_QNID] map back to original numbering

      var aprops := mposjoin(taitems, taconts, ws.fetch(ATTR_PROP)); # [j,ATTR_PROPID]
      var avals := mposjoin(aprops, taconts, ws.fetch(PROP_VAL)); # [j,ATTR_VAL]
      var avalid := add_string_bulk(ws, cont, _PROP_VAL, PROP_VAL_UPDATE, avals, true); # [j,DOC_VALID]
      avalid := mapping.leftjoin(avalid); # [i,DOC_VALID]

      var aownid := list.leftjoin(update_node_item).leftjoin(ws.fetch(PRE_NID).find(cont)); # [i,DOC_NID]

      # check whether attribute already occurs on element and if so, raise error
      var aownunique := aownid.tunique().mirror(); # [DOC_NID,DOC_NID] unique owners of to-be-inserted attributes
      # get the (updated) set of attributes that belong to affected owners
      var owners := get_attr_own(ws, aownunique, cont) # [DOC_NID,DOC_ATID]
        .reverse() # [DOC_ATID,DOC_NID]
        .kdiff(attr_own_update) # [DOC_ATID,DOC_NID]
        .access(BAT_WRITE).insert(attr_own_update.join(aownunique));
      # for those attributes, get the (updated) QNIDs
      var qnames := owners.mirror().leftjoin(attr_qn).kdiff(attr_qn_update) # [DOC_ATID,DOC_QNID]
        .copy().access(BAT_WRITE).insert(owners.mirror().leftjoin(attr_qn_update));
      # compbine the two
      var oldownqn := owners.reverse().join(qnames); # [DOC_NID,DOC_QNID]
      # same for the new combos
      var newownqn := aownid.reverse().join(aqnid); # [DOC_NID,newDOC_QNID]
      # intersection must be empty
      if (sintersect(oldownqn, newownqn).count() > 0) {
        ERROR("inserted attribute already present on element: use replace value\n");
      }

      # figure out an ID for the new attribute
      var attrid := add_attr_bulk(ws, cont, list.project(oid_nil));
      attr_own_update.insert(aownid.tmark(attrid));
      attr_qn_update.insert(aqnid.tmark(attrid));
      attr_prop_update.insert(avalid.tmark(attrid));
    }
  }

  if (elems.count() > 0) {
    # insert new nodes
    elems := elems.mirror(); # [i,i]
    var modified_nid := ws.fetch(MODIFIED_NID);
    var ancestor_nid := ws.fetch(ANCESTOR_NID);
    var new_page := ws.fetch(NEW_PAGE);
    # one at a time
    elems.leftjoin(update_insert_node_item)@batloop() {
#       if (debug) printf("\nstart of loop for argument %d\n", int($h));
      # insertcont - container from which to insert an element
      # insertitem - item to insert
      var idx := $h; # to ease debugging
      var insertcont := batcont.fetch(idx);
      var insertitem := $t;

      if (not(isnil(ws.fetch(PRE_KIND).find(insertcont).find(insertitem)))) { # don't insert holes
        # insertsize - size of item to be inserted
        # doccont - container of document-to-be-modified
        var insertsize := ws.fetch(PRE_SIZE).find(insertcont).find(insertitem) + 1;
        var doccont := batdoccont.fetch(idx);
        var map_pid := ws.fetch(MAP_PID).find(doccont);
        var map_pid_update := ws.fetch(MAP_PID_UPDATE).find(doccont);

        # figure out where to insert the new element
        # the position is identified using two values: the ID of the node
        # *after* which the element is to be inserted, and the level at which
        # it is to be inserted.
        var docinsertafter_oldpre := update_node_item.fetch(idx);
        var pre_nid := ws.fetch(PRE_NID).find(doccont);
        var pre_kind := ws.fetch(PRE_KIND).find(doccont);
        if (pre_kind.find(docinsertafter_oldpre) >= DOCUMENT) {
          ERROR("cannot insert into a document node\n");
        }
        var nid_rid := ws.fetch(NID_RID).find(doccont);
        var nid_rid_update := ws.fetch(NID_RID_UPDATE).find(doccont);
        var docinsertafter_rid := findupdate(nid_rid, nid_rid_update, pre_nid.find(docinsertafter_oldpre));
        var docinsertlevel := ws.fetch(PRE_LEVEL).find(doccont).find(docinsertafter_oldpre);
        var docinsertcmd := insert_update.fetch(idx);
        if (not(isnil(docinsertcmd)) and not(isnil(docinsertafter_rid))) {
          if (docinsertcmd = lng(UPDATE_INSERT_FIRST)) {
            docinsertlevel :+= chr(1);
          } else if (docinsertcmd = lng(UPDATE_INSERT_LAST)) {
            docinsertlevel :+= chr(1);
            docinsertafter_oldpre := oid(lng(docinsertafter_oldpre) + lng(ws.fetch(PRE_SIZE).find(doccont).find(docinsertafter_oldpre)));
          } else if (docinsertcmd = lng(UPDATE_INSERT_BEFORE)) {
            docinsertafter_oldpre := oid(lng(docinsertafter_oldpre) - 1LL);
          } else if (docinsertcmd = lng(UPDATE_INSERT_AFTER)) {
            docinsertafter_oldpre := oid(lng(docinsertafter_oldpre) + lng(ws.fetch(PRE_SIZE).find(doccont).find(docinsertafter_oldpre)));
          } else if (docinsertcmd = lng(UPDATE_REPLACECONTENT)) {
            docinsertlevel :+= chr(1);
          }
          var pre_size := ws.fetch(PRE_SIZE).find(doccont);
          var rid_size := ws.fetch(_RID_SIZE).find(doccont);
          var rid_size_update := ws.fetch(RID_SIZE_UPDATE).find(doccont);
          var pre_level := ws.fetch(PRE_LEVEL).find(doccont);
          var rid_level := ws.fetch(_RID_LEVEL).find(doccont);
          var rid_level_update := ws.fetch(RID_LEVEL_UPDATE).find(doccont);
          var pre_kind := ws.fetch(PRE_KIND).find(doccont);
          var rid_kind := ws.fetch(_RID_KIND).find(doccont);
          var rid_kind_update := ws.fetch(RID_KIND_UPDATE).find(doccont);
          var pre_prop := ws.fetch(PRE_PROP).find(doccont);
          var rid_prop := ws.fetch(_RID_PROP).find(doccont);
          var rid_prop_update := ws.fetch(RID_PROP_UPDATE).find(doccont);
          var rid_nid := ws.fetch(_RID_NID).find(doccont);
          var rid_nid_update := ws.fetch(RID_NID_UPDATE).find(doccont);

          var docsize;            # current size of document
          # if the size of item 0 was changed, used the new size, else use the original size
          # we assume that page 0 on which item 0 is located already exists...
          {
            var root_rid := antiswizzle(0@0, map_pid_update);
            if (rid_size_update.exist(root_rid)) {
              docsize := rid_size_update.find(root_rid);
            } else {
              docsize := pre_size.find(0@0);
            }
            docsize :+= 1;        # count element as well (not just size of descendants)
          }

          {
            # docinsertafter_oldpre may point to a hole, so back track to a non-hole
            # item; this uses the original (unmodified) document
            var cur := docinsertafter_oldpre;
            var ret := pre_kind.find(cur);
            while (isnil(ret)) {
              # look for non-empty nodes in semi-bulk fashion
              var lim := cur;
              cur := oid(max(0LL, lng(cur) - lng(REMAP_PAGE_SIZE)));
              ret := pre_kind.reverse().ord_select(cur, lim).reverse().ord_select(chr_nil,chr_nil).reverse().max();
              docinsertafter_oldpre := ret;
            }
          }

          # docinsertafter_oldpre uses the "old" numbering scheme (i.e. the unmodified
          # document), translate to the modified document numbering
          # also recalculate docinsertafter_rid since docinsertafter_oldpre may have changed
          docinsertafter_rid := findupdate(nid_rid, nid_rid_update, pre_nid.find(docinsertafter_oldpre));
          var docinsertafter_newpre := swizzle(docinsertafter_rid, map_pid_update);
          var docinsertbefore_newpre := oid(lng(docinsertafter_newpre) + 1);
          var docinsertbefore_rid := antiswizzle(docinsertbefore_newpre, map_pid_update);

          # the start of the page on which the new element is to be inserted
          var pageno := oid(lng(docinsertbefore_newpre) >> REMAP_PAGE_BITS);
          # the physical page id
          var pageid := map_pid_update.reverse().find(pageno);
          # the rid of the first element on the page
          var pagebase := oid(lng(pageid) << REMAP_PAGE_BITS);
          var pagelast := oid(lng(pagebase) + REMAP_PAGE_MASK);
          var isoldpage := false;
          if (map_pid.exist(pageid)) {
            isoldpage := not(isnil(map_pid.find(pageid)));
          }

          if (docinsertcmd = lng(UPDATE_REPLACECONTENT)) {
            var size;
            extend_protect(ws, doccont);
            if (isoldpage) {
              if (rid_size_update.exist(docinsertafter_rid)) {
                size := rid_size_update.find(docinsertafter_rid);
              } else {
                size := pre_size.find(docinsertafter_oldpre);
              }
            } else {
              size := rid_size.find(docinsertafter_rid);
            }
            if (size > 0) {
              do_delete_nodes(ws, doccont, oid(lng(docinsertafter_newpre) + 1), size - 1);
            }
            extend_unprotect(ws, doccont);
          }

          # figure out the number of holes on the page on which the new
          # element is to be inserted (we use the PRE_KIND table for this,
          # holes are indicated with NIL)
          var rid_kind_page;
          if (isoldpage) {
            rid_kind_page := pre_kind.reverse().select(swizzle(pagebase, map_pid), swizzle(pagelast, map_pid)).reverse().seqbase(pagebase);
            var rid_kind_page_update := rid_kind_update.reverse().select(pagebase, pagelast).reverse();
            rid_kind_page := rid_kind_page.copy().access(BAT_WRITE).key(true).myupdate(rid_kind_page_update);
            # record that this page gets changed (new pages don't need to be recorded)
            ws.fetch(MODIFIED_PAGE).insert(doccont, pageid);
          } else {
            rid_kind_page := rid_kind.reverse().select(pagebase, pagelast).reverse();
          }
          var holeatend;          # size of hole at end of page
          {
            var rid_kind_page_used := rid_kind_page.tmark(0@0).uselect(chr_nil, chr_nil);
            if (rid_kind_page_used.count() > 0) {
              holeatend := int(REMAP_PAGE_MASK - lng(max(reverse(rid_kind_page_used))));
            } else {
              holeatend := int(REMAP_PAGE_SIZE);
            }
          }

          if (insertsize > holeatend) {
            # not enough space on the current page

            # datasize is the size of the data after the insertpoint and before the hole at the end
            var datasize := int((REMAP_PAGE_SIZE - (lng(docinsertbefore_newpre) and REMAP_PAGE_MASK)) - holeatend);

            # number of new pages needed (we need to insert insertsize, we
            # have holeatend available; round up to whole number of pages)
            var npages := ((insertsize - holeatend) + int(REMAP_PAGE_MASK)) >> REMAP_PAGE_BITS;
            # the size of the hole we're going to insert
            # we move the bit after the insert point to the last inserted page
            var shiftsize := npages << REMAP_PAGE_BITS;

            var newpages := new(void, oid, npages);
            var lastpage;
            var i := 0;
            while (i < npages) {
              lastpage := ws_newpage(ws, doccont);
              newpages.append(lastpage);
              i :+= 1;
            }
            new_page.insert(newpages.reverse().project(doccont).reverse());
            var cpstart_rid, cpsize, cpwhere_rid, newholeatend;
            if ((holeatend = 0) and (datasize > int(REMAP_PAGE_SIZE / 2))) {
              # insert new pages before current
              newpages := newpages.seqbase(pageno);
              map_pid_update.replace([oid]([+]([int](map_pid_update.select(pageno, oid_nil, true, true)), npages)), true);
              map_pid_update.myupdate(newpages.reverse());
              docinsertafter_rid := antiswizzle(docinsertafter_newpre, map_pid_update);
              docinsertbefore_rid := antiswizzle(docinsertbefore_newpre, map_pid_update);
              # we need to copy the initial part of the current page to the first new page
              cpstart_rid := oid(lng(pageid) << REMAP_PAGE_BITS);
              cpsize := int(REMAP_PAGE_SIZE) - datasize;
              cpwhere_rid := oid(lng(newpages.fetch(0)) << REMAP_PAGE_BITS);
              newholeatend := holeatend;
            } else {
              # insert new pages after current
              newpages := newpages.seqbase(oid(lng(pageno) + 1));
              map_pid_update.replace([oid]([+]([int](map_pid_update.select(pageno, oid_nil, false, true)), npages)), true);
              map_pid_update.myupdate(newpages.reverse());
              # we need to copy the data at the end to the end of the last new page
              cpstart_rid := docinsertbefore_rid;
              cpsize := datasize;
              var pgstart := oid(lng(lastpage) << REMAP_PAGE_BITS);
              # (insertsize - datasize - holeatend) is what gets inserted after the current page;
              # ANDing that with REMAP_PAGE_MASK gives us how much gets inserted on the last page
              cpwhere_rid := oid(int(pgstart) + (((insertsize - datasize) - holeatend) and int(REMAP_PAGE_MASK)));

              if (cpwhere_rid > pgstart) {
                # fix up hole at start of last page since it doesn't
                # extend to the end of the page anymore (this will be
                # overwritten again)
                extend_protect(ws, doccont);
                var update_hole := rid_size.reverse().select(pgstart, cpwhere_rid, true, false).reverse();
                update_hole := [nilor]([-](update_hole.project(int(cpwhere_rid) - 1), [int](update_hole.mirror())), int_nil);
                rid_size.replace(update_hole, true); 
                extend_unprotect(ws, doccont);
              }
              newholeatend := int(REMAP_PAGE_SIZE) - ((int(cpwhere_rid) + cpsize) and int(REMAP_PAGE_MASK));
            }

            # move data after insert point to end of last inserted page
            if (cpsize > 0) {
              var rid_size_data;
              var rid_level_data;
              var rid_kind_data;
              var rid_prop_data;
              var rid_nid_data;
              if (isoldpage) {
                var cpend_rid := oid((lng(cpstart_rid) + cpsize) - 1);
                var cpstart_pre := swizzle(cpstart_rid, map_pid);
                var cpend_pre := oid((lng(cpstart_pre) + cpsize) - 1);
                rid_size_data := pre_size.reverse().select(cpstart_pre, cpend_pre).reverse().seqbase(cpstart_rid);
                rid_level_data := pre_level.reverse().select(cpstart_pre, cpend_pre).reverse().seqbase(cpstart_rid);
                rid_kind_data := pre_kind.reverse().select(cpstart_pre, cpend_pre).reverse().seqbase(cpstart_rid);
                rid_prop_data := pre_prop.reverse().select(cpstart_pre, cpend_pre).reverse().seqbase(cpstart_rid);
                rid_nid_data := pre_nid.reverse().select(cpstart_pre, cpend_pre).reverse().seqbase(cpstart_rid);
                # update slice with replacement values
                var rid_level_update_data := rid_level_update.reverse().select(cpstart_rid, cpend_rid).reverse();
                var rid_kind_update_data := rid_kind_update.reverse().select(cpstart_rid, cpend_rid).reverse();
                var rid_prop_update_data := rid_prop_update.reverse().select(cpstart_rid, cpend_rid).reverse();
                var rid_nid_update_data := rid_nid_update.reverse().select(cpstart_rid, cpend_rid).reverse();
                var rid_size_update_data := rid_size_update.reverse().select(cpstart_rid, cpend_rid).reverse();
                rid_level_data := rid_level_data.copy().access(BAT_WRITE).key(true).myupdate(rid_level_update_data);
                rid_kind_data := rid_kind_data.copy().access(BAT_WRITE).key(true).myupdate(rid_kind_update_data);
                rid_prop_data := rid_prop_data.copy().access(BAT_WRITE).key(true).myupdate(rid_prop_update_data);
                rid_nid_data := rid_nid_data.copy().access(BAT_WRITE).key(true).myupdate(rid_nid_update_data);
                rid_size_data := rid_size_data.copy().access(BAT_WRITE).key(true).myupdate(rid_size_update_data);
                # all these moved nodes are modified
                modified_nid.reverse().accbuild("hash");
                modified_nid.insert([lng](rid_nid_data.select(oid_nil, oid_nil).reverse().project(doccont).reverse()).[>>](OID_PAGE_BITS));
              } else {
                var cpend_rid := oid((lng(cpstart_rid) + cpsize) - 1);
                extend_protect(ws, doccont);
                rid_size_data := rid_size.reverse().select(cpstart_rid, cpend_rid).reverse();
                rid_level_data := rid_level.reverse().select(cpstart_rid, cpend_rid).reverse();
                rid_kind_data := rid_kind.reverse().select(cpstart_rid, cpend_rid).reverse();
                rid_prop_data := rid_prop.reverse().select(cpstart_rid, cpend_rid).reverse();
                rid_nid_data := rid_nid.reverse().select(cpstart_rid, cpend_rid).reverse();
                extend_unprotect(ws, doccont);
                # nodes on a new page have already been added to modified_nid table
              }
              if (holeatend != newholeatend) {
                # Adjust sizes of moved data that point beyond the end of it.
                var rid_end_data := [+]([int](rid_size_data.mirror()), [niland](rid_size_data, INT_MAX));
                var past_end := rid_end_data.uselect(int(cpstart_rid) + cpsize, int_nil);
                rid_size_data.replace([nilplus](past_end.mirror().join(rid_size_data), newholeatend - holeatend), true);
              }
              extend_protect(ws, doccont);
              rid_size.replace(rid_size_data.tmark(cpwhere_rid), true);
              rid_level.replace(rid_level_data.tmark(cpwhere_rid), true);
              rid_kind.replace(rid_kind_data.tmark(cpwhere_rid), true);
              rid_prop.replace(rid_prop_data.tmark(cpwhere_rid), true);
              rid_nid.replace(rid_nid_data.tmark(cpwhere_rid), true);
              extend_unprotect(ws, doccont);

              var nid_rid_data := rid_nid_data.tmark(cpwhere_rid).select(oid_nil, oid_nil).reverse();
              nid_rid_update.myupdate(nid_rid_data);

              # overwrite the just moved data with a hole
              rid_size_data := [nilor]([-](rid_size_data.project((int(cpstart_rid) + cpsize) - 1), [int](rid_size_data.mirror())), int_nil);
              rid_level_data := rid_level_data.project(cast(nil, rid_level.ttype()));
              rid_kind_data := rid_kind_data.project(cast(nil, rid_kind.ttype()));
              rid_prop_data := rid_prop_data.project(oid_nil);
              rid_nid_data := rid_nid_data.project(oid_nil);
              if (isoldpage) {
                rid_size_update.myupdate(rid_size_data);
                rid_level_update.myupdate(rid_level_data);
                rid_kind_update.myupdate(rid_kind_data);
                rid_prop_update.myupdate(rid_prop_data);
                rid_nid_update.myupdate(rid_nid_data);
              } else {
                extend_protect(ws, doccont);
                rid_size.replace(rid_size_data, true);
                rid_level.replace(rid_level_data, true);
                rid_kind.replace(rid_kind_data, true);
                rid_prop.replace(rid_prop_data, true);
                rid_nid.replace(rid_nid_data, true);
                extend_unprotect(ws, doccont);
              }
            }

            # We've inserted new pages and copied the data on the rest
            # of the page to the last inserted page.  Now increase the
            # sizes of all ancestors.  Note that no holes can cross the
            # insert point (docinsertafter_newpre points to a non-hole
            # element)
            # Note, that even though the sizes of the moved data are not
            # yet consistent with the new situation, that is not a
            # problem, since we're only looking at sizes of data before
            # the insertion point.
            var ancestors_oldpre := new(void,oid).seqbase(0@0).append(0@0).append(ll_ancestor_or_self(new(void,oid).seqbase(0@0).append(0@0), new(void,oid).seqbase(0@0).append(docinsertafter_oldpre), pre_size, pre_level));
            var ancestors_nid := ancestors_oldpre.join(pre_nid);
            var ancestors_newrid := ancestors_nid.join(nid_rid).access(BAT_WRITE).myupdate(ancestors_nid.join(nid_rid_update));
            var ancestors_newpre := [swizzle](ancestors_newrid, [oid](map_pid_update));
            var ancestors_size := ancestors_oldpre.join(pre_size).access(BAT_WRITE).myupdate(ancestors_newrid.join(rid_size_update));
            # now figure out which ancestors *end* at the hole
            var ancestors_end := [oid]([+]([lng](ancestors_newpre), ancestors_size));

            var new_size := new(oid, int); # collect new sizes until we apply
            # select those that start before and end inside the moved data
            var indata := ancestors_end.uselect(docinsertbefore_newpre, oid(lng(docinsertbefore_newpre) + datasize), true, false);
            if (indata.count() > 0) {
              new_size.insert([+](indata.mirror().join(ancestors_size), (shiftsize + holeatend) - newholeatend));
            }
            # select those that end inside the hole at the end
            var inhole := ancestors_end.uselect(oid(lng(docinsertbefore_newpre) + datasize), oid(lng(docinsertbefore_newpre) + datasize + holeatend), true, false);
            if (inhole.count() > 0) {
              new_size.insert([int]([-](((((lng(docinsertbefore_newpre) + datasize) - 1) + (shiftsize + holeatend)) - newholeatend), [lng](inhole.mirror().join(ancestors_newpre)))));
            }
            # select those that end after the moved data
            var behind := ancestors_end.uselect(oid(lng(docinsertbefore_newpre) + datasize + holeatend), oid_nil);
            if (behind.count() > 0) {
              new_size.insert([+](behind.mirror().join(ancestors_size), shiftsize));
            }
            # figure out new and old pages since they need to be updated differently and apply
            var upd_pages := [oid]([>>]([lng](new_size.mirror().join(ancestors_newrid)), REMAP_PAGE_BITS));
            var is_new_page := [isnil](upd_pages.outerjoin(map_pid));
            var old_page := is_new_page.uselect(false);
            if (old_page.count() > 0) {
              rid_size_update.myupdate(ancestors_newrid.reverse().join(old_page.mirror().join(new_size)));
            }
            var new_page := is_new_page.uselect(true);
            if (new_page.count() > 0) {
              var tmp := ancestors_newrid.reverse().join(new_page.mirror().join(new_size));
              extend_protect(ws, doccont);
              rid_size.myupdate(tmp);
              extend_unprotect(ws, doccont);
            }
            # register which ancestors are being changed
            ancestor_nid.insert(new_size.project(doccont).reverse().join(ancestors_nid));

            # recalculate isoldpage (and pageid) after having inserted the new pages
            pageid := map_pid_update.reverse().find(pageno);
            isoldpage := false;
            if (map_pid.exist(pageid)) {
              isoldpage := not(isnil(map_pid.find(pageid)));
            }
          } else {
            # the inserted data fits on the current page but we may have to
            # move data around to make a hole big ineough in the right place

            # holesize is the size of the hole after the node where we have to insert
            var holesize := 0;
            {
              var rid := docinsertbefore_rid;
              var pre := docinsertbefore_newpre;
              if (int(pre) >= docsize) {
                holesize := int(REMAP_PAGE_SIZE - (lng(docsize) and REMAP_PAGE_MASK));
              }
              while ((int(pre) < docsize) and (rid <= pagelast)) {
                var s := 0;
                if (isoldpage) {
                  if (rid_size_update.exist(rid)) {
                    s := rid_size_update.find(rid);
                  } else {
                    s := pre_size.find(swizzle(rid, map_pid));
                  }
                } else {
                  extend_protect(ws, doccont);
                  if (rid_size.exist(rid)) 
                      s := rid_size.find(rid);
                  extend_unprotect(ws, doccont);
                }
                if (isnil(niland(s, int_nil))) {
                  # we're looking at a hole
                  var h := niland(s, INT_MAX) + 1;
                  holesize :+= h;
                  pre := oid(lng(pre) + h);
                  rid := oid(lng(rid) + h);
                } else {
                  # not a hole, terminate loop
                  pre := oid(docsize);
                }
              }
            }
            if (holesize < insertsize) {
              # not enough space in the right place
              movedata(ws, doccont, oid(lng(docinsertbefore_newpre) + holesize), int(REMAP_PAGE_SIZE - ((lng(docinsertbefore_newpre) and REMAP_PAGE_MASK) + holesize + holeatend)), insertsize - holesize);
            }
          }

          # we now have a hole that is big enough below docinsertafter_newpre

          # We must adjust the sizes of all ancestors of the new element;
          # this means all elements that currently end at the newly created
          # hole and whose level is less than the level of the new element

          {
            # the document collection node (0@0) is ancestor to everybody but not returned by ll_ancestor
            var ancestors_oldpre := new(void,oid).seqbase(0@0).append(0@0).append(ll_ancestor_or_self(new(void,oid).seqbase(0@0).append(0@0), new(void,oid).seqbase(0@0).append(docinsertafter_oldpre), pre_size, pre_level));
            var ancestors_nid := ancestors_oldpre.join(pre_nid);
            var ancestors_newrid := ancestors_nid.join(nid_rid).access(BAT_WRITE).myupdate(ancestors_nid.join(nid_rid_update));
            var ancestors_newpre := [swizzle](ancestors_newrid, [oid](map_pid_update));
            var ancestors_size := ancestors_oldpre.join(pre_size).access(BAT_WRITE).myupdate(ancestors_newrid.join(rid_size_update));
            # now figure out which ancestors *end* at the hole
            var ancestors_end := [oid]([+]([lng](ancestors_newpre), ancestors_size)).uselect(docinsertafter_newpre).mirror();
            # figure out the level and select those with level < docinsertlevel
            var upd_ancestors := ancestors_end.join(ancestors_oldpre).join(pre_level).access(BAT_WRITE).myupdate(ancestors_end.join(ancestors_newrid).join(rid_level_update)).uselect(chr_nil, docinsertlevel, true, false);
            # now update the sizes of these ancestors (if any)
            if (upd_ancestors.count() > 0) {
              # calculate new size
              var new_size := [+](upd_ancestors.mirror().join(ancestors_size), insertsize);
              # figure out new and old pages since they need to be updated differently
              var upd_pages := [oid]([>>]([lng](upd_ancestors.mirror().join(ancestors_newrid)), REMAP_PAGE_BITS));
              var is_new_page := [isnil](upd_pages.outerjoin(map_pid));
              var old_page := is_new_page.uselect(false);
              if (old_page.count() > 0) {
                rid_size_update.myupdate(ancestors_newrid.reverse().join(old_page.mirror().join(new_size)));
              }
              var new_page := is_new_page.uselect(true);
              if (new_page.count() > 0) {
                var tmp := ancestors_newrid.reverse().join(new_page.mirror().join(new_size));
                extend_protect(ws, doccont);
                rid_size.myupdate(tmp);
                extend_unprotect(ws, doccont);
              }
            }
          }

          # copy the new element to the hole and do all the other work
          # only look at unedited version of to-be-inserted document
          # (required semantics)

          # get NID values for new nodes
          var newnids := ws_newnids(ws, doccont, insertsize).hmark(0@0); # [idx,NID]
          var newnididx := 0@0;

          var insert_pre_size := ws.fetch(PRE_SIZE).find(insertcont);
          var insert_pre_level := ws.fetch(PRE_LEVEL).find(insertcont);
          var insert_pre_prop := ws.fetch(PRE_PROP).find(insertcont);
          var insert_pre_kind := ws.fetch(PRE_KIND).find(insertcont);
          var insert_pre_nid := ws.fetch(PRE_NID).find(insertcont);
          var insert_pre_cont := ws.fetch(PRE_CONT).find(insertcont);
          var samedoc := false;     # perhaps we can optimize
          if (is_constant(insert_pre_cont)) {
            insert_pre_cont := bat2constant(insert_pre_cont);
            if (insert_pre_cont = doccont) {
              samedoc := true;
            }
          }
          var insert_attr_qn := ws.fetch(ATTR_QN).find(insertcont);
          var insert_attr_cont := ws.fetch(ATTR_CONT).find(insertcont);
          var sameattrdoc := false;
          if (is_constant(insert_attr_cont)) {
            insert_attr_cont := bat2constant(insert_attr_cont);
            if (insert_attr_cont = doccont) {
              sameattrdoc := true;
            }
          }
          var insert_attr_prop := ws.fetch(ATTR_PROP).find(insertcont);

          var attr_own_update := ws.fetch(ATTR_OWN_UPDATE).find(doccont);
          var attr_qn_update := ws.fetch(ATTR_QN_UPDATE).find(doccont);
          var attr_prop_update := ws.fetch(ATTR_PROP_UPDATE).find(doccont);
          var nid_qn_ins_update := ws.fetch(NID_QN_INS_UPDATE).find(doccont);

          # the difference in level for inserted elements between original
          # document and updated document
          var leveldiff := int(docinsertlevel) - int(insert_pre_level.find(insertitem));
  #         if (debug) printf("leveldiff %d\n", leveldiff);

          if (insert_pre_kind.find(insertitem) = TEXT) {
            # if we're inserting a text node, we may have to merge
            # consecutive text nodes
            # we remember the position in a hacky place
            # note, insertsize == 1 in this case
            var texts := ws.fetch(UPDATED_TEXT);
            texts.insert(doccont, newnids.find(newnididx));
          }

          # do the actual insert: copy the data from the source document
          # into the prepared hole
          # docinsertpoint is the point at which the next batch is going to be inserted
          var docinsertpoint := oid(lng(docinsertafter_newpre) + 1);
          while (insertsize > 0) {
            # docinsertpoint_rid is the RID for the new data
            var docinsertpoint_rid := antiswizzle(docinsertpoint, map_pid_update);
            # recalculate isoldpage for this iteration
            pageid := oid(lng(docinsertpoint_rid) >> REMAP_PAGE_BITS);
            isoldpage := false;
            if (map_pid.exist(pageid)) {
              isoldpage := not(isnil(map_pid.find(pageid)));
            }
            # one page at a time
            var batchsize := REMAP_PAGE_SIZE - (lng(docinsertpoint) and REMAP_PAGE_MASK);
            if (batchsize > lng(insertsize)) {
              batchsize := lng(insertsize);
            }
            var insprop := insert_pre_prop.reverse().select(insertitem, oid(lng(insertitem) + batchsize), true, false).reverse();
            var inskind := insert_pre_kind.reverse().select(insertitem, oid(lng(insertitem) + batchsize), true, false).reverse();
            var inssize := insert_pre_size.reverse().select(insertitem, oid(lng(insertitem) + batchsize), true, false).reverse();
            var inslevel := insert_pre_level.reverse().select(insertitem, oid(lng(insertitem) + batchsize), true, false).reverse();
            var insnid := insert_pre_nid.reverse().select(insertitem, oid(lng(insertitem) + batchsize), true, false).reverse();
            inslevel := [chr]([+]([int](inslevel), leveldiff));
            var texts, comments, pis, elems;
            var lastdata := insertitem;
            {
              var kindbats := inskind.splitkind();
              texts := kindbats.fetch(int(TEXT));
              comments := kindbats.fetch(int(COMMENT));
              pis := kindbats.fetch(int(PI));
              elems := kindbats.fetch(int(ELEMENT));
              var docs := kindbats.fetch(int(DOCUMENT));
              if (docs.count() > 0) {
                lastdata := docs.reverse().max();
              }
            }
            var insnnids := inskind.uselect(chr_nil, chr_nil).mark(newnididx).join(newnids); # PRE-New NID
            newnididx := oid(wrd(newnididx) + insnnids.count());
            if (not(samedoc)) {
              insprop := insprop.copy().access(BAT_WRITE);
              if (texts.count() > 0) {
                lastdata := max(lastdata, texts.reverse().max());
                texts := texts.mirror().tmark(0@0);
                var textval := mposjoin(texts.leftjoin(insert_pre_prop), texts.leftjoin(insert_pre_cont), ws.fetch(PROP_TEXT));
                var textids := add_string_bulk(ws, doccont, _PROP_TEXT, PROP_TEXT_UPDATE, textval, true);
                insprop.replace(texts.reverse().join(textids));
              }
              if (comments.count() > 0) {
                lastdata := max(lastdata, comments.reverse().max());
                comments := comments.mirror().tmark(0@0);
                var commentval := mposjoin(comments.leftjoin(insert_pre_prop), comments.leftjoin(insert_pre_cont), ws.fetch(PROP_COM));
                var commentids := add_string_bulk(ws, doccont, _PROP_COM, PROP_COM_UPDATE, commentval, true);
                insprop.replace(comments.reverse().join(commentids));
              }
              if (pis.count() > 0) {
                lastdata := max(lastdata, pis.reverse().max());
                pis := pis.mirror().tmark(0@0);
                var insval := mposjoin(pis.leftjoin(insert_pre_prop), pis.leftjoin(insert_pre_cont), ws.fetch(PROP_INS));
                var tgtval := mposjoin(pis.leftjoin(insert_pre_prop), pis.leftjoin(insert_pre_cont), ws.fetch(PROP_TGT));
                var piids := add_pi_bulk(ws, doccont, tgtval, insval);
                insprop.replace(pis.reverse().join(pis.reverse().join(piids)));
              }
            }
            if (elems.count() > 0) {
              lastdata := max(lastdata, elems.reverse().max());
              elems := elems.mirror(); # PRE-PRE subset of the elements
              var prenids := elems.leftjoin(insert_pre_nid); # PRE-NID subset of the elements
              var preattrs := get_attr_own(ws, prenids, insertcont); # PRE_ATID of all attrs on the element subset, ATID is key
              var attrattrs := preattrs.reverse().mirror(); # ATID-ATID
              var attrnqnids; # ATID-New QNID
              if (sameattrdoc) {
                attrnqnids := attrattrs.leftjoin(insert_attr_qn);
              } else {
                var a := attrattrs.tmark(0@0);
                var prefurilocval := mposjoin(a.leftjoin(insert_attr_qn), a.leftjoin(insert_attr_cont), ws.fetch(QN_PREFIX_URI_LOC));
                attrnqnids := a.reverse().leftjoin(find_qn_bulk(ws, doccont, prefurilocval, bit_nil));
              }
              var attrnprops; # ATID-New PROPID
              if (sameattrdoc) {
                attrnprops := attrattrs.leftjoin(insert_attr_prop);
              } else {
                var a := attrattrs.tmark(0@0);
                var propval := mposjoin(a.leftjoin(insert_attr_prop), a.leftjoin(insert_attr_cont), ws.fetch(PROP_VAL));
                attrnprops := a.reverse().leftjoin(add_string_bulk(ws, doccont, _PROP_VAL, PROP_VAL_UPDATE, propval, true));
              }
              if (attrattrs.count() > 0) {
                # first claim enough entries in the ATTR table with the lock set
                var attr_id := add_attr_bulk(ws, doccont, attrattrs.project(oid_nil));
                var nattrattrs := attrattrs.tmark(attr_id); # New ATID-ATID
                var nattrnqnids := nattrattrs.join(attrnqnids); # New ATID-New QNID
                var nattrnids := nattrattrs.join(preattrs.reverse()).join(insnnids); # New ATID-New NID
                var nattrnprops := nattrattrs.join(attrnprops); # New ATID-New PROPID
                attr_own_update.insert(nattrnids);
                attr_qn_update.insert(nattrnqnids);
                attr_prop_update.insert(nattrnprops);
              }
              if (not(samedoc)) {
                var e := elems.tmark(0@0);
                var prefurilocval := mposjoin(e.leftjoin(insprop), e.leftjoin(insert_pre_cont), ws.fetch(QN_PREFIX_URI_LOC));
                var elemids := e.reverse().leftjoin(find_qn_bulk(ws, doccont, prefurilocval, bit_nil));
                insprop.replace(elemids);
              }
              var newnidqn := elems.leftjoin(insnnids).reverse().join(insprop); # [newNID,newQNID]
              nid_qn_ins_update.insert(newnidqn);
            }
            var lastsize := inssize.find(oid((lng(insertitem) + batchsize) - 1));
            if (isnil(niland(lastsize, int_nil))) {
              if (niland(lastsize, INT_MAX) > 0) {
                # last to-be-inserted node is in the middle of a hole, fix the hole to end at this node
                var hole := inssize.reverse().select(lastdata, oid_nil, false, true).reverse();
                inssize := inssize.copy().access(BAT_WRITE).replace([nilor]([-](hole.project(hole.count()), [int](hole.mark(1@0))), int_nil));
              }
            }
            insnid := insnid.copy().access(BAT_WRITE).replace(insnnids);
            insnid := insnid.tmark(docinsertpoint_rid);
            nid_rid_update.insert(insnid.select(oid_nil, oid_nil).reverse());
            insprop := insprop.tmark(docinsertpoint_rid);
            inssize := inssize.tmark(docinsertpoint_rid);
            inslevel := inslevel.tmark(docinsertpoint_rid);
            inskind := inskind.tmark(docinsertpoint_rid);
            if (isoldpage) {
              rid_size_update.myupdate(inssize);
              rid_level_update.myupdate(inslevel);
              rid_kind_update.myupdate(inskind);
              rid_prop_update.myupdate(insprop);
              rid_nid_update.myupdate(insnid);
            } else {
              extend_protect(ws, doccont);
              rid_size.replace(inssize, true);
              rid_level.replace(inslevel, true);
              rid_kind.replace(inskind, true);
              rid_prop.replace(insprop, true);
              rid_nid.replace(insnid, true);
              extend_unprotect(ws, doccont);
            }

            insertsize :-= int(batchsize);
            docinsertpoint := oid(lng(docinsertpoint) + batchsize);
            insertitem := oid(lng(insertitem) + batchsize);
          }
        }
      }
    }
  }
}

# note: extend read-lock must be taken while calling this function!!
PROC do_delete_nodes(bat[void, bat] ws, oid cont, oid newpre, int delsize) : void
{
  var map_pid := ws.fetch(MAP_PID).find(cont);
  var map_pid_update := ws.fetch(MAP_PID_UPDATE).find(cont);
  var pre_size := ws.fetch(PRE_SIZE).find(cont);
  var rid_size := ws.fetch(_RID_SIZE).find(cont);
  var rid_size_update := ws.fetch(RID_SIZE_UPDATE).find(cont);
  var nid_rid := ws.fetch(NID_RID).find(cont);
  var nid_rid_update := ws.fetch(NID_RID_UPDATE).find(cont);
  var pre_kind := ws.fetch(PRE_KIND).find(cont);
  var rid_kind := ws.fetch(_RID_KIND).find(cont);
  var rid_kind_update := ws.fetch(RID_KIND_UPDATE).find(cont);
  var pre_level := ws.fetch(PRE_LEVEL).find(cont);
  var rid_level := ws.fetch(_RID_LEVEL).find(cont);
  var rid_level_update := ws.fetch(RID_LEVEL_UPDATE).find(cont);
  var pre_prop := ws.fetch(PRE_PROP).find(cont);
  var rid_prop := ws.fetch(_RID_PROP).find(cont);
  var rid_prop_update := ws.fetch(RID_PROP_UPDATE).find(cont);
  var pre_nid := ws.fetch(PRE_NID).find(cont);
  var rid_nid := ws.fetch(_RID_NID).find(cont);
  var rid_nid_update := ws.fetch(RID_NID_UPDATE).find(cont);
  var nid_qn_ins_update := ws.fetch(NID_QN_INS_UPDATE).find(cont);
  var nid_qn_del_update := ws.fetch(NID_QN_DEL_UPDATE).find(cont);
  var modified_page := ws.fetch(MODIFIED_PAGE);
  var modified_nid := ws.fetch(MODIFIED_NID);

  var pageno := oid(lng(newpre) >> REMAP_PAGE_BITS);
  var pageid := map_pid_update.reverse().find(pageno);
  var next_pagebase := oid((lng(pageno) + 1) << REMAP_PAGE_BITS);
  var isoldpage := false;
  if (map_pid.exist(pageid)) {
    isoldpage := not(isnil(map_pid.find(pageid)));
  }
  var rid := oid((lng(newpre) and REMAP_PAGE_MASK) or (lng(pageid) << REMAP_PAGE_BITS));
  var ancestors_newpre;
  var nid;
  if (isoldpage) {
    if (rid_nid_update.exist(rid)) {
      nid := rid_nid_update.find(rid);
    } else {
      nid := pre_nid.find(swizzle(rid, map_pid));
    }
  } else {
    nid := rid_nid.find(rid);
  }
  if (not(isnil(nid))) {
    if (nid_rid.exist(nid)) {
      var oldrid := nid_rid.find(nid);
      if (not(isnil(oldrid))) {
        var oldpre := swizzle(oldrid, map_pid);
        var ancestors_oldpre := new(void,oid).seqbase(0@0).append(0@0).append(ll_ancestor(new(void,oid).seqbase(0@0).append(0@0), new(void,oid).seqbase(0@0).append(oldpre), pre_size, pre_level));
        var ancestors_nid := ancestors_oldpre.join(pre_nid);
        var ancestors_newrid := ancestors_nid.join(nid_rid).access(BAT_WRITE).myupdate(ancestors_nid.join(nid_rid_update));
        ancestors_newpre := [swizzle](ancestors_newrid, map_pid_update);
      }
    }
  }
  if (isnil(ancestors_newpre)) {
    # if we could not use the fast ll_ancestor, use the slow(er) mil_ancestor
    ancestors_newpre := mil_ancestor(ws, cont, newpre);
  }
  while (delsize >= 0) {
    var rid := oid((lng(newpre) and REMAP_PAGE_MASK) or (lng(pageid) << REMAP_PAGE_BITS));
    var nsize; # new size we're going to write (may join with consecutive hole)
    var pgsize; # size of hole we're dealing with this iteration (does not cross page boundary)
    if (oid(lng(newpre) + delsize) >= next_pagebase) {
      # new hole extends into next page
      nsize := (int(next_pagebase) - int(newpre)) - 1;
      pgsize := nsize;
    } else {
      # new hole wholly contained on this page
      pgsize := delsize;
      if (oid((lng(newpre) + delsize) + 1) >= next_pagebase) {
        # new hole ends at page boundary
        nsize := delsize;
      } else {
        # room to spare after new hole, maybe coalesce with consecutive hole
        var next_rid := oid((lng(rid) + delsize) + 1);
        if (isoldpage) {
          if (rid_size_update.exist(next_rid)) {
            nsize := rid_size_update.find(next_rid);
          } else {
            nsize := pre_size.find(swizzle(next_rid, map_pid));
          }
        } else {
          nsize := rid_size.find(next_rid);
        }
        if (isnil(niland(nsize, int_nil))) { # result of niland(nsize, int_nil) is either 0 or int_nil
          # there is a following hole
          nsize := (delsize + niland(nsize, INT_MAX)) + 1;
        } else {
          # no following hole
          nsize := delsize;
        }
      }
    }

    if (isoldpage) {
      modified_page.insert(cont, pageid);
    }

    var update_data;
    var oldpre := oid_nil;
    if (isoldpage) {
      if (rid_nid_update.exist(rid)) {
        var nid := rid_nid_update.find(rid);
        if (nid_rid.exist(nid)) {
          var oldrid := nid_rid.find(nid);
          oldpre := swizzle(oldrid, map_pid);
        }
      } else {
        oldpre := swizzle(rid, map_pid);
      }

      var pid_map := ws.fetch(PID_MAP).find(cont);
      if (not(isnil(oldpre))) {
        update_data := [swizzle](pre_nid.reverse().select(oldpre, oid(lng(oldpre) + pgsize)), pid_map).hmark(rid);
        update_data.access(BAT_WRITE).myupdate(rid_nid_update.reverse().select(rid, oid(lng(rid) + pgsize)).reverse());
      } else {
        # we're deleting a new node, so there is no data about it (or its descendants) in pre_nid
        update_data := rid_nid_update.reverse().select(rid, oid(lng(rid) + pgsize)).reverse().sort();
      }
    } else {
      update_data := rid_nid.reverse().select(rid, oid(lng(rid) + pgsize)).reverse();
    }
    {
      var nid_qn_del;
      if (isoldpage) {
        var rk := update_data.mirror().outerjoin(rid_kind_update).access(BAT_WRITE); # [rid,KIND/nil]
        rk.replace(rk.uselect(chr_nil).mirror().[swizzle](map_pid).join(pre_kind));  # [rid,KIND]
        var r := rk.uselect(ELEMENT).mirror();                                       # [rid,rid] (elements)
        var rn := r.outerjoin(rid_nid_update).access(BAT_WRITE);                     # [rid,NID/nil]
        rn.replace(rn.uselect(oid_nil).mirror().[swizzle](map_pid).join(pre_nid));   # [rid,NID]
        var rp := rn.join(nid_rid)          # [rid,oldRID]
          .[swizzle](map_pid)               # [rid,oldPRE]
          .join(pre_prop);                  # [rid,oldPROP]
        nid_qn_del := rn.reverse().join(rp);
        nid_qn_ins_update.delete(rn.reverse()); # in case we're deleting new nodes
      } else {
        nid_qn_del := update_data.reverse() # [NID,rid]
          .join(rid_kind)                   # [NID,KIND]
          .uselect(ELEMENT)                 # [NID,nil] (only elements)
          .mirror()                         # [NID,NID] (only elements)
          .join(nid_rid)                    # [NID,oldRID]
          .[swizzle](map_pid)               # [NID,oldPRE]
          .join(pre_prop);                  # [NID,oldPROP]
      }
      nid_qn_ins_update.delete(nid_qn_del);
      nid_qn_del_update.insert(nid_qn_del);
    }

    var rid_nid_page;
    if (isoldpage) {
      if (isnil(oldpre)) {
        rid_nid_page := rid_nid_update.reverse().select(rid, oid(lng(rid) + pgsize)).reverse();
      } else {
        rid_nid_page := pre_nid.reverse().select(oldpre, oid(lng(oldpre) + pgsize)).hmark(rid);
        var rid_nid_page_update := rid_nid_update.reverse().select(rid, oid(lng(rid) + pgsize));
        rid_nid_page := rid_nid_page.copy().access(BAT_WRITE).key(true).replace(rid_nid_page_update);
      }
    } else {
      rid_nid_page := rid_nid.reverse().select(rid, oid(lng(rid) + pgsize)).reverse();
    }

    var nid_rid_updates := rid_nid_page.select(oid_nil, oid_nil).reverse().project(oid_nil);
    nid_rid_update := myupdate(nid_rid_update, nid_rid_updates);
    modified_nid.reverse().accbuild("hash");
    modified_nid.insert([lng](nid_rid_update.project(cont).reverse()).[>>](OID_PAGE_BITS));

    if (lng(nsize) = REMAP_PAGE_MASK) {
      # deleting whole page
      var del_page := ws.fetch(DEL_PAGE);
      del_page.insert(cont, pageid);
      # use the *before* version of map_pid_update (not that this is
      # particularly important: we're dealing with ancestors which
      # necessarily come before the to-be-deleted page)
      var pid_map_update := map_pid_update.select(oid_nil, oid_nil).reverse().sort().tmark(0@0);
      # now update map_pid_update
      map_pid_update.replace([oid]([-]([int](map_pid_update.select(pageno, oid_nil, false, true)), 1)), true);
      map_pid_update.replace(pageid, oid_nil, true);
      # figure out which ancestors' sizes need to adjusted
      var ancestors_newrid := [swizzle](ancestors_newpre, pid_map_update);
      var ancestors_isnewpage := [isnil](outerjoin([oid]([>>]([lng](ancestors_newrid), REMAP_PAGE_BITS)), map_pid));
      var ancestors_nid;
      {
        # distinguish the three cases:
        # rid is on new page
        var a := ancestors_isnewpage.uselect(true).mirror().join(ancestors_newrid).join(rid_nid);
        # rid is on old page and was modified
        var b := ancestors_isnewpage.uselect(false).mirror().join(ancestors_newrid).join(rid_nid_update);
        # rid is on old page and was not modified
        var c := ancestors_isnewpage.uselect(false).kdiff(b).mirror().join(ancestors_newrid).[swizzle](map_pid).join(pre_nid);
        # combine
        ancestors_nid := a.access(BAT_WRITE).insert(b).insert(c).order().tmark(0@0);
      }
      var ancestors_oldpre := ancestors_nid.join(nid_rid).[swizzle](map_pid);
      var ancestors_size := ancestors_oldpre.join(pre_size).access(BAT_WRITE).myupdate(ancestors_newrid.join(rid_size_update));
      # now figure out which ancestors get smaller
      var ancestors_end := [oid]([+]([lng](ancestors_newpre), ancestors_size));
      var inpage := ancestors_end.uselect(newpre, oid(lng(newpre) + REMAP_PAGE_SIZE), true, false);
      var afterpage := ancestors_end.uselect(oid(lng(newpre) + REMAP_PAGE_SIZE), oid_nil, true, false);
      # nodes that end in the deleted page are truncated to end at the deleted node
      var ancestors_newsize := [int]([-](lng(newpre), [lng](inpage.mirror().join(ancestors_newpre))));
      # nodes that end beyond the deleted page are just made smaller
      ancestors_newsize.access(BAT_WRITE).insert([-](afterpage.mirror().join(ancestors_size), int(REMAP_PAGE_SIZE)));
      var old_page := ancestors_isnewpage.uselect(false);
      if (old_page.count() > 0) {
         rid_size_update.myupdate(ancestors_newrid.reverse().join(old_page.mirror().join(ancestors_newsize)));
      }
      var new_page := ancestors_isnewpage.uselect(true);
      if (new_page.count() > 0) {
        rid_size.myupdate(ancestors_newrid.reverse().join(new_page.mirror().join(ancestors_newsize)));
      }
      # remember which ancestors were changed
      var ancestor_nid := ws.fetch(ANCESTOR_NID);
      ancestor_nid.insert(ancestors_newpre.project(cont).reverse().join(ancestors_nid));
      # compensate for per-page house keeping below
      newpre := oid((lng(newpre) - pgsize) - 1);
      next_pagebase := oid(lng(next_pagebase) - REMAP_PAGE_SIZE);
    } else {
      update_data := [nilor]([-](update_data.project(nsize), [int](update_data.mark(0@0))), int_nil);
      if (isoldpage) {
        rid_size_update.myupdate(update_data);
      } else {
        rid_size.replace(update_data, true);
      }

      update_data := update_data.project(cast(nil, rid_level_update.ttype()));
      if (isoldpage) {
        rid_level_update.myupdate(update_data);
      } else {    
        rid_level.replace(update_data, true);
      }

      update_data := update_data.project(cast(nil, rid_kind_update.ttype()));
      if (isoldpage) {
        rid_kind_update.myupdate(update_data);
      } else {
        rid_kind.replace(update_data, true);
      }

      update_data := update_data.project(oid_nil);
      if (isoldpage) {
        rid_prop_update.myupdate(update_data);
      } else {
        rid_prop.replace(update_data, true);
      }

      update_data := update_data.project(oid_nil);
      if (isoldpage) {
        rid_nid_update.myupdate(update_data);
      } else {
        rid_nid.replace(update_data, true);
      }
    }

    delsize :-= pgsize + 1;
    newpre := oid((lng(newpre) + pgsize) + 1);
    if (newpre >= next_pagebase) {
      # per page house keeping
      pageno := oid(lng(newpre) >> REMAP_PAGE_BITS);
      pageid := map_pid_update.reverse().find(pageno);
      next_pagebase := oid((lng(pageno) + 1) << REMAP_PAGE_BITS);
      isoldpage := false;
      if (map_pid.exist(pageid)) {
        isoldpage := not(isnil(map_pid.find(pageid)));
      }
    }
  }
}

PROC do_update_delete(bat[void, bat] ws, bat[void,oid] update_node_item, bat[void,int] update_node_kind) : void
{
  var conts := update_node_kind.get_container();
  var types := update_node_kind.get_types();
  var attrs := types.ord_uselect(ATTR);
  var elems := types.ord_uselect(ELEM);

  if (attrs.count() > 0) {
    attrs := attrs.mirror(); # [i,i]
    var aconts := attrs.leftjoin(conts); # [i,CONT]
    {
      var modified_attr := ws.fetch(MODIFIED_ATTR);
      modified_attr.insert(aconts.reverse().join(update_node_item));
    }
    aconts.tunique()@batloop() {
      var cont := $h;
      var list := aconts.ord_uselect(cont).mirror(); # [i,i]
      var attr_prop_updates := ws.fetch(ATTR_PROP_UPDATE).find(cont);
      var attr_own_updates := ws.fetch(ATTR_OWN_UPDATE).find(cont);
      var attr_qn_updates := ws.fetch(ATTR_QN_UPDATE).find(cont);
      var attrlist := list.leftjoin(update_node_item).reverse().project(oid_nil);
      attr_prop_updates.insert(attrlist);
      attr_own_updates.insert(attrlist);
      attr_qn_updates.insert(attrlist);
    }
  }

  if (elems.count() > 0) {
    # extract the update_node_item values that refer to elements
    elems := elems.mirror();
    update_node_item := elems.leftjoin(update_node_item);

    # the "update_node_item" parameter contains PRE values
    # instead of NID values; the PRE values refer to the unmodified
    # document
    update_node_item@batloop() {
      var oldpre := $t;         # the original PRE value of the to-be-deleted node
      var cont := conts.fetch($h);
      var pre_kind := ws.fetch(PRE_KIND).find(cont);
      if (pre_kind.find(oldpre) >= DOCUMENT) {
        ERROR("document nodes cannot be deleted");
      }
      if (ws.fetch(PRE_LEVEL).find(cont).find(oldpre) = chr(0)) {
        ERROR("root nodes cannot be deleted");
      }

      # translate PRE value to NID value which is also valid in the modified document
      var pre_nid := ws.fetch(PRE_NID).find(cont);
      var nid := pre_nid.find(oldpre);
      var nid_rid := ws.fetch(NID_RID).find(cont);
      var nid_rid_update := ws.fetch(NID_RID_UPDATE).find(cont);
      var rid;
      if (nid_rid_update.exist(nid)) {
        rid := nid_rid_update.find(nid);
      } else {
        rid := nid_rid.find(nid);
      }
      # if (isnil(rid)) the element is already gone, so nothing more to do
      if (not(isnil(rid))) {
        var map_pid := ws.fetch(MAP_PID).find(cont);
        var map_pid_update := ws.fetch(MAP_PID_UPDATE).find(cont);
        var pre := swizzle(rid, map_pid_update);
        var pre_size := ws.fetch(PRE_SIZE).find(cont);
        var rid_size := ws.fetch(_RID_SIZE).find(cont);
        var rid_size_update := ws.fetch(RID_SIZE_UPDATE).find(cont);
        var pageid := oid(lng(rid) >> REMAP_PAGE_BITS);
        var isoldpage := false;
        if (map_pid.exist(pageid)) {
          isoldpage := not(isnil(map_pid.find(pageid)));
        }

        extend_protect(ws, cont);
        var size;
        if (isoldpage) {
          if (rid_size_update.exist(rid)) {
            size := rid_size_update.find(rid);
          } else {
            size := pre_size.find(oldpre);
          }
        } else {
          size := rid_size.find(rid);
        }

        if (not(isnil(niland(size, int_nil)))) { # result of niland(size, int_nil) is either 0 or int_nil
          var rid_kind := ws.fetch(_RID_KIND).find(cont);
          var rid_kind_update := ws.fetch(RID_KIND_UPDATE).find(cont);
          var rid_nid := ws.fetch(_RID_NID).find(cont);
          var rid_nid_update := ws.fetch(RID_NID_UPDATE).find(cont);

          # record where we're deleting a node
          {
            var pid_map_update := map_pid_update.select(oid_nil, oid_nil).reverse().sort().tmark(0@0);
            var docsize;                # current size of document
            # if the size of item 0 was changed, used the new size, else use the original size
            # we assume that page 0 on which item 0 is located already exists...
            {
              var root_rid := swizzle(0@0, pid_map_update);
              if (rid_size_update.exist(root_rid)) {
                docsize := rid_size_update.find(root_rid);
              } else {
                docsize := pre_size.find(0@0);
              }
            }
            var nxtpre := oid(lng(pre) + size + 1), nxtrid, nxtsiz := 0;
            if (int(nxtpre) < docsize) {
              nxtrid := swizzle(nxtpre, pid_map_update);
              if (map_pid.exist(oid(lng(nxtrid) >> REMAP_PAGE_BITS))) {
                if (rid_size_update.exist(nxtrid)) {
                  nxtsiz := rid_size_update.find(nxtrid);
                } else {
                  nxtsiz := pre_size.find(swizzle(nxtrid, map_pid));
                }
              } else {
                nxtsiz := rid_size.find(nxtrid);
              }
            }
            while ((int(nxtpre) < docsize) and isnil(niland(nxtsiz, int_nil))) {
              nxtpre := oid(lng(nxtpre) + niland(nxtsiz, INT_MAX) + 1);
              if (int(nxtpre) < docsize) {
                nxtrid := swizzle(nxtpre, pid_map_update);
                if (map_pid.exist(oid(lng(nxtrid) >> REMAP_PAGE_BITS))) {
                  if (rid_size_update.exist(nxtrid)) {
                    nxtsiz := rid_size_update.find(nxtrid);
                  } else {
                    nxtsiz := pre_size.find(swizzle(nxtrid, map_pid));
                  }
                } else {
                  nxtsiz := rid_size.find(nxtrid);
                }
              }
            }
            # only record spot if next node is a TEXT since only then do
            # we have to worry about coalescing text nodes
            if (int(nxtpre) < docsize) {
              var nxtknd;
              var nxtnid;
              if (map_pid.exist(oid(lng(nxtrid) >> REMAP_PAGE_BITS))) {
                if (rid_kind_update.exist(nxtrid)) {
                  nxtknd := rid_kind_update.find(nxtrid);
                } else {
                  nxtknd := pre_kind.find(swizzle(nxtrid, map_pid));
                }
                if (rid_nid_update.exist(nxtrid)) {
                  nxtnid := rid_nid_update.find(nxtrid);
                } else {
                  nxtnid := pre_nid.find(swizzle(nxtrid, map_pid));
                }
              } else {
                nxtknd := rid_kind.find(nxtrid);
                nxtnid := rid_nid.find(nxtrid);
              }
              if (nxtknd = TEXT) {
                ws.fetch(UPDATED_TEXT).insert(cont, nxtnid);
              }
            }
          }

          # do the actual deleting in a subroutine
          do_delete_nodes(ws, cont, pre, size);
        }
        extend_unprotect(ws, cont);
      }
    }
  }
}

PROC add_attr_bulk(BAT[void,bat] ws, oid cont, BAT[oid,oid] oid_list) : oid
{ 
  coll_lock_set(ws, cont, COLL_SHORTLOCK, "add_attr_bulk");
  var ws_logtime := usec();
  var attr_own := ws.fetch(_ATTR_OWN).find(cont);
  var attr_prop := ws.fetch(_ATTR_PROP).find(cont);
  var attr_qn := ws.fetch(_ATTR_QN).find(cont);
  pf_assert(CATCH({ attr_own.append(oid_list, true);
                    attr_prop.append(oid_list, true);
                    attr_qn.append(oid_list, true); }), "master update failed (add_attr)");
  var attr_id := oid(attr_prop.count() - oid_list.count());
  coll_lock_unset(ws, cont, COLL_SHORTLOCK, "add_attr_bulk", ws_logtime);
  return attr_id;
}

PROC add_string_bulk(bat[void,bat] ws, oid cont, int tableid, int updtableid, bat[oid,str] valbat, bit dolock) : bat[oid,oid]
{
  if (dolock) coll_lock_set(ws, cont, COLL_SHORTLOCK, "add_string_bulk");
  var ws_logtime := usec();
  var table := ws.fetch(tableid).find(cont);
  var oldsize := count(table);
  var missing := valbat.tdiff(table).tunique();
  if (missing.count() > 0)
    pf_assert(CATCH(table.append(missing.reverse(), true)), "master update failed (add_string)");
  var logvals := table.slice(oldsize, count(table)).copy();
  var validbat := valbat.leftjoin(table.reverse());
  if (dolock) coll_lock_unset(ws, cont, COLL_SHORTLOCK, "add_string_bulk", ws_logtime);
  if (count(logvals) > 0) {
    ws.fetch(updtableid).find(cont).insert(logvals); # log new values
  }
  return validbat;
}

# note: coll lock must be taken while calling this function!!
PROC add_pi_bulk(bat[void,bat] ws, oid cont, bat[oid,str] tgtbat, bat[oid,str] insbat) : bat[oid,oid]
{
  coll_lock_set(ws, cont, COLL_SHORTLOCK, "add_pi_bulk");
  var ws_logtime := usec();
  var instgtbat := [+]([+](insbat, NS_ACCEL_SEP), tgtbat);
  var prop_ins := ws.fetch(_PROP_INS).find(cont);
  var prop_tgt := ws.fetch(_PROP_TGT).find(cont);
  var prop_ins_tgt := [+]([+](prop_ins, NS_ACCEL_SEP), prop_tgt);
  var idbat := instgtbat.outerjoin(prop_ins_tgt.reverse());
  var newidlist := idbat.uselect(oid_nil);
  if (newidlist.count() > 0) {
    # insert missing strings
    # first figure out unique combinations
    var instgtunique := newidlist.mirror().join(instgtbat).tunique().reverse().seqbase(0@0);
    # then extract instruction and target
    var instgtuniquesplit := instgtunique.splitbat(NS_ACCEL_SEP);
    # add them to respective BATs

    pf_assert(CATCH({ prop_ins.append(instgtuniquesplit.fetch(0), true);
                      prop_tgt.append(instgtuniquesplit.fetch(1), true); }), "master update failed (add_pi)");

    # recalculate ids
    prop_ins_tgt := [+]([+](prop_ins, NS_ACCEL_SEP), prop_tgt);
    idbat := instgtbat.outerjoin(prop_ins_tgt.reverse());
    # we also need to log the new values
    ws.fetch(PROP_INS_UPDATE).find(cont).insert(prop_ins.join(instgtuniquesplit.fetch(0).reverse().mirror()));
    ws.fetch(PROP_TGT_UPDATE).find(cont).insert(prop_tgt.join(instgtuniquesplit.fetch(1).reverse().mirror()));
  }
  coll_lock_unset(ws, cont, COLL_SHORTLOCK, "add_pi_bulk", ws_logtime);
  return idbat;
}


PROC find_qn_bulk(bat[void,bat] ws, oid cont, bat[oid,str] pref_uri_loc, bit add) : bat[oid,oid]
{
  var qn_prefix_uri_loc := ws.fetch(_QN_PREFIX_URI_LOC).find(cont);
  var qn_map := pref_uri_loc.outerjoin(qn_prefix_uri_loc.reverse());
  var UPDATE := 0;
  if (isnil(add)) { # update case
    add := true; UPDATE := QN_HISTOGRAM_UPDATE - QN_HISTOGRAM;
  }
  if (add) {
    var miss := qn_map.uselect(oid_nil);
    if (miss.count() > 0) {
      var qn_loc := ws.fetch(_QN_LOC).find(cont);
      var qn_uri := ws.fetch(_QN_URI).find(cont);
      var qn_prefix := ws.fetch(_QN_PREFIX).find(cont);
      var qn_uri_loc := ws.fetch(_QN_URI_LOC).find(cont);
      var qn_histogram := ws.fetch(_QN_HISTOGRAM).find(cont);

      # get the unique missing strings and append them
      coll_lock_set(ws, cont, COLL_SHORTLOCK, "find_qn_bulk");
      var ws_logtime := usec();
      var _base := oid(count(qn_histogram));
      var _delta := reverse(miss).join(pref_uri_loc).histogram();
      var _pref_uri_loc := _delta.hmark(_base);
      var _histogram := [lng](_delta.tmark(_base));
      var _split := splitbat(_pref_uri_loc, NS_ACCEL_SEP);
      var _pref  := _split.fetch(0);
      var _uri   := _split.fetch(1);
      var _loc   := _split.fetch(2);
      var _uri_loc := [+]([+](_uri, NS_ACCEL_SEP), _loc);
      pf_assert(CATCH({ qn_loc.append(_loc, true); 
                        qn_uri.append(_uri, true); 
                        qn_prefix.append(_pref, true);
                        qn_uri_loc.append(_uri_loc, true);
                        qn_prefix_uri_loc.append(_pref_uri_loc, true); 
                        qn_histogram.append(_histogram, true); }), "master update failed (add_qn)");

      # recalculate map now
      qn_map := pref_uri_loc.outerjoin(qn_prefix_uri_loc.reverse());
      coll_lock_unset(ws, cont, COLL_SHORTLOCK, "find_qn_bulk", ws_logtime);

      # log new values
      ws.fetch(QN_LOC + UPDATE).find(cont).insert(_loc);
      ws.fetch(QN_URI + UPDATE).find(cont).insert(_uri);
      ws.fetch(QN_PREFIX + UPDATE).find(cont).insert(_pref);
      ws.fetch(QN_URI_LOC + UPDATE).find(cont).insert(_uri_loc);
      ws.fetch(QN_PREFIX_URI_LOC + UPDATE).find(cont).insert(_pref_uri_loc);
      ws.fetch(QN_HISTOGRAM + UPDATE).find(cont).insert(_histogram);
    }
  }
  return qn_map;
}

PROC fix_consecutive_texts(bat[void, bat] ws, bat[oid,oid] contnid) : void
{
  # for now ignore position information
  contnid.reverse().tunique()@batloop() {
    var cont := $h;

    extend_protect(ws, cont);

    var map_pid := ws.fetch(MAP_PID).find(cont);
    var map_pid_update := ws.fetch(MAP_PID_UPDATE).find(cont);

    var pre_level := ws.fetch(PRE_LEVEL).find(cont);
    var rid_level := ws.fetch(_RID_LEVEL).find(cont);
    var rid_level_update := ws.fetch(RID_LEVEL_UPDATE).find(cont);
    var pre_kind := ws.fetch(PRE_KIND).find(cont);
    var rid_kind := ws.fetch(_RID_KIND).find(cont);
    var rid_kind_update := ws.fetch(RID_KIND_UPDATE).find(cont);
    var pid_map_update := map_pid_update.select(oid_nil, oid_nil).reverse().sort().tmark(0@0);

    var lvkd := new(oid, sht, rid_level.count()); # [RID,(level<<8)|kind]

    # go through virtual (updated) pages in logical order
    pid_map_update@batloop() {
      var pageno := $h;         # logical (PRE)
      var pageid := $t;         # physical (RID)

      var isoldpage := false;
      if (map_pid.exist(pageid)) {
        isoldpage := not(isnil(map_pid.find(pageid)));
      }

      var pagestart := oid(lng(pageid) << REMAP_PAGE_BITS); # RID value of start of page
      var pagelast := oid(lng(pagestart) + REMAP_PAGE_MASK); # RID value of last of page

      var lvpg;
      var kdpg;
      if (isoldpage) {
        var pgstart := swizzle(pagestart, map_pid); # PRE value of start of page
        var pglast := swizzle(pagelast, map_pid); # PRE value of last of page
        lvpg := pre_level.reverse().select(pgstart, pglast).reverse().seqbase(pagestart); # [RID,LEVEL]
        var lvpgup := rid_level_update.reverse().select(pagestart, pagelast).reverse(); # [RID,LEVEL]
        lvpg := lvpg.copy().access(BAT_WRITE).key(true).replace(lvpgup); # [RID,LEVEL]
        kdpg := pre_kind.reverse().select(pgstart, pglast).reverse().seqbase(pagestart); # [RID,KIND]
        var kdpgup := rid_kind_update.reverse().select(pagestart, pagelast).reverse(); # [RID,KIND]
        kdpg := kdpg.copy().access(BAT_WRITE).key(true).replace(kdpgup); # [RID,KIND]
      } else {
        lvpg := rid_level.reverse().select(pagestart, pagelast).reverse(); # [RID,LEVEL]
        kdpg := rid_kind.reverse().select(pagestart, pagelast).reverse(); # [RID,KIND]
      }

      # select all non-hole entries for both level and kind tables
      var l := lvpg.ord_select(chr_nil,chr_nil); # [RID,LEVEL]
      var k := l.mirror().leftfetchjoin(kdpg); # [RID,KIND]
      # combine them into a single table
      var x := [+]([<<]([sht](l),8),[sht](k)); # [RID,(LEVEL<<8)|KIND]
      lvkd.insert(x);
    }

    lvkd := [sht]([swizzle]([oid](lvkd).reverse(), map_pid_update).reverse()); # [newPRE,(level<<8)|kind]] 

    # use an undocumented feature of CTrefine: it does not check whether
    # the lhs is sorted
    # we now get unique OIDs for each stretch of consecutive elements
    # with the same value
    var r := CTrefine(lvkd, lvkd.project(nil)); # [newPRE,GRP] tsorted
    # s is a list of nodes where there are more than one consecutive
    # nodes of the same type at the same level, the rhs is the per-group OID
    var s := r.leftjoin(r.reverse().{count}().ord_uselect(2, int_nil).mirror()); # [newPRE,GRP] tsorted subselection of r
    # and now select the text elements from there
    var t := [and](lvkd, sht(255)).ord_uselect(sht(TEXT)).mirror().leftjoin(s).chk_order(); # [newPRE,GRP] hsorted,tsorted subselection of s
    # at this point we must do the real merging and deleting of text
    # nodes, but at least we know where they are
    if (t.count() > 0) {
      var rid_size := ws.fetch(_RID_SIZE).find(cont);
      var rid_size_update := ws.fetch(RID_SIZE_UPDATE).find(cont);
      var pre_prop := ws.fetch(PRE_PROP).find(cont);
      var rid_prop := ws.fetch(_RID_PROP).find(cont);
      var rid_prop_update := ws.fetch(RID_PROP_UPDATE).find(cont);
      var pre_nid := ws.fetch(PRE_NID).find(cont);
      var rid_nid := ws.fetch(_RID_NID).find(cont);
      var rid_nid_update := ws.fetch(RID_NID_UPDATE).find(cont);
      var t1 := [swizzle](t.reverse(), pid_map_update).reverse(); # [RID,GRP]

      var t1p := [oid]([>>]([lng](t1.mirror()), REMAP_PAGE_BITS)); # [RID,PGID]
      # [RID,RID] combos from t1.mirror() that refer to old pages
      var t1mold := t1p.leftjoin(map_pid).ord_uselect(oid_nil, oid_nil).mirror();
      # [RID,RID] combos from t1.mirror() that refer to new pages
      var t1mnew := t1p.reverse().kdiff(map_pid).reverse().mirror();

      var t2 := [swizzle](t1mold, map_pid); # [RID,oldPRE]
      # [RID,PROP] combination from unmodified nodes, modified nodes, and new nodes
      var rp := t2.join(pre_prop).access(BAT_WRITE).key(true).myupdate(t1mold.join(rid_prop_update)).myupdate(t1mnew.join(rid_prop)); # [RID,PROP]

      var gp := t1.reverse().leftjoin(rp); # [GRP,PROP]
      coll_lock_set(ws, cont, COLL_SHORTLOCK, "consec-texts");
      var ws_logtime := usec();
      var c := gp.leftfetchjoin(ws.fetch(_PROP_TEXT).find(cont)); # [GRP,content] hsorted
      var v := string_join(c, c.kunique().project("")); # [GRP,content] combined content with unique GRP values
      var i := add_string_bulk(ws, cont, _PROP_TEXT, PROP_TEXT_UPDATE, v, false); # [GRP,SID] OID into _PROP_TEXT table
      coll_lock_unset(ws, cont, COLL_SHORTLOCK, "consec-texts", ws_logtime);
      var p := t.reverse().kunique().reverse(); # [PRE,GRP] one representative from t per group
      var p1 := [swizzle](p.reverse(), pid_map_update).reverse(); # [RID,GRP]
      var h1 := kdiff(t1, p1).project(oid_nil); # [RID,nil] the other RIDs

      p1.leftjoin(i)@batloop() {
        var pageid := oid(lng($h) >> REMAP_PAGE_BITS);
        var isoldpage := false;
        if (map_pid.exist(pageid)) {
          isoldpage := not(isnil(map_pid.find(pageid)));
        }
        if (isoldpage) {
          rid_prop_update.myupdate($h, $t);
        } else {
          rid_prop.replace($h, $t, true);
        }
      }
      var nid_rid_update := ws.fetch(NID_RID_UPDATE).find(cont);
      h1@batloop() {
        var pageid := oid(lng($h) >> REMAP_PAGE_BITS);
        var isoldpage := false;
        if (map_pid.exist(pageid)) {
          isoldpage := not(isnil(map_pid.find(pageid)));
        }
        var nid;
        if (isoldpage) {
          if (rid_nid_update.exist($h)) {
            nid := rid_nid_update.find($h);
          } else {
            nid := pre_nid.find(swizzle($h, map_pid));
          }
          rid_prop_update.myupdate($h, $t);
          rid_size_update.myupdate($h, int_nil);
          rid_nid_update.myupdate($h, $t);
          rid_kind_update.myupdate($h, chr_nil);
          rid_level_update.myupdate($h, chr_nil);
        } else {
          nid := rid_nid.find($h);
          rid_prop.replace($h, $t, true);
          rid_size.replace($h, int_nil, true);
          rid_nid.replace($h, $t, true);
          rid_kind.replace($h, chr_nil, true);
          rid_level.replace($h, chr_nil, true);
        }
        nid_rid_update.myupdate(nid, oid_nil);
      }
    }
    extend_unprotect(ws, cont);
  }
}
@* Module Implementation
@h
#ifndef PF_SUPPORT_H
#define PF_SUPPORT_H

#include <monet.h>
#include "pathfinder.h"
#include "pf_support.proto.h"
#include <monet_interpreter.h>
#include <monettime.h>

#endif
@c

#include "pf_config.h"
#include "pf_support.h"
#include <gdk_scanselect.h> /* for type-specific HT_bunfastins_nocheck_noinc(), until they're moved to gdk.mx */
#include <algebra.h> /* needed for result size estimation in CMDenumerate */
#include <math.h> /* needed for round_up */

#if SIZEOF_OID == SIZEOF_INT
#define oidoid_bunfastins(b,h,t) intint_bunfastins(b,h,t);
#else
#define oidoid_bunfastins(b,h,t) lnglng_bunfastins(b,h,t);
#endif
@c
/* delete a file from the local tmp/ directory in the dbfarm; protect against abuse */
int CMDcleantmpdir(lng *lim) {
        DIR *dirp = opendir("tmp");
        char path[PATHLENGTH];
        struct dirent *dent;

        if (dirp == NULL)
  	        return GDK_SUCCEED;
        while ((dent = readdir(dirp)) != NULL) {
		struct stat st;
                if ((dent->d_name[0] == '.') && ((dent->d_name[1] == 0) || (dent->d_name[1] == '.' && dent->d_name[2] == 0))) {
                        continue;
                }
                GDKfilepath(path, "tmp/", dent->d_name, NULL);
		if (stat(path, &st) == 0 && st.st_mtime < *lim) {
                    int ret = unlink(path);
                    IODEBUG THRprintf(GDKout, "#unlink %s = %d\n", path, ret);
                }
        }
        closedir(dirp);
  	return GDK_SUCCEED;
}

int CMDdelete_nodes_prepare_pre_size( BAT** ret, BAT *pre_size, BAT *pre_ )
{
	BAT *res = NULL;
	size_t cnt = 0;
	int bs_res;
	BUN dst_res;
	int new_size = int_nil;
	bit triv_prop;

	BATcheck(pre_size, "delete_nodes_prepare_pre_size");
	BATcheck(pre_,     "delete_nodes_prepare_pre_size");
	ERRORcheck(!BAThdense(pre_size),
	           "delete_nodes_prepare_pre_size: head of BAT pre_size must be dense.\n");
	ERRORcheck((pre_->htype!=TYPE_oid) && (pre_->htype!=TYPE_void),
	           "ERRORcheck: head of BAT pre_ must be oid.\n");
	ERRORcheck((pre_->htype==TYPE_void) && !BAThdense(pre_),
	           "ERRORcheck: head of BAT pre_ must not be nil.\n");
	ERRORcheck(!(BAThordered(pre_)&1),
	           "delete_nodes_prepare_pre_size: head of BAT pre_ must be sorted.\n");

	cnt = BATcount(pre_);
        res = BATnew(TYPE_oid,TYPE_int,cnt);
	if (res == NULL) {
		GDKerror("delete_nodes_prepare_pre_size: BATnew(TYPE_oid, TYPE_int, "SZFMT") failed.\n", cnt);
		return GDK_FAIL;
	}
	bs_res   = BUNsize(res);
	dst_res  = BUNfirst(res) + ((cnt-1) * bs_res);

	if (BAThdense(pre_)) {
		oid fst_pre = pre_->hseqbase;
		oid lst_pre = fst_pre + cnt;
		oid cur_pre;
		BUN src_pre_size = NULL;
		BUNfndVOID(src_pre_size, pre_size, &lst_pre);
		if (src_pre_size != NULL) {
			int old_size = *(int*)BUNtloc(pre_size, src_pre_size);
			if (old_size&(1<<31)) {
				new_size = (((old_size&GDK_int_max) + 1) | 1<<31);
			} else {
				new_size = 1<<31;
			}
		} else {
			new_size = 1<<31;
		}
		
		for (cur_pre = lst_pre - 1 ; cur_pre >= fst_pre ; cur_pre--) {
			oidint_bunfastins_nocheck_noinc(res, dst_res, &cur_pre, &new_size);
			dst_res -= bs_res;
			new_size = (((new_size&GDK_int_max) + 1) | 1<<31); /* new_size--; */
		}
	} else {
		oid prev_pre = oid_nil;
		int bs_pre_  = BUNsize(pre_);
		BUN fst_pre_ = BUNfirst(pre_);
		BUN lst_pre_ = BUNlast(pre_);
		BUN cur_pre_;
		for (cur_pre_ = lst_pre_ - bs_pre_ ; cur_pre_ >= fst_pre_ ; cur_pre_ -= bs_pre_) {
			oid cur_pre = *(oid*)BUNhloc(pre_, cur_pre_);
			if (cur_pre + 1 != prev_pre) {
				BUN src_pre_size = NULL;
				prev_pre = cur_pre + 1;
				BUNfndVOID(src_pre_size, pre_size, &prev_pre);
				if (src_pre_size != NULL) {
					int old_size = *(int*)BUNtloc(pre_size, src_pre_size);
					if (old_size&(1<<31)) {
						new_size = (((old_size&GDK_int_max) + 1) | 1<<31); /* old_size - 1 */
					} else {
						new_size = 1<<31;
					}
				} else {
					new_size = 1<<31;
				}
			}
			oidint_bunfastins_nocheck_noinc(res, dst_res, &cur_pre, &new_size);
			prev_pre = cur_pre;
			dst_res -= bs_res;
			new_size = (((new_size&GDK_int_max) + 1) | 1<<31); /* new_size--; */
		}
	}
	

        if (!res->batDirty) res->batDirty = TRUE;
	res->batBuns->free = cnt * bs_res;
	BATsetcount(res, cnt);
        triv_prop = (BATcount(res) < 2);
        BATkey(res,TRUE);
        BATkey(BATmirror(res),triv_prop);
        res->hsorted = GDK_SORTED;
        res->tsorted = (triv_prop?GDK_SORTED:FALSE);
        res->hdense = (triv_prop || BAThdense(pre_));
        res->tdense = FALSE;
        if (BATcount(res) == 0) {
                BATseqbase(res, (oid)0);
        } else if (res->hdense) {
                BATseqbase(res, *(oid*)BUNhloc(res,BUNfirst(res)));
        }
	BATseqbase(BATmirror(res), oid_nil);

        *ret = res;
	return GDK_SUCCEED;
}  

int 
CMDisFakeProject(bit *r, ptr v, int tpe) {
	*r = (tpe == TYPE_bat)?is_fake_project((BAT*) v):1;
	return GDK_SUCCEED;
}

int
CMDfakeProject(BAT **res, ptr val, int tpe) 
{
	assert(val);
	if (tpe == TYPE_bat) {
		*res = (BAT*) val;
		BBPfix((*res)->batCacheid);
		return GDK_SUCCEED;
	}
	*res = BATnew(TYPE_void, tpe, 1);
	if (*res) {
		if (BUNappend(*res, val, FALSE)) return GDK_SUCCEED;
		BBPreclaim(*res);
	}
	return GDK_FAIL;
}

#include <constant.proto.h> /* for CMDconstCopy */

int
CMDdeFakeProject(ptr ret, int *tpe, ptr val, int t) 
{
	if (t == TYPE_bat) {
		BAT *b = (BAT*) val;
		if (!is_fake_project(b)) {
			*tpe = TYPE_bat;
			*(BAT**) ret = b;
			BBPfix(b->batCacheid);
			return GDK_SUCCEED;
		}
		val = BUNtail(b, BUNfirst(b));
		t = ATOMtype(b->ttype);
	} 
	return CMDconstCopy(ret, val, *tpe = t);
}

int
CMDfetchConvert(ptr res, int *tpe, BAT* b, int* pos) 
{
	int ret = GDK_FAIL;
	if (*pos >= 0 || ((size_t) *pos) < BATcount(b)) {
		bat bid = *(bat*) BUNtail(b, BUNptr(b, BUNindex(b, BUNfirst(b))+*pos));
		int fix = BBPfix(bid);
		BAT *bn = NULL;
		if (fix == 0 || (bn=BBPdescriptor(bid)) == NULL) {
			if (fix) BBPunfix(bid);
			GDKerror("fetch(%s) illegal BAT at position %d.\n", BBP_logical(b->batCacheid), *pos); 
		} else if (is_fake_project(bn)) {
			ret = CMDconstCopy(res, BUNfirst(bn), *tpe = ATOMtype(bn->ttype));
			BBPunfix(bid);
		} else {
			ret = GDK_SUCCEED;
			*(BAT**)res = bn;
			*tpe = TYPE_bat;
		}
	} else {
		GDKerror("fetch(%s) illegal position %d.\n", BBP_logical(b->batCacheid), *pos); 
	}
	return ret;
}

int
CMDinsertConvert(BAT** res, BAT *b, ptr h, ptr t, int tpe) 
{
	int ret = GDK_FAIL;
	BAT *bn = NULL;

	if (!tpe)
		return GDK_FAIL;
	assert(h);
	assert(t);
	if (CMDfakeProject(&bn, t, tpe) == GDK_SUCCEED) {
		if (BUNins(*res = b, BAThtype(b) == TYPE_bat ? (ptr) &((BAT *) h)->batCacheid : h, &bn->batCacheid, FALSE)) {
			BBPfix(b->batCacheid);
			ret = GDK_SUCCEED;
		}
		BBPunfix(bn->batCacheid);
	}
	return ret;
}

int
CMDappendConvert(BAT** res, BAT *b, ptr t, int tpe) 
{
	int ret = GDK_FAIL;
	BAT *bn = NULL;

	if (!tpe)
		return GDK_FAIL;
	assert(t);
	if (CMDfakeProject(&bn, t, tpe) == GDK_SUCCEED) {
		if (BUNappend(*res = b, &bn->batCacheid, FALSE)) {
			BBPfix(b->batCacheid);
			ret = GDK_SUCCEED;
		}
		BBPunfix(bn->batCacheid);
	}
	return ret;
}

static int 
merged_union( BAT** res, int nbats, BAT **b) 
{
	BAT *bn[MAXPARAMS>>1], *BN;
	BUN cur[MAXPARAMS], dst[MAXPARAMS>>1], DST;
	int bs[MAXPARAMS], bns[MAXPARAMS>>1], BS;
	size_t idx[MAXPARAMS], cnt[MAXPARAMS];
	int npairs = 1, i, j, k, any, b0 = -1, b1 = -1;
	bit concat = FALSE;
	chr *w = NULL;
	size_t sze = 0, h = 0;
	
	*res = NULL;

	/* check arguments */
	ERRORcheck(BATcount(b[0])>1 && !(BATtordered(b[0])&1), "merged_union: tail of first BAT must be sorted.\n");
	ERRORcheck(BATcount(b[1])>1 && !(BATtordered(b[1])&1), "merged_union: tail of second BAT must be sorted.\n");
	if (nbats&1) {
		GDKerror("merged_union: uneven number of BATs: %d.\n", nbats);
		return GDK_FAIL;
	}
	npairs = nbats>>1;

	for (i=0; i<nbats; i+=2) {
		bit ci, cj;
		j = i+1;
		ci = !is_fake_project(b[i]);
		cj = !is_fake_project(b[j]);
		if (ci && b0 < 0) {
			b0 = i;
		}
		if (cj && b1 < 0) {
			b1 = j;
		}
		if (ci && !BAThdense(b[i])) {
			GDKerror("merged_union: BAT %d must have a dense head.\n", i+1);
			return GDK_FAIL;
		}
		if (cj && !BAThdense(b[j])) {
			GDKerror("merged_union: BAT %d must have a dense head.\n", j+1);
			return GDK_FAIL;
		}
		if (ATOMtype(b[i]->ttype) != ATOMtype(b[j]->ttype)) {
			GDKerror("merged_union: BATs %d (ttype=%d) & %d (ttype=%d) must have the same tail types.\n", 
					i+1, b[i]->ttype, j+1, ATOMtype(b[j]->ttype));
			return GDK_FAIL;
		}
		if (ci && b0 >= 0 && b[i]->hseqbase != b[b0]->hseqbase) {
			GDKerror("merged_union: BAT %d (hseqbase="OIDFMT") must have the same hseqbase as BAT %d (hseqbase="OIDFMT").\n", 
					i+1, b[i]->hseqbase, b0+1, b[b0]->hseqbase);
			return GDK_FAIL;
		}
		if (cj && b1 >= 0 && b[j]->hseqbase != b[b1]->hseqbase) {
			GDKerror("merged_union: BAT %d (hseqbase="OIDFMT") must have the same hseqbase as BAT %d (hseqbase="OIDFMT").\n", 
					j+1, b[j]->hseqbase, b1+1, b[b1]->hseqbase);
			return GDK_FAIL;
		}
		if (ci && b0 >= 0 && BATcount(b[i]) != BATcount(b[b0])) {
			GDKerror("merged_union: BAT %d ("SZFMT" BUNs) must have the same size as BAT %d ("SZFMT" BUNs).\n", 
					i+1, BATcount(b[i]), b0+1, BATcount(b[b0]));
			return GDK_FAIL;
		}
		if (cj && b1 >= 0 && BATcount(b[j]) != BATcount(b[b1])) {
			GDKerror("merged_union: BAT %d ("SZFMT" BUNs) must have the same size as BAT %d ("SZFMT" BUNs).\n", 
					j+1, BATcount(b[j]), b1+1, BATcount(b[b1]));
			return GDK_FAIL;
		}
	}
	if (b0 < 0) {
		GDKerror("merged_union: at least one of the 'odd' BATs must be materialized, i.e., no 'fake_project'.\n");
		return GDK_FAIL;
	}
	if (b1 < 0) {
		GDKerror("merged_union: at least one of the 'even' BATs must be materialized, i.e., no 'fake_project'.\n");
		return GDK_FAIL;
	}

	/* create result BATs */

	sze = BATcount(b[b0]) + BATcount(b[b1]);
	BN = BATnew(TYPE_void, TYPE_bat, npairs);
	if (BN == NULL) {
		GDKerror("merged_union: BATnew(TYPE_void, TYPE_bat, %d) failed.\n", npairs);
		return GDK_FAIL;
	}
	for (k=0; k<npairs; k++) {
		i = k<<1;
		bn[k] = BATnew(TYPE_void, ATOMtype(b[i]->ttype), sze);
		if (bn[k] == NULL) {
			GDKerror("merged_union: BATnew(TYPE_void, %s, " SZFMT ") failed.\n", ATOMname(ATOMtype(b[i]->ttype)), sze);
			while (k>0) {
				BBPreclaim(bn[--k]);
			}
			BBPreclaim(BN);
			return GDK_FAIL;
		}
	}
	if (sze > 0) {
		w = (chr*)GDKmalloc(sze);
		if (w == NULL) {
			GDKerror("merged_union: GDKmalloc(" SZFMT ") failed.\n", sze);
			goto cleanup;
		}
	}

	/* do the merged_union */

	for (k=0; k<npairs; k++) {
		bns[k] = BUNsize(bn[k]);
		dst[k] = BUNlast(bn[k]);
	}
	for (i=0; i<nbats; i++) {
		if (is_fake_project(b[i])) {
			bs[i] = 0;
		} else {
			bs[i] = BUNsize(b[i]);
		}
		cur[i] = BUNfirst(b[i]);
		idx[i] = 0;
		if (i&1) {
			cnt[i] = BATcount(b[b1]);
		} else {
			cnt[i] = BATcount(b[b0]);
		}
	}
@= merged_union_0
	/*  @1: ATOMstorage(b[@3]->ttype) (chr, sht, int, flt, lng, dbl, any=b[@3]->ttype)
	 *  @2: tloc, tvar, tail
	 *  @3: 0, 1
	 *  @5: tail value comparison,
		e.g.,	simple_LE(BUN@2(b[0],cur[0]), BUN@2(b[1],cur[1]), @1)
		or	atom_GT(BUN@2(b[0],cur[0]), BUN@2(b[1],cur[1]), @1)
	 */
	/* copy tails from BAT @3 to the results; 
	   for each BUN, remember in w, whether it came from BAT 0 or BAT 1 */
	while ((idx[@3] < cnt[@3]) && (@4)) {
		void@1_bunfastins_nocheck_noinc(bn[0],dst[0],0,BUN@2(b[@3],cur[@3]));
		idx[@3]++;
		cur[@3] += bs[@3];
		dst[0] += bns[0];
		w[h++] = (chr)@3;
	}
@= merged_union_1
	/*  @1: ATOMstorage(b[0]->ttype) (chr, sht, int, flt, lng, dbl, any=b[0]->ttype)
	 *  @2: tloc, tvar, tail (for BAT 0)
	 *  @3: tloc, tvar, tail (for BAT 1)
	 *  @4: simple, atom
	 */
	/* merge-union the first two BATs; regard and preserve tail-order */
	h = 0;
	concat = (BATcount(b[0])==0 || BATcount(b[1])==0);
	if (!concat) {
		concat = ( BATtordered(b[0])&BATtordered(b[1])&1 && \
		           @4_LE(BUN@2(b[0],BUNlast(b[0])-BUNsize(b[0])),BUN@3(b[1],cur[1]),@1) );
	}
	if (!concat) {
		while ((idx[0] < cnt[0]) && (idx[1] < cnt[1])) {
			@:merged_union_0(@1,@2,0,@4_LE(BUN@2(b[0],cur[0]),BUN@3(b[1],cur[1]),@1))@
			if (idx[0] < cnt[0]) {
				@:merged_union_0(@1,@3,1,@4_GT(BUN@2(b[0],cur[0]),BUN@3(b[1],cur[1]),@1))@
			}
		}
	}
	/* get remaining BUNs */
	@:merged_union_0(@1,@2,0,TRUE)@
	@:merged_union_0(@1,@3,1,TRUE)@
@= merged_union_2
	/*  @1: ATOMstorage(b[0]->ttype) (chr, sht, int, flt, lng, dbl, any=b[0]->ttype)
	 *  @2: tloc, tvar, tail (for BAT 0)
	 *  @3: simple, atom
	 */
	@:merged_union_1(@1,@2,@2,@3)@
	break;
@= merged_union_3
	/*  @1: ATOMstorage(b[@3]->ttype) (chr, sht, int, flt, lng, dbl, any=b[0]->ttype)
	 *  @2: tloc, tvar, tail
	 */
	/* merge-union each of the remaining BAT-pairs; 
	   w tell us, from which BAT we need to get the next BUN */
	for (h=0; h<sze; h++) {
		j = i + (int)w[h];
		void@1_bunfastins_nocheck_noinc(bn[k],dst[k],0,BUN@2(b[j],cur[j]));
		idx[j]++;
		cur[j] += bs[j];
		dst[k] += bns[k];
	}
@= merged_union_4
	/*  @1: ATOMstorage(b[@3]->ttype) (chr, sht, int, flt, lng, dbl, any=b[0]->ttype)
	 *  @2: tloc, tvar, tail
	 */
	@:merged_union_3(@1,@2)@
	break;
@c
	/* merge-union the first two BATs */
/* HACK(?): compare [v]oid (unsigned) as int/lng (signed) to get nil's first... */
#if SIZEOF_OID == SIZEOF_INT
	if (b[0]->ttype==TYPE_void && b[1]->ttype==TYPE_void) {
		@:merged_union_1(int,tvar,tvar,simple)@
	} else if (b[0]->ttype==TYPE_void && b[1]->ttype==TYPE_oid) {
		@:merged_union_1(int,tvar,tloc,simple)@
	} else if (b[0]->ttype==TYPE_oid && b[1]->ttype==TYPE_void) {
		@:merged_union_1(int,tloc,tvar,simple)@
	} else if (b[0]->ttype==TYPE_oid && b[1]->ttype==TYPE_oid) {
		@:merged_union_1(int,tloc,tloc,simple)@
#else
	if (b[0]->ttype==TYPE_void && b[1]->ttype==TYPE_void) {
		@:merged_union_1(lng,tvar,tvar,simple)@
	} else if (b[0]->ttype==TYPE_void && b[1]->ttype==TYPE_oid) {
		@:merged_union_1(lng,tvar,tloc,simple)@
	} else if (b[0]->ttype==TYPE_oid && b[1]->ttype==TYPE_void) {
		@:merged_union_1(lng,tloc,tvar,simple)@
	} else if (b[0]->ttype==TYPE_oid && b[1]->ttype==TYPE_oid) {
		@:merged_union_1(lng,tloc,tloc,simple)@
#endif
	} else {
		any = b[0]->ttype;
		switch(ATOMstorage(b[0]->ttype)) {
		case TYPE_chr:	@:merged_union_2(chr,tloc,simple)@
		case TYPE_sht:	@:merged_union_2(sht,tloc,simple)@
		case TYPE_int:	@:merged_union_2(int,tloc,simple)@
		case TYPE_flt:	@:merged_union_2(flt,tloc,simple)@
		case TYPE_lng:	@:merged_union_2(lng,tloc,simple)@
		case TYPE_dbl:	@:merged_union_2(dbl,tloc,simple)@
		default:
			if (b[0]->tvarsized) {
				@:merged_union_2(any,tvar,atom)@
			} else {
				@:merged_union_2(any,tloc,atom)@
			}
		}
	}
	/* merge-union each of the remaining BAT-pairs */
	for (k=1; k<npairs; k++) {
		i = (k<<1);
		j = i+1;
/* HACK(?): compare [v]oid (unsigned) at int/lng (signed) to get nil's first... */
#if SIZEOF_OID == SIZEOF_INT
		if (b[i]->ttype==TYPE_void || b[j]->ttype==TYPE_void) {
			@:merged_union_3(int,tail,simple)@
		} else if (b[i]->ttype==TYPE_oid && b[j]->ttype==TYPE_oid) {
			@:merged_union_3(int,tloc,simple)@
#else
		if (b[i]->ttype==TYPE_void || b[j]->ttype==TYPE_void) {
			@:merged_union_3(lng,tail,simple)@
		} else if (b[i]->ttype==TYPE_oid && b[j]->ttype==TYPE_oid) {
			@:merged_union_3(lng,tloc,simple)@
#endif
		} else {
			any = b[i]->ttype;
			switch(ATOMstorage(b[i]->ttype)) {
			case TYPE_chr:	@:merged_union_4(chr,tloc)@
			case TYPE_sht:	@:merged_union_4(sht,tloc)@
			case TYPE_int:	@:merged_union_4(int,tloc)@
			case TYPE_flt:	@:merged_union_4(flt,tloc)@
			case TYPE_lng:	@:merged_union_4(lng,tloc)@
			case TYPE_dbl:	@:merged_union_4(dbl,tloc)@
			default:
				if (b[i]->tvarsized) {
					@:merged_union_4(any,tvar)@
				} else {
					@:merged_union_4(any,tloc)@
				}
			}
		}
	}

	/* set BAT properties */

	for (k=0; k<npairs; k++) {
		BAT *b0 = b[2*k], *b1 = b[(2*k)+1];
		BATseqbase(bn[k], (oid)0);
		bn[k]->batBuns->free = dst[k] - bn[k]->batBuns->base;
		BATsetcount(bn[k], bn[k]->batBuns->free/BUNsize(bn[k]));
		if (!bn[k]->batDirty) bn[k]->batDirty = TRUE;
		BATkey(bn[k],TRUE);
		BATkey(BATmirror(bn[k]),FALSE);
		bn[k]->hsorted = GDK_SORTED;
		if (k==0 || (BATcount(b0)==0 && BATcount(b1)==0)) {
			bn[k]->tsorted = GDK_SORTED;
		} else
		  if (BATcount(b0)==0) {
			bn[k]->tsorted = (BATtordered(b1)&1 ? GDK_SORTED : FALSE);
		} else
		  if (BATcount(b1)==0) {
			bn[k]->tsorted = (BATtordered(b0)&1 ? GDK_SORTED : FALSE);
		} else {
			bn[k]->tsorted = ( ( concat && \
			                     BATtordered(b0)&BATtordered(b1)&1 && \
			                     atom_LE(BUNtail(b0,BUNlast(b0)-BUNsize(b0)),BUNtail(b1,BUNfirst(b1)),b0->ttype) ) \
			                  ? GDK_SORTED \
			                  : FALSE );
		}
		bn[k]->hdense = TRUE;
		bn[k]->tdense = FALSE;
	}

	/* insert bn[] BATs in BN BAT */

	DST = BUNlast(BN);
	BS = BUNsize(BN);
	BATseqbase(BN, (oid)0);
	for (k=0; k<npairs; k++) {
		voidany_bunfastins_nocheck_noinc(BN,DST,0,&bn[k]->batCacheid);
		BBPunfix(bn[k]->batCacheid);
		DST += BS;
	}
	BN->batBuns->free = DST - BN->batBuns->base;
	BATsetcount(BN, BN->batBuns->free/BS); 
	if (!BN->batDirty) BN->batDirty = TRUE;
	BATkey(BN,TRUE);
	BATkey(BATmirror(BN),TRUE);
	BN->hsorted = GDK_SORTED;
	BN->tsorted = FALSE;
	BN->hdense = TRUE;
	BN->tdense = FALSE;

	*res = BN;

	GDKfree(w);

	return GDK_SUCCEED;
bunins_failed:
	GDKerror("merged_union: bunins failed.\n");
cleanup:
	BBPreclaim(BN);
	for (k=0; k<npairs; k++) {
		BBPreclaim(bn[k]);
	}
	return GDK_FAIL;
}

int CMDmerged_union( BAT** res, ptr L, int ltpe, ptr R, int rtpe, ... )
{
	int i, tpe, nbats = 0, ret = GDK_SUCCEED;
	BAT *b[MAXPARAMS];
	va_list ap;
        ptr p;

	/* first convert any constant parameters to fake projects */
	if (CMDfakeProject(b+nbats, L, ltpe) == GDK_SUCCEED) {
		nbats++;
		if (CMDfakeProject(b+nbats, R, rtpe) == GDK_SUCCEED) {
			nbats++;
			va_start(ap,rtpe);
			while((p = va_arg(ap, ptr)) != NULL) {
        		        tpe = va_arg(ap, int);
				if (CMDfakeProject(b+nbats, p, tpe) == GDK_SUCCEED) {
					nbats++;
				} else {
					ret = GDK_FAIL;
					break;
				}
			}
			va_end(ap);
		}
	}
	if (ret == GDK_SUCCEED) {
		ret = merged_union(res, nbats, b);
	}
	/* unfix all bats; this destroys any created fake projects */
	for(i=0; i<nbats; i++) 
		BBPunfix(b[i]->batCacheid);
	return ret;
}

int
CMDll_strSplit(BAT **res, BAT *strs, BAT *seps)
{
        BAT *bn;
        BUN p_str, p_sep, last_str_row;
        int bs_str, bs_sep;
        size_t cnt, seplen;
        oid base;
        char *actual_str;
        bit triv_prop;

        /* check arguments */

        BATcheck(strs, "ll_strSplit");
        BATcheck(seps, "ll_strSplit");

        cnt = BATcount(strs);
        base = strs->hseqbase;
        ERRORcheck(!BAThdense(strs) || !BAThdense(seps), 
                "ll_strSplit: input BATs (strs, seps) must be void-headed with non-nil seqbase.\n");
        ERRORcheck(BATcount(seps)!=cnt || seps->hseqbase!=base,
                "ll_strSplit: input BATs (strs, seps) must be head-aligned.\n");

        /* create result BAT */

        bn = BATnew(TYPE_oid, TYPE_str, 2*cnt);
        if (bn == NULL) {
                GDKerror("ll_strSplit: BATnew(TYPE_oid, TYPE_str, %d) failed.\n", 2*cnt);
                return GDK_FAIL;
        }

        /* do the ll_strSplit */

        bs_str = BUNsize(strs);
        bs_sep = BUNsize(seps);

        p_str = BUNfirst(strs);
        p_sep = BUNfirst(seps);

        for (last_str_row = BUNlast(strs) ;
             p_str < last_str_row ;
             p_str += bs_str, p_sep += bs_sep) {

                str sep = (str)BUNtvar(seps, p_sep);
                seplen = strlen(sep);
                actual_str = GDKstrdup((str)BUNtvar(strs, p_str));

                if (!seplen)
                {
                        bunfastins(bn, &base, actual_str);
                        base++;
                        continue;
                }
  
                while (actual_str) {
                        char *e = strstr(actual_str, sep);
  
                        if (!e)
                                break;
  
                        *e = 0;
                        bunfastins(bn, &base, actual_str);
                        actual_str = e + seplen;
                }
 
                if (actual_str && *actual_str)
                        bunfastins(bn, &base, actual_str);
                base++;
        }

        /* set result properties */

        if (!bn->batDirty) bn->batDirty = TRUE;
        triv_prop = (BATcount(bn) < 2);
        BATkey(bn,triv_prop);
        BATkey(BATmirror(bn),triv_prop);
        bn->hsorted = GDK_SORTED;
        bn->tsorted = triv_prop;
        bn->hdense = triv_prop;
        bn->tdense = FALSE;
        if (BATcount(bn) == 0) {
                BATseqbase(bn, (oid)0);
        } else if (BATcount(bn) == 1) {
                BATseqbase(bn, *(oid*)BUNhloc(bn,BUNfirst(bn)));
        }
        *res = bn;
        return GDK_SUCCEED;
bunins_failed:
        GDKerror("ll_strSplit: bunins failed.\n");
        return GDK_FAIL;
}


int
CMDnormSpace(str *res, str string)
{
        char *ws, *cur, *pointer;
        size_t n;

        pointer = GDKstrdup(string);
        if (pointer == NULL) {
                GDKerror("normSpace: GDKmalloc(" SZFMT ") failed.\n",
                         strlen (string) + 1);
                return GDK_FAIL;
        }

        *res = pointer;

        cur = string;
        ws = " \f\n\r\t\v";
        n = strspn (cur, ws);
        if (n)
        {
            *pointer = ' ';
            pointer++;
            cur += n;
        }

        while ((n = strcspn (cur, ws))) {
            while (n > 0)
            {
                *pointer = *cur;
                pointer++;
                cur++;
                n--;
            }
            n = strspn (cur, ws);
            if (n)
            {
                *pointer = ' ';
                pointer++;
                cur += n;
            }
        }
        *pointer = '\0';

        return GDK_SUCCEED;
}

int
math_unary_up_ROUND(dbl *res, dbl *x)
{
        if (*x == dbl_nil) {
                *res = dbl_nil;
        } else {
                double integral;
                double tmp = modf(*x, &integral);

                tmp = floor(tmp + 0.5);
                tmp += integral;

                *res = tmp;
        }

        return (GDK_SUCCEED);
}




#define KIND_NODE (oid)0
#define KIND_TEXT (oid)1
#define KIND_STR  (oid)2

int CMDcombine_text_string( BAT** res, BAT *iter, BAT *kind, BAT *str_value, int *result_size )
{
	BAT *bn;
	BUN qi, pi, pk, ps;
	int bsi, bsk, bss;
	size_t cnt, len, strsize = 1024;
	oid base0, base, i0, k0;
	str actual_str = NULL;
	bit triv_prop;
	
	*res = NULL;

	/* check arguments */

	BATcheck(iter, "combine_text_string");
	BATcheck(kind, "combine_text_string");
	BATcheck(str_value, "combine_text_string");

	cnt = BATcount(iter);
	base = iter->hseqbase;
	ERRORcheck(!BAThdense(iter) || !BAThdense(kind) || !BAThdense(str_value), 
		"combine_text_string: all input BATs (iter, kind, str_value) must be void-headed with non-nil seqbase.\n");
	ERRORcheck(BATcount(kind)!=cnt || BATcount(str_value)!=cnt || kind->hseqbase!=base || str_value->hseqbase!=base,
		"combine_text_string: all input BATs (iter, kind, str_value) must be head-aligned.\n");
	ERRORcheck(!(BATtordered(iter)&1),
		"combine_text_string: input BAT iter must be sorted on tail.\n");

	/* create result BAT */

	bn = BATnew(TYPE_oid, TYPE_str, *result_size);
	if (bn == NULL) {
		GDKerror("combine_text_string: BATnew(TYPE_oid, TYPE_str, %d) failed.\n", *result_size);
		return GDK_FAIL;
	}

	/* do the combine_text_string */

	bsi = BUNsize(iter);
	bsk = BUNsize(kind);
	bss = BUNsize(str_value);

	pi = BUNfirst(iter);
	pk = BUNfirst(kind);
	ps = BUNfirst(str_value);

	/* allocate str buffer */
	actual_str = (str)GDKmalloc(strsize);
	if (actual_str == NULL) {
		GDKerror("combine_text_string: GDKmalloc(" SZFMT ") failed.\n", strsize);
		goto cleanup;
	}
	len = 0;
	actual_str[0] = '\0';
	k0 = oid_nil;
	i0 = *(oid*)BUNtail(iter,pi); /* FIXME: "tvar" (void) vs. "tloc" (oid) */
	base0 = oid_nil;

	for (qi = BUNlast(iter) ; pi < qi ; pi += bsi, pk += bsk, ps += bss) {
		oid i = *(oid*)BUNtail(iter,pi); /* FIXME: "tvar" (void) vs. "tloc" (oid) */
		oid k = *(oid*)BUNtail(kind,pk); /* FIXME: "tvar" (void) vs. "tloc" (oid) */
		str s = (str)BUNtvar(str_value,ps);
		size_t l = strlen(s) + 1;
		if ( i0 < i || k == KIND_NODE ) {
			/* new iter or new node */
			if (len > 0) {
				/* actual_str of (previous) iter is not empty => insert it into result */
				bunfastins(bn, &base0, actual_str);
				len = 0;
				actual_str[0] = '\0';
			}
			k0 = oid_nil;
		}
		if (len+l >= strsize) {
			/* extend str buffer */
			do {
				strsize *= 2;
			} while (len+l >= strsize);
			actual_str = GDKrealloc(actual_str, strsize);
			if (actual_str == NULL) {
				GDKerror("combine_text_string: GDKrealloc(" SZFMT ") failed.\n", strsize);
				goto cleanup;
			}
		}
		if (k0 == KIND_STR && k == KIND_STR) {
			/* insert ' '-separator between adjacent STRs */
			actual_str[len++] = ' ';
			actual_str[len] = '\0';
		}
        if (k != KIND_NODE)
        {
		    strcpy(actual_str + len, s); /* we know it fits */
		    len += l - 1;	/* compensate for +1 earlier on */
        }
		i0 = i;
		k0 = k;
		base0 = base++;
	}

	if (len > 0) {
		bunfastins(bn, &base0, actual_str);
	}

	/* set result properties */

	if (!bn->batDirty) bn->batDirty = TRUE;
	triv_prop = (BATcount(bn) < 2);
	BATkey(bn,TRUE);
	BATkey(BATmirror(bn),triv_prop);
	bn->hsorted = GDK_SORTED;
	bn->tsorted = triv_prop;
	bn->hdense = triv_prop;
	bn->tdense = FALSE;
	if (BATcount(bn) == 0) {
		BATseqbase(bn, (oid)0);
	} else if (BATcount(bn) == 1) {
		BATseqbase(bn, *(oid*)BUNhloc(bn,BUNfirst(bn)));
	}
	*res = bn;

	return GDK_SUCCEED;
bunins_failed:
	GDKerror("combine_text_string: bunins failed.\n");
cleanup:
	if (actual_str) GDKfree(actual_str);
	BBPreclaim(bn);
	return GDK_FAIL;
}

int CMDstring_join ( BAT** res, BAT *iter_str, BAT *separator  )
{
	BAT *bn;
	BUN bun_iter_str, bun_sep, last_iter_str, last_sep;
	int bs_iter_str, bs_sep;
	size_t len, sep_len, strsize = 1024;
	oid i, i0, sep_oid, sep0;
	str s, sep, actual_str = NULL;
	bit triv_prop, first_string;
	
	*res = NULL;

	/* check arguments */

	BATcheck(iter_str, "string_join");
	BATcheck(separator, "string_join");

	ERRORcheck(!(BAThordered(iter_str)&1),
		"string_join: input BAT iter_str must be sorted on head.\n");
	ERRORcheck(!(BAThordered(separator)&1),
		"string_join: input BAT separator must be sorted on head.\n");

	/* create result BAT */

	bn = BATnew(TYPE_oid, TYPE_str, BATcount(separator));
	if (bn == NULL) {
		GDKerror("string_join: BATnew(TYPE_oid, TYPE_str, %d) failed.\n", BATcount(separator));
		return GDK_FAIL;
	}

	/* do the string_join */

	bs_iter_str = BUNsize(iter_str);
	bs_sep = BUNsize(separator);

	bun_iter_str = BUNfirst(iter_str);
	last_iter_str = BUNlast(iter_str);
	bun_sep = BUNfirst(separator);
	last_sep = BUNlast(separator);

	/* handling the empty cases */

	if (bun_iter_str == last_iter_str && bun_sep == last_sep) {
		/* ... head */
		BATkey (bn, TRUE);
                bn->hsorted = GDK_SORTED;
		bn->hdense = TRUE;
		BATseqbase (bn, (oid)0); /* does not really matter */
		/* ... tail */
		BATkey (BATmirror(bn), TRUE);
                bn->tsorted = GDK_SORTED;
		bn->tdense = FALSE;
		*res = bn;
 	       	return GDK_SUCCEED;
	}
	else if (bun_sep == last_sep) {
		GDKerror("string_join: expected oid %i@0 in iter_str "
                         "is missing in separator (0 rows).",
                         *(oid*)BUNhead(iter_str,bun_iter_str)); /* FIXME: "hvar" (void) vs. "hloc" (oid) */
		goto cleanup;
	}
	else if (bun_iter_str == last_iter_str) {
		sep0 = oid_nil;
		sep_oid = *(oid*)BUNhead(separator,bun_sep); /* FIXME: "hvar" (void) vs. "hloc" (oid) */
		@:append_sep_and_set_props@
 	       	return GDK_SUCCEED;
	}

	/* allocate str buffer */
	actual_str = (str)GDKmalloc(strsize);
	if (actual_str == NULL) {
		GDKerror("string_join: GDKmalloc(" SZFMT ") failed.\n", strsize);
		goto cleanup;
	}
	len = 0;
	actual_str[0] = '\0';

	i0 = *(oid*)BUNhead(iter_str,bun_iter_str); /* FIXME: "hvar" (void) vs. "hloc" (oid) */
	first_string = 1;
	sep0 = oid_nil;

@= get_next_separator
	/* - goes to the next row in the separator bat
	     to get the separator for the next iter value
	   - produces an error if oids are not aligned */
	sep_oid = *(oid*)BUNhead(separator,bun_sep); /* FIXME: "hvar" (void) vs. "hloc" (oid) */

	while (i0 > sep_oid && bun_sep != last_sep) {
		if (sep0 == sep_oid) {
			GDKerror("string_join: the head of separator has to be keyed.");
			goto cleanup;
		}
		bunfastins(bn, &sep_oid, "");
		sep0 = sep_oid;
		bun_sep += bs_sep;
	        sep_oid = *(oid*)BUNhead(separator,bun_sep); /* FIXME: "hvar" (void) vs. "hloc" (oid) */
	}
	if (i0 == sep_oid && bun_sep != last_sep) {
		sep = (str)BUNtvar(separator,bun_sep);
		sep_len = strlen(sep);
	}
	else {
		GDKerror("string_join: expected oid %i@0 in iter_str "
			 "is missing in separator.",
			 i0);
		goto cleanup;
	}
@c
	@:get_next_separator@

	for (; bun_iter_str < last_iter_str ; bun_iter_str += bs_iter_str) {
		i = *(oid*)BUNhead(iter_str,bun_iter_str); /* FIXME: "hvar" (void) vs. "hloc" (oid) */
		s = (str)BUNtvar(iter_str,bun_iter_str);

		size_t l;

		if (i0 < i) {
			bunfastins(bn, &i0, actual_str);
			len = 0;
			actual_str[0] = '\0';
			i0 = i;
			first_string = 1;

			sep0 = sep_oid;
			bun_sep += bs_sep;
			@:get_next_separator@
		}

		l = strlen(s) + sep_len + 1;

		if (len+l >= strsize) {
			/* extend str buffer */
			do {
				strsize *= 2;
			} while (len+l >= strsize);
			actual_str = GDKrealloc(actual_str, strsize);
			if (actual_str == NULL) {
				GDKerror("string_join: GDKrealloc(" SZFMT ") failed.\n", strsize);
				goto cleanup;
			}
		}

		/* adds the separator (starting before the second string) */

		if (first_string) {
			first_string = 0;
		}
		else {
			strcpy(actual_str+len, sep); /* we know it fits */
			len += sep_len;
		}

		strcpy(actual_str + len, s); /* we know it fits */
		len += l - sep_len - 1;	/* just strlen(s) */
	}

	/* inserts last row */

	if (len > 0) {
		bunfastins(bn, &i0, actual_str);
		sep0 = sep_oid;
		bun_sep += bs_sep;
	}

@= append_sep_and_set_props
	while (bun_sep != last_sep) {
	        sep_oid = *(oid*)BUNhead(separator,bun_sep); /* FIXME: "hvar" (void) vs. "hloc" (oid) */
		if (sep0 == sep_oid) {
			GDKerror("string_join: the head of separator has to be keyed.");
			goto cleanup;
		}
		bunfastins(bn, &sep_oid, "");
		sep0 = sep_oid;
		bun_sep += bs_sep;
	}

	GDKfree(actual_str);

	/* set result properties */

	if (!bn->batDirty) bn->batDirty = TRUE;
	triv_prop = (BATcount(bn) < 2);
	BATkey(bn,TRUE);
	BATkey(BATmirror(bn),triv_prop);
	bn->hsorted = GDK_SORTED;
	bn->tsorted = triv_prop;
	bn->hdense = triv_prop;
	bn->tdense = FALSE;
	if (BATcount(bn) == 0) {
		BATseqbase(bn, (oid)0);
	} else if (BATcount(bn) == 1) {
		BATseqbase(bn, *(oid*)BUNhloc(bn,BUNfirst(bn)));
	}
	*res = bn;
@c
	@:append_sep_and_set_props@

	return GDK_SUCCEED;
bunins_failed:
	GDKerror("string_join: bunins failed.\n");
cleanup:
	if (actual_str) GDKfree(actual_str);
	BBPreclaim(bn);
	return GDK_FAIL;
}

int CMDenumerate ( BAT** res, BAT *startval, BAT *length )
{
        BAT *bn;
        BUN cur_bun, length_bun, bun_last;
        lng bs_startval, bs_length, number, counter, result_size;
        size_t cnt;
        oid base, head_oid;
        bit triv_prop;

        /* check arguments */

        BATcheck(startval, "enumerate");
        BATcheck(length, "enumerate");

        cnt = BATcount(startval);
        base = startval->hseqbase;
        ERRORcheck(!BAThdense(startval) || !BAThdense(length), 
                "enumerate: input BATs (startval, length) must be void-headed with non-nil seqbase.\n");
        ERRORcheck(BATcount(length)!=cnt || length->hseqbase!=base,
                "enumerate: input BATs (startval, length) must be head-aligned.\n");

        /* create result BAT */
        if (CMDsum_lng_lng(&result_size, length) == GDK_FAIL) {
                GDKerror("enumerate: summing up 'length' failed.\n");
                return GDK_FAIL;
        }

        bn = BATnew(TYPE_oid, TYPE_lng, result_size);
        if (bn == NULL) {
                GDKerror("enumerate: BATnew(TYPE_oid, TYPE_lng, %d) failed.\n", result_size);
                return GDK_FAIL;
        }

        /* do the enumerate */

        for (cur_bun = BUNfirst(startval), length_bun = BUNfirst(length),
             bun_last = BUNlast(startval),
             bs_startval = BUNsize(startval), bs_length = BUNsize(length);
             cur_bun < bun_last;
             cur_bun += bs_startval, length_bun += bs_length)
        {
	        head_oid = *(oid*)BUNhead(startval,cur_bun); /* FIXME: "hvar" (void) vs. "hloc" (oid) */
                number = *(lng*) BUNtloc (startval, cur_bun);
                counter = *(lng*) BUNtloc (length, length_bun);
                while (counter)
                {
		        bunfastins(bn, &head_oid, &number);
                        number++;
                        counter--;
                }
        }

	if (!bn->batDirty) bn->batDirty = TRUE;
	triv_prop = (BATcount(bn) < 2);
	BATkey(bn,triv_prop);
	BATkey(BATmirror(bn),triv_prop);
	bn->hsorted = GDK_SORTED;
	bn->tsorted = triv_prop;
	bn->hdense = triv_prop;
	bn->tdense = FALSE;
	if (BATcount(bn) == 0) {
		BATseqbase(bn, (oid)0);
	} else if (BATcount(bn) == 1) {
		BATseqbase(bn, *(oid*)BUNhloc(bn,BUNfirst(bn)));
	}
	*res = bn;

	return GDK_SUCCEED;
bunins_failed:
	GDKerror("enumerate: bunins failed.\n");
	BBPreclaim(bn);
	return GDK_FAIL;
}

int CMDmposjoin ( BAT** res, BAT* pre, BAT* cont, BAT* ws_item )
{
	BAT *bn, **batlist = NULL, *the_cont_bat = NULL;
	BUN q, pp, pf, pw, dst;
	int bs, bsp, bsf, ii = 0, tt = 0;
	size_t cnt, len;
	oid base, wl, wh;
	bit triv_prop;
	bit fake_cont = FALSE;
	bit fake_ws_item = FALSE;
	
	*res = NULL;

	/* check arguments */

	BATcheck(pre, "mposjoin");
	BATcheck(cont, "mposjoin");
	BATcheck(ws_item, "mposjoin");

	len = BATcount(ws_item);
	cnt = BATcount(pre);
	base = pre->hseqbase;
	fake_cont = is_fake_project(cont);
	
	if (fake_cont) {
		ERRORcheck(!BAThdense(pre), 
			"mposjoin: input BAT pre must have a dense head.\n");
		ERRORcheck(!BAThdense(ws_item), 
			"mposjoin: input BAT ws_item must have a dense head.\n");
	} else {
		ERRORcheck(!BAThdense(pre) || !BAThdense(cont) || !BAThdense(ws_item), 
			"mposjoin: all input BATs (pre, cont, ws_item) must have a dense head.\n");
		ERRORcheck(BATcount(cont)!=cnt || cont->hseqbase!=base,
			"mposjoin: first two input BATs (pre & cont) must be head-aligned.\n");
	}
	ERRORcheck(len==0,
		"mposjoin: third input BAT (ws_item) must not be empty.\n");

	the_cont_bat = BATdescriptor(*(bat*)BUNtloc(ws_item, BUNfirst(ws_item)));
	tt = the_cont_bat->ttype;
	BBPunfix(the_cont_bat->batCacheid);
	the_cont_bat = NULL;

	if (cnt==0) {
		@:mpos_res_empty@
	}

	ii = 0;
	
	if (fake_cont) {
		oid the_cont_id = *(oid*)BUNtail(cont, BUNfirst(cont));
		len = 1;
		batlist = &the_cont_bat;
		BUNfndVOID(pw, ws_item, &the_cont_id);
		if (pw==NULL) {
			@:mpos_res_empty@
		}

		@:mpos_init_batlist@
		if (!fake_ws_item) {
			*res = bn = BATleftfetchjoin(pre, batlist[0], oid_nil);
			@:mpos_free_batlist(0)@
			if (bn == NULL) {
				GDKerror("mposjoin: BATleftfetchjoin(pre, ws_item["SZFMT"]) failed.\n",
					 the_cont_id);
				return GDK_FAIL;
			}
			return GDK_SUCCEED;
		}
	
	} else {
		batlist = (BAT**)GDKmalloc(len * sizeof(BAT*));
		if (batlist == NULL) {
			GDKerror("mposjoin: GDKmalloc(" SZFMT ") failed.\n", len * sizeof(BAT*));
			return GDK_FAIL;
		}
		BATloopFast(ws_item, pw, q, bs) {
			@:mpos_init_batlist@
		}
	}

@= mpos_init_batlist
{
		bit fake_item = FALSE;
		bat bid = *(bat*)BUNtloc(ws_item, pw);
		batlist[ii] = BATdescriptor(bid);
		fake_item = is_fake_project(batlist[ii]);
		fake_ws_item |= fake_item;
		if (!(fake_item || BAThdense(batlist[ii]))) {
			GDKerror("mposjoin: all BATs in the tail of the third input BAT (ws_item) must have a dense head.\n");
			ii++;
			@:mpos_free_batlist(0)@
			return GDK_FAIL;
		}
		if (ATOMtype(batlist[0]->ttype) != ATOMtype(batlist[ii]->ttype)) {
			GDKerror("mposjoin: all BATs in the tail of the third input BAT (ws_item) must have the same tail type (%d: %d != %d).\n",
				ii, batlist[ii]->ttype, batlist[0]->ttype);
			ii++;
			@:mpos_free_batlist(0)@
			assert(0);
			return GDK_FAIL;
		}
		ii++;
}	
@
@= mpos_res_empty
	cnt = 0;
	@:mpos_res_create@
	@:mpos_res_prop@

	return GDK_SUCCEED;
@
@= mpos_res_create
	/* create result BAT */

	bn = BATnew(TYPE_void, ATOMtype(tt), cnt);
	if (bn == NULL) {
		GDKerror("mposjoin: BATnew(TYPE_void, %s, " SZFMT ") failed.\n", ATOMname(ATOMtype(tt)), cnt);
		@:mpos_free_batlist(0)@
		return GDK_FAIL;
	}
	BATseqbase(bn, base);
@c
	@:mpos_res_create@
	bs = BUNsize(bn);
	dst = BUNlast(bn);

	/* do the mposjoin */

	wl = ws_item->hseqbase;
	wh = wl + len - 1;

	bsp = BUNsize(pre);
	if (fake_cont) {
		bsf = 0;
	} else {
		bsf = BUNsize(cont);
	}

	pp = BUNfirst(pre);
	pf = BUNfirst(cont);

@= mposjoin
	/*  @1: ATOMstorage(ATOMtype(batlist[0]->ttype)) (chr, sht, int, flt, lng, dbl, any)
	 *  @2: tloc, tvar, tail
	 */
	 if (fake_cont) {
	 	/* !fake_ws_item handled above */
	 	@:mposjoin_fake_fake(@1,@2)@
	 } else {
		if (fake_ws_item) {
			@:mposjoin_(@1,@2,is_fake_project(b))@
		} else {
			@:mposjoin_(@1,@2,FALSE)@
		}
	}
	break;
@
@= mposjoin_fake_fake
	/*  @1: ATOMstorage(ATOMtype(batlist[0]->ttype)) (chr, sht, int, flt, lng, dbl, any)
	 *  @2: tloc, tvar, tail
	 */
	BAT *b = batlist[0];
	ptr  w = BUN@2(b, BUNfirst(b));
	for (q = BUNlast(pre) ; pp < q ; pp += bsp) {
		void@1_bunfastins_nocheck_noinc(bn,dst,0,w);
		dst += bs;
	}
@
@= mposjoin_
	/*  @1: ATOMstorage(ATOMtype(batlist[0]->ttype)) (chr, sht, int, flt, lng, dbl, any)
	 *  @2: tloc, tvar, tail
	 *  @3: FALSE / is_fake_project(b)
	 */
	for (q = BUNlast(pre) ; pp < q ; pp += bsp, pf += bsf) {
		oid p = *(oid*)BUNtail(pre,pp);  /* FIXME: "tvar" (void) vs. "tloc" (oid) */
		oid f = *(oid*)BUNtail(cont,pf); /* FIXME: "tvar" (void) vs. "tloc" (oid) */
		if (f >= wl && f <= wh) {
			BAT *b = batlist[f - wl];
			if (@3) {
				void@1_bunfastins_nocheck_noinc(bn,dst,0,BUN@2(b,BUNfirst(b)));
				dst += bs;
			} else {
				oid l = b->hseqbase;
				oid h = l + BATcount(b) - 1;
				if (p >= l && p <= h) {
					oid b0 = BUNindex(b,BUNfirst(b));
					void@1_bunfastins_nocheck_noinc(bn,dst,0,BUN@2(b,BUNptr(b,p-l+b0)));
					dst += bs;
				} 
			}
		}
	}
@c

	switch(ATOMstorage(ATOMtype(batlist[0]->ttype))) {
	case TYPE_chr:	@:mposjoin(chr,tloc,simple)@
	case TYPE_sht:	@:mposjoin(sht,tloc,simple)@
#if SIZEOF_OID == SIZEOF_INT
	/* cannot use tloc on oid(void) */
	case TYPE_int:	@:mposjoin(int,tail,simple)@
#else
	case TYPE_int:	@:mposjoin(int,tloc,simple)@
#endif
	case TYPE_flt:	@:mposjoin(flt,tloc,simple)@
#if SIZEOF_OID == SIZEOF_LNG
	/* cannot use tloc on oid(void) */
	case TYPE_lng:	@:mposjoin(lng,tail,simple)@
#else
	case TYPE_lng:	@:mposjoin(lng,tloc,simple)@
#endif
	case TYPE_dbl:	@:mposjoin(dbl,tloc,simple)@
	default:
		if (batlist[0]->tvarsized) {
			@:mposjoin(any,tvar,atom)@
		} else {
			@:mposjoin(any,tloc,atom)@
		}
	}
	bn->batBuns->free = dst - bn->batBuns->base;
	BATsetcount(bn, bn->batBuns->free/BUNsize(bn));

@= mpos_res_prop
    /* check result size */
    if (BATcount(pre) > BATcount(bn)) {
		GDKerror("mposjoin: missing matches. The result requires the "
                 "same number of tuples as the first two input arguments: "SZFMT" < "SZFMT".", BATcount(bn), BATcount(pre));
		@:mpos_free_batlist(bn)@
		return GDK_FAIL;
	}
    if (BATcount(pre) < BATcount(bn)) {
		GDKerror("mposjoin: more than one match per input tuple. "
                 "seqbase does not match anymore.");
		@:mpos_free_batlist(bn)@
		return GDK_FAIL;
	}

	/* set result properties */

	if (!bn->batDirty) bn->batDirty = TRUE;
	triv_prop = (BATcount(bn) < 2);
	BATkey(bn,TRUE);
	BATkey(BATmirror(bn),triv_prop);
	bn->hsorted = GDK_SORTED;
	bn->tsorted = triv_prop;
	bn->hdense = TRUE;
	bn->tdense = FALSE;

	*res = bn;
@c
	@:mpos_res_prop@
	@:mpos_free_batlist(0)@

	return GDK_SUCCEED;
bunins_failed:
	GDKerror("mposjoin: bunins failed.\n");
@= mpos_free_batlist
	if (@1) BBPreclaim(bn);
	while (ii > 0) {
		BBPunfix(batlist[--ii]->batCacheid);
	}
	if (batlist && !fake_cont) {
		GDKfree(batlist);
	}
@c
	@:mpos_free_batlist(bn)@
	return GDK_FAIL;
}


int CMDmvaljoin ( BAT** res, BAT* pre, BAT* cont, BAT* ws_item )
{
	BAT *bn, **batlist = NULL, *the_cont_bat = NULL;
	BUN q, pp, pf, pw;
	int bsp, bsf, bsw, ii = 0;
	size_t cnt, len, sze;
	oid base, wl, wh;
	bit triv_prop, all_hash, all_sort, some_hash, some_sort, all_key, fake_cont = FALSE;
	
	*res = NULL;

	/* check arguments */

	BATcheck(pre, "mvaljoin");
	BATcheck(cont, "mvaljoin");
	BATcheck(ws_item, "mvaljoin");

	len = BATcount(ws_item);
	cnt = BATcount(pre);
	base = pre->hseqbase;
	fake_cont = is_fake_project(cont);
	
	if (fake_cont) {
		ERRORcheck(!BAThdense(pre) || !BAThdense(ws_item), 
			"mvaljoin: input BATs pre & ws_item must have a dense head.\n");
	} else {
		ERRORcheck(!BAThdense(pre) || !BAThdense(cont) || !BAThdense(ws_item), 
			"mvaljoin: all input BATs (pre, cont, ws_item) must have a dense head.\n");
		ERRORcheck(BATcount(cont)!=cnt || cont->hseqbase!=base,
			"mvaljoin: first two input BATs (pre & cont) must be head-aligned.\n");
	}
	ERRORcheck(len==0,
		"mvaljoin: third input BAT (ws_item) must not be empty.\n");

	sze = cnt;
	ii = 0;
	all_hash = all_sort = all_key = TRUE;
	some_hash = some_sort = FALSE;
	
	if (sze == 0) {
		@:mval_res_empty@
	}
	if (fake_cont) {
		oid the_cont_id = *(oid*)BUNtail(cont, BUNfirst(cont));
		len = 1;
		batlist = &the_cont_bat;
		BUNfndVOID(pw, ws_item, &the_cont_id);
		if (pw==NULL) {
			@:mval_res_empty@
		}

		@:mval_init_batlist@
		*res = bn = BATleftjoin(pre, BATmirror(batlist[0]), oid_nil);
		@:mval_free_batlist@
		if (bn == NULL) {
			GDKerror("mvaljoin: BATleftjoin(pre, BATmirror(ws_item["SZFMT"])) failed.\n",
				 the_cont_id);
			return GDK_FAIL;
		}
		return GDK_SUCCEED;

	} else {
		batlist = (BAT**)GDKmalloc(len * sizeof(BAT*));
		if (batlist == NULL) {
			GDKerror("mvaljoin: GDKmalloc(" SZFMT ") failed.\n", len * sizeof(BAT*));
			return GDK_FAIL;
		}
		BATloopFast(ws_item, pw, q, bsw) {
			@:mval_init_batlist@
		}
	}
	if (all_key) {
		sze = cnt;
	}

@= mval_init_batlist
{
	bit hsh=0, srt, view;
	bat bid = *(bat*)BUNtloc(ws_item, pw);
	batlist[ii] = BATdescriptor(bid);
 	view = (batlist[ii]->htype == TYPE_void && batlist[ii]->hseqbase != oid_nil);
	srt = (BATtordered(batlist[ii])&1);
	if (batlist[ii]->ttype == TYPE_void && batlist[ii]->tseqbase != oid_nil) {
		view = 1; /* do not create the hash table on a void column */
	} else {
		hsh = (BAThash(BATmirror(batlist[ii]), 0) != NULL);
		if (batlist[ii]->ttype != TYPE_oid) {
			GDKerror("mvaljoin: all BATs in the tail of the third input BAT (ws_item) must have tail type OID.\n");
			ii++;
			@:mval_free_batlist@
			return GDK_FAIL;
		}
	}
	if (view && !VIEWparent(batlist[ii])) { 
                /* avoid BUNhead()/BUNtail() corruption due to multi-threading */
		batlist[ii] = VIEWcreate(batlist[ii]);  /* view shares master hash table */
		BBPunfix(VIEWparent(batlist[ii])); /* release the fix of BATdescriptor */
	}
	all_hash &= hsh;
	all_sort &= srt;
	some_hash |= hsh;
	some_sort |= srt;
	all_key &= (batlist[ii]->tkey!=0);
	sze = MAX(sze, BATcount(batlist[ii]));
	ii++;
}	
@
@= mval_res_empty
	sze = 0;
	@:mval_res_create@
	@:mval_res_prop@

	return GDK_SUCCEED;
@
@= mval_res_create
	/* create result BAT */

	bn = BATnew(TYPE_oid, TYPE_oid, sze);
	if (bn == NULL) {
		GDKerror("mvaljoin: BATnew(TYPE_oid, TYPE_oid, " SZFMT ") failed.\n", sze);
		@:mval_free_batlist@
		return GDK_FAIL;
	}
@c
	@:mval_res_create@

	/* do the mvaljoin */

	wl = ws_item->hseqbase;
	wh = wl + len - 1;

	bsp = BUNsize(pre);
	bsf = BUNsize(cont);

	pp = BUNfirst(pre);
	pf = BUNfirst(cont);

@= hash_only
	hash_t h;
	BUN r;
	b = BATmirror(b);
	HASHloop_oid(b, b->hhash, h, (ptr)(&p), r) {
		oidoid_bunfastins(bn,&base,BUNtail(b,r)); /* FIXME: "tvar" (void) vs. "tloc" (oid) */
	}
@= sort_only
	BUN rp, rq;
	int bs;
	SORTloop_oid(b, rp, rq, (ptr)(&p), (ptr)(&p), bs) {
		oidoid_bunfastins(bn,&base,BUNhead(b,rp)); /* FIXME: "hvar" (void) vs. "hloc" (oid) */
	}
@= scan_only
	BUN rp, rq;
	int bs;
	BATloopFast(b, rp, rq, bs) {
		if (p == *(oid*)BUNtail(b,rp)) { /* FIXME: "tvar" (void) vs. "tloc" (oid) */
			oidoid_bunfastins(bn,&base,BUNhead(b,rp)); /* FIXME: "hvar" (void) vs. "hloc" (oid) */
		}
	}
@= hash_scan
	if (b->thash) {
		@:hash_only@
	} else {
		@:scan_only@
	}
@= sort_scan
	if (BATtordered(b)&1) {
		@:sort_only@
	} else {
		@:scan_only@
	}
@= hash_sort_scan
	if (b->thash) {
		@:hash_only@
	} else if (BATtordered(b)&1) {
		@:sort_only@
	} else {
		@:scan_only@
	}
@= mvaljoin
	for (q = BUNlast(pre) ; pp < q ; pp += bsp, pf += bsf) {
		oid p = *(oid*)BUNtail(pre,pp); /* FIXME: "tvar" (void) vs. "tloc" (oid) */
		oid f = *(oid*)BUNtail(cont,pf); /* FIXME: "tvar" (void) vs. "tloc" (oid) */
		if (f >= wl && f <= wh) {
			BAT *b = batlist[f - wl];
			@1
		}
		base++;
	}
@c

	if (all_hash) {
		@:mvaljoin(@:hash_only@)@
	} else if (all_sort) {
		@:mvaljoin(@:sort_only@)@
	} else if (some_hash && some_sort) {
		@:mvaljoin(@:hash_sort_scan@)@
	} else if (some_hash) {
		@:mvaljoin(@:hash_scan@)@
	} else if (some_sort) {
		@:mvaljoin(@:sort_scan@)@
	} else {
		@:mvaljoin(@:scan_only@)@
	}

@= mval_res_prop
	/* set result properties */

	if (!bn->batDirty) bn->batDirty = TRUE;
	triv_prop = (BATcount(bn) < 2);
	BATkey(bn,triv_prop||all_key);
	BATkey(BATmirror(bn),triv_prop);
	bn->hsorted = GDK_SORTED;
	bn->tsorted = triv_prop;
	bn->hdense = triv_prop;
	bn->tdense = triv_prop;
	if (BATcount(bn) == 0) {
		BATseqbase(bn, (oid)0);
		BATseqbase(BATmirror(bn), (oid)0);
	} else if (BATcount(bn) == 1) {
		BATseqbase(bn, *(oid*)BUNhloc(bn,BUNfirst(bn)));
		BATseqbase(BATmirror(bn), *(oid*)BUNtloc(bn,BUNfirst(bn)));
	}

	*res = bn;
@c
	@:mval_res_prop@
	@:mval_free_batlist@

	return GDK_SUCCEED;
bunins_failed:
	GDKerror("mvaljoin: bunins failed.\n");
	BBPreclaim(bn);
@= mval_free_batlist
	while (ii > 0) {
		BBPunfix(batlist[--ii]->batCacheid);
	}
	if (batlist && !fake_cont) {
		GDKfree(batlist);
	}
@c
	@:mval_free_batlist@
	return GDK_FAIL;
}



/*
 * Worker for the ebv() function.
 */
BAT *
BATebv (BAT *b)
{
    BAT *ret = NULL;    /* return value */
    BUN p = 0, q = 0;   /* BUN variables for iteration */
    int bunsz;          /* BUN size used for iteration (BATloopFast) */
    oid old;            /* Last head value we had seen */
    bit val;            /* Boolean result value that belongs to `old' */
    size_t cnt = 0;     /* "guess" of result cardinality */
    bit trivial;        /* indicator for "trivial" result properties */

    /* Just in case BATebv might be called from elsewhere than CMDebv:
     * check, that b in not NULL.
     */
    BATcheck(b, "BATebv");
    /* Input must be sorted by head value, tail must be boolean */
    ERRORcheck (!(BAThordered(b)&1), "BATebv: head of BAT must be sorted.\n");
    /* Needed only, if you want to disallow TYPE_void, 
     * which also matches the oid requirement of the signature;
     * or if BATebv could be called from somewhere/one else than CMDebv.
     */
    ERRORcheck (!(b->htype == TYPE_oid),
                "BATebv: head of BAT must have oid type.\n");
@(
    /* Not needed, if BATebv is only called from CMDebv,
     * since the signature only allows bit-tailed BATs.
     */
    ERRORcheck (!(b->ttype == TYPE_bit),
                "BATebv: tail of BAT must have bit type.\n");
@)

    /* Try to "guess" the result cardinality:
     * on the one hand, we don't want too allocate (far) too much memory;
     * on the other hand, we want to avoid BATextends (i.e., (large) memcpy's),
     * that occur if we initially allow too little space ... 
     * Obviously, BATcount(b) is the upper limit;
     * lower limit is just a "wild guess"...
     */
    if (BAThkey (b))
        cnt = BATcount (b);
    else
        cnt = MIN (200, BATcount (b));

    /* Create return BAT */
    ret = BATnew (b->htype, TYPE_bit, cnt);
    if (!ret)
        return ret;

    /*
     * Iterate over the input BAT.
     * Whenever we see a head value the first time, we record its tail
     * value in val. If we see the same head value a second (third,...)
     * time, we re-set val to false. When we reach the next head
     * value, (old, val) will be the correct BUN for the last group.
     */

    /* Initialize, ... */
    bunsz = BUNsize (b);
    p = BUNfirst (b);
    old = *(oid *) BUNhloc (b, p);
    val = *(bit *) BUNtloc (b, p);
    /* ... skip first, ... */
    p += bunsz;
    /* ... and process the rest. */
    for(q = BUNlast(b); p < q; p += bunsz) {
    	oid *head = (oid *) BUNhloc (b, p);
        if (*head == old)
            val = FALSE;
        else {
            bunfastins (ret, &old, &val);
            old = *head;
            val = *(bit *) BUNtloc (b, p);
        }
    }
    /* Don't forget to produce the last BUN. */
    bunfastins (ret, &old, &val);
    
    /* Set result properties ... */
    cnt = BATcount (ret);
    trivial = (cnt < 2) ? TRUE : FALSE;
    /* ... head ... */
    BATkey (ret, TRUE);
    ret->hsorted = GDK_SORTED;
    ret->hdense = trivial;
    if (trivial == TRUE) {
        if (cnt == 0)
            BATseqbase (ret, (oid)0); /* does not really matter */
        else /* (cnt == 1) */
            BATseqbase (ret, old);
    }
    /* ... tail */
    BATkey (BATmirror(ret), trivial);
    ret->tsorted = trivial;
    ret->tdense = FALSE;

    return ret;

/* required by bunfastins macro */
bunins_failed:
    BBPreclaim(ret);
    return NULL;
}

/*
 * Implementation of ebv(). Basically just calls BATebv().
 */
int
CMDebv (BAT **result, BAT *b)
{
    return (*result = BATebv (b)) ? GDK_SUCCEED : GDK_FAIL;
}

int
CMDinvalid_qname(str *ret, BAT *b) 
{
        str s = ATOMnilptr(TYPE_str);
	BUN p, q;
	int xx;

	BATloopFast(b,p,q,xx) {
            char *r = BUNtail(b,p);
	    if ((*r >= 'a' && *r <= 'z') || (*r >= 'A' && *r <= 'Z') || *(unsigned char*) r >= 128) {
                r++;
                while((*r >= 'a' && *r <= 'z') || (*r >= 'A' && *r <= 'Z') || (*(unsigned char*) r >= 128) ||
                      (*r == '_') || (*r == '.') || (*r == '-') || (*r >= '0' && *r <= '9')) r++;
                if (*r == 0) continue; /* ok */
            }
	    s = (str) BUNtail(b,p); break;
	}
	*ret = GDKstrdup(s);
        return GDK_SUCCEED;

}

int
CMDlastmod_time(timestamp *ret, str filename) {
	struct stat st;
	*(lng*) ret = lng_nil;
	if (stat(filename, &st) == 0) {
		lng msecs = 1000*st.st_mtime;
		int year = 1970, one = 1, zero = 0;
		timestamp ts;
        	daytime dt;
		date d;
		tzone tz;
        	return date_create(&d, &year, &one, &one) &&
             	       daytime_create(&dt, &zero, &zero, &zero, &zero) &&
	               tzone_create(&tz, &zero) &&
                       timestamp_create(&ts, &d, &dt, &tz) &&
                       timestamp_add(ret, &ts, &msecs);
        }
	return GDK_SUCCEED;
}


typedef struct stack_item si;

struct stack_item {
	int *sze;	/* size to be updated */
	oid pre;	/* pre-order rank of subtree root (without holes) */
	oid limit;	/* pre-order rank of last node in subtree (with holes) */
};

#define PUSH(stack,s,p,l) \
	stack_top++;\
	if (stack_top >= stack_size) {\
		si* newstack = (si*) GDKrealloc(stack, (stack_size*=2)*sizeof(si));\
		if (newstack == NULL) {\
			GDKerror("correct_sizes: could not re-allocate stack of size %d.\n", stack_size*sizeof(si));\
			GDKfree(stack);\
	 		return GDK_FAIL;\
		}\
                stack = newstack;\
	}\
	stack[stack_top].sze = s;\
	stack[stack_top].pre = p;\
	stack[stack_top].limit = l;

#define POP(stack,s) \
	si s = stack[stack_top];\
	stack_top--;

/* StM:
 * Open: Does the following still work properly even if we're dealing with
 * multiple fragments in function map2NODE_interface() in
 * compiler/mil/milprint_summer.c ?
 */
int
CMDcorrect_sizes(BAT **ret, BAT *bat_iter, BAT *bat_item, BAT *bat_size) {
	BUN cur_iter, cur_item, cur_size;
	BUN fst_size, lst_size;
	int bs_iter, bs_item, bs_size;
	size_t cnt;
	oid base;
	oid prev_iter, prev_item, pre;
	bit fake_iter;
	si *stack = NULL;
	int stack_top = -1, stack_size = 128;
	
	/* check arguments */

	BATcheck(bat_iter, "correct_sizes");
	BATcheck(bat_item, "correct_sizes");
	BATcheck(bat_size, "correct_sizes");

	cnt = BATcount(bat_size);
	base = bat_size->hseqbase;
	fake_iter = is_fake_project(bat_iter);
	
	ERRORcheck(!((fake_iter || BAThdense(bat_iter)) && BAThdense(bat_item) && BAThdense(bat_size)), 
		"correct_sizes: all input BATs (iter, item, size) must have a dense head.\n");
	ERRORcheck(!((fake_iter || BATcount(bat_iter)==cnt) && BATcount(bat_item)==cnt),
		"correct_sizes: all input BATs (iter, item, size) must have the same size.\n");
	ERRORcheck(!((fake_iter || bat_iter->hseqbase==base) && bat_item->hseqbase==base),
		"correct_sizes: all input BATs (iter, item, size) must have the same seqbase.\n");

	stack = (si*)GDKmalloc(stack_size*sizeof(si));
	if (stack == NULL) {
		GDKerror("correct_sizes: could not allocate stack of size %d.\n", stack_size*sizeof(si));
	 	return GDK_FAIL;
	}
	stack_top = -1;

	bs_iter = (fake_iter?0:BUNsize(bat_iter));
	bs_item = BUNsize(bat_item);
	bs_size = BUNsize(bat_size);
	cur_iter = BUNfirst(bat_iter);
	cur_item = BUNfirst(bat_item);
	fst_size = BUNfirst(bat_size);
	lst_size = BUNlast(bat_size);

	prev_iter = oid_nil;
	prev_item = oid_nil;
	pre = oid_nil;
	for (cur_size = fst_size ; cur_size < lst_size ; cur_size += bs_size, cur_item += bs_item, cur_iter += bs_iter) {
		oid iter = *(oid*)BUNtail(bat_iter, cur_iter);
		oid item = *(oid*)BUNtail(bat_item, cur_item);
		int *sze =  (int*)BUNtail(bat_size, cur_size);
		int size = *sze;
		    pre  = *(oid*)BUNhead(bat_size, cur_size);
		while (stack_top >= 0 && ( iter != prev_iter || item > stack[stack_top].limit || item < prev_item )) {
			POP(stack, s);
			if (s.limit > prev_item) {
				*s.sze = pre - s.pre;
			} else {
				*s.sze = pre - s.pre - 1;
			}
		}
		if (size > 0) {
			PUSH(stack, sze, pre, item+size);
		}
		prev_iter = iter;
		prev_item = item;
	}
	while (stack_top >= 0) {
		POP(stack, s);
		if (s.limit > prev_item) {
			*s.sze = pre - s.pre + 1;
		} else {
			*s.sze = pre - s.pre;
		}
	}
	GDKfree(stack);

	if (!bat_size->batDirty) bat_size->batDirty = TRUE;

	BBPfix(bat_size->batCacheid);
	*ret = bat_size;
	return GDK_SUCCEED;
}


int
CMDSplitBat(BAT **res, BAT *b, str sep)
{
	BAT **bn;
	size_t nbats = 0;
	size_t maxbats = 16;
	BUN p, q, r;
	str s, e;
	size_t seplen = strlen(sep);
	char *buf = GDKmalloc(BUFSIZ);
	size_t buflen = BUFSIZ;
	size_t l;
	size_t i;
	size_t bs = BATcount(b);
	oid base, o;

	if (buf == NULL)
		return GDK_FAIL;

	ERRORcheck(seplen == 0, "splitbat: separator must not be empty.\n");

	o = base = b->hseqbase;
	bn = GDKmalloc(maxbats * sizeof(BAT *));
	if (bn == NULL)
		goto bunins_failed;
	memset(bn, 0, maxbats * sizeof(BAT *));
	BATloop(b, p, q) {
		s = (str) BUNtail(b, p);
		l = strlen(s);
		if (l >= buflen) {
			while (l >= (buflen += BUFSIZ))
				;
			buf = GDKrealloc(buf, buflen);
			if (buf == NULL)
				goto bunins_failed;
		}
		strcpy(buf, s);
		s = buf;
		i = 0;
		while (s) {
			if ((e = strstr(s, sep)) != NULL)
				*e = 0;
			if (i >= nbats) {
				size_t j;

				if (nbats >= maxbats) {
					bn = GDKrealloc(bn, maxbats + 16);
					if (bn == NULL)
						goto bunins_failed;
					memset(&bn[maxbats], 0, 16 * sizeof(BAT *));
					maxbats += 16;
				}
				bn[i] = BATnew(TYPE_void, TYPE_str, bs);
				if (bn[i] == NULL)
					goto bunins_failed;
				bn[i] = BATseqbase(bn[i], base);
				BATkey(BATmirror(bn[i]), FALSE);
				bn[i]->tsorted = 0;
				for (j = 0; j < bs; j++)
					bunfastins(bn[i], NULL, str_nil);
				nbats++;
			}
			BUNfndVOID(r, bn[i], &o);
			ATOMreplace(TYPE_str, bn[i]->theap, BUNtloc(bn[i], r), s);
			i++;
			s = e ? e + seplen : NULL;
		}
		o++;
	}
	GDKfree(buf);
	*res = BATnew(TYPE_void, TYPE_bat, nbats);
	if (*res == NULL)
		goto bunins_failed;
	*res = BATseqbase(*res, 0);
	BATkey(BATmirror(*res), FALSE);
	(*res)->tsorted = 0;
	for (i = 0; i < nbats; i++) {
		bunfastins(*res, NULL, &bn[i]->batCacheid);
		BBPunfix(bn[i]->batCacheid);
	}
	GDKfree(bn);
	return GDK_SUCCEED;

  bunins_failed:
	if (bn) {
		for (i = 0; i < nbats; i++)
			if (bn[i])
				BBPreclaim(bn[i]);
		GDKfree(bn);
	}
	if (buf)
		GDKfree(buf);
	return GDK_FAIL;
}

#define MAXKIND 5
int
CMDSplitKind(BAT **res, BAT *b)
{
	size_t i, n = BATcount(b);
	BAT *bn[MAXKIND];
	oid *cur[MAXKIND];

	for (i = 0; i < MAXKIND; i++)
		bn[i] = NULL;

	for (i = 0; i < MAXKIND; i++) {
		bn[i] = BATnew(TYPE_oid, TYPE_void, n);
		if (bn[i] == NULL) goto bunins_failed;

		/* inherit sortedness and keyness */
		bn[i]->tsorted = 0;
		bn[i]->hsorted = b->hsorted;
		BATkey(bn[i], b->hkey & 1);
		cur[i] = (oid*) BUNfirst(bn[i]);
	}
	if (BAThdense(b) && BUNsize(b) == sizeof(chr)) {
		/* fast split-select with direct oid-inserts and predication */
		chr *v = ((chr*) BUNfirst(b)) - b->hseqbase;	
		for(i=b->hseqbase, n+=b->hseqbase; i<n; i++) {
			int val = v[i];
			if ((val >= 0) & (val < MAXKIND))
				*cur[val]++ = i;
		}
	} else {
		BUN p, q;
		BATloopFast(b, p, q, i) {
			int val = *(chr*) BUNtloc(b, p);
			if ((val >= 0) & (val < MAXKIND))
				*cur[val]++ = *(oid*) BUNhead(b,p);
		}
	}
	for(i=0; i<MAXKIND; i++) {
		bn[i]->batBuns->free = ((BUN) cur[i]) - bn[i]->batBuns->base;
		BATsetcount(bn[i], bn[i]->batBuns->free/sizeof(oid));
	}
	*res = BATnew(TYPE_void, TYPE_bat, MAXKIND);
	if (*res == NULL)
		goto bunins_failed;
	*res = BATseqbase(*res, 0);
	BATkey(BATmirror(*res), FALSE);
	(*res)->tsorted = 0;
	for (i = 0; i < MAXKIND; i++) {
		bunfastins(*res, NULL, &bn[i]->batCacheid);
		BBPunfix(bn[i]->batCacheid);
	}
	return GDK_SUCCEED;
bunins_failed:
	for (i = 0; i < MAXKIND; i++)
		if (bn[i]) BBPreclaim(bn[i]);
	return GDK_FAIL;
}

@= nilarithC
int
CMDnil@1(int *res, int *v1, int *v2)
{
	*res = *v1 @2 *v2;
	return GDK_SUCCEED;
}
@c
@:nilarithC(and,&)@
@:nilarithC(or,|)@
@:nilarithC(plus,+)@


@+ Global Transaction Locking

While concurrency control in MonetDB/XQuery is optimistic and fine-grained (ehh, per page
conflict detection) there is some coarse grained control as well. It serves to avoid
thrashing on conflicting update loads: the database is automatically brought into serial 
update mode. Also, querying the meta-tables leads to full blocking of document management
(shred doc/delete doc) queries.

We have three hooks inserted in the MIL transaction mgmt code:
- pflock_begin() (called from ws_create) => QUERY STARTS
- pflock_end()   (called from ws_destroy) => QUERY ENDS
- pflock_meta()  (called during execution) => QUERY READS THE META TABLES

The below mechanism ensures that:
- when updating queries abort due to concurrency conflicts, they are restarted 
  and run in exclusive mode
- after an abort we run not only that query, but a convoy of PF_CONVOY next updating 
  queries in exclusive mode. This to gracefully degrade (avoid thrashing) in updating 
  loads that are highly conflictive.

Exclusive updating means that we need a counted-lock/exclusive-barrier (PF_UPDATE_LOCK,PF_UPDATE_BARRIER)
In normal mode, updating queries use the 'counting' path, that is, the first entering update
acquires the exclusive barrier, and the last leaving releases it. In exclusive mode, all updating 
queries take the exclusive barrier.

Similarly, there is a protection against seeing an inconsistent meta-database state.
Just before a query uses the documents()/collections() functions, all new incoming
document management queries (that shred and delete XML documents) are blocked,
and we must wait until all such active queries have finished.

This is again implemented using a counted-lock/exclusive-barrier (PF_META_LOCK,PF_META_BARRIER).
The first docmgmt query acquires the exclusive-barrier, and the last finishing releases it.
A query that calls pflock_meta() also must get this barrier, if it is the first to call it.
When those queries do pflock_end() the last must free the lock, and thus we must know which queries 
have taken it (this is also needed for not asking it twice). 

We store the information in the BAT 'ws_overlaps_ws' using [wsid,nil] BUNs.

A special case is when a document management query itself issues such a meta-information function.
Then we enter in exclusive single-docmgt query execution mode (SPECIAL CASE). Such a query is never
included in ws_overlaps_ws. Instead, a single variable 'pf_special' records the wsid.
@c
#define PF_CONVOY 5
BAT* ws_overlaps_ws = NULL;
MT_Lock pf_runtime_lock[6];
MT_Sema pf_runtime_sema[3];
int pf_nreaders, pf_convoy, pf_ndocmgt;
lng pf_writer, pf_special;

#define PF_SHORT_LOCK        pf_runtime_lock[0]
#define PF_WAL_LOCK          pf_runtime_lock[1]
#define PF_FREE_LOCK         pf_runtime_lock[2]
#define PF_EXTEND_LOCK       pf_runtime_lock[3]
#define PF_META_LOCK         pf_runtime_lock[4]
#define PF_UPDATE_LOCK       pf_runtime_lock[5]
#define PF_META_BARRIER      pf_runtime_sema[0]
#define PF_UPDATE_BARRIER    pf_runtime_sema[1]
#define PF_EXTEND_BARRIER    pf_runtime_sema[2]

#define PF_MODE_RDONLY 0 /* read-only query */
#define PF_MODE_DOCMGT 1 /* document management query */ 
#define PF_MODE_UPDATE 2 /* updating query */
#define PF_MODE_RETRY  3 /* updating query run for the second time (exclusive mode) */

/* get handle to the shared locks */
int CMDpflock_get(ptr *ret, int* nr) {
    *ret = (ptr) &pf_runtime_lock[(*nr)&3];
    if (*nr > 3) *ret = (ptr) &PF_EXTEND_BARRIER; /* hack: do not feel like creating a CMDsema_get */
    return GDK_SUCCEED;
}

/* query execution starts */
int CMDpflock_begin(lng *wsid) 
{
    int mode = ((*wsid) & (3LL << 30)) >> 30;
    if (mode == PF_MODE_RDONLY) 
        return GDK_SUCCEED;

    if (mode == PF_MODE_DOCMGT) {
        /* multiple docmgts may be active in parallel. the first takes the meta-barrier */
        int take_barrier;
        MT_set_lock(PF_META_LOCK, "pflock_begin: metaLock for mgmt");
        MT_set_lock(PF_SHORT_LOCK, "pflock_begin: shortLock for mgmt");
        take_barrier = (pf_ndocmgt++ == 0);
        MT_unset_lock(PF_SHORT_LOCK, "pflock_begin: shirtLock for mgmt");
        if (take_barrier) {
            MT_down_sema(PF_META_BARRIER, "pflock_begin: metaBarrier");
        }
        MT_unset_lock(PF_META_LOCK, "pflock_begin: metaLock for mgmt");
        return GDK_SUCCEED;
    }

    /* we only have update queries below this point; first check if we are in exclusive retry mode */
    MT_set_lock(PF_SHORT_LOCK, "pflock_begin: shortLock for convoy"); /* protects pf_convoy */
    if (mode == PF_MODE_RETRY) {
        pf_convoy += PF_CONVOY; /* re-try: schedule a convoy */
    } else if (pf_convoy > 0) {
        mode = PF_MODE_RETRY; pf_convoy--; /* convert to exclusive execution due to convoy */
    } 
    MT_unset_lock(PF_SHORT_LOCK, "pflock_begin: shortLock for convoy");

    if (mode == PF_MODE_UPDATE) {
	/* update in concurrent mode */
        MT_set_lock(PF_UPDATE_LOCK, "pflock_begin: updateLock for update");
        if (pf_nreaders++ == 0) 
            MT_down_sema(PF_UPDATE_BARRIER, "pflock_begin: updateBarrier for update");
        MT_unset_lock(PF_UPDATE_LOCK, "pflock_begin: updateLock for update");
    } else {
	/* update in exclusive mode */
        MT_down_sema(PF_UPDATE_BARRIER, "pflock_begin: updateBarrier for retry");
        MT_set_lock(PF_SHORT_LOCK, "pflock_begin: shortLock for writer");
        pf_writer = *wsid;
        MT_unset_lock(PF_SHORT_LOCK, "pflock_begin: shortLock for writer");
    }
    return GDK_SUCCEED;
}

/* query execution finishes; note that we have PF_SHORT_LOCK during this */
int CMDpflock_end(lng *wsid) 
{
    int mode = ((*wsid) & (3LL << 30)) >> 30;
    lng nil = lng_nil;

    if (mode == PF_MODE_DOCMGT) {
        /* last docmgt function yields the meta barrier 
         */
        if (*wsid == pf_special) {
            pf_special = -1;
        } else {
            pf_ndocmgt--;
        }
        if (pf_ndocmgt == 0) {
            MT_up_sema(PF_META_BARRIER, "pflock_end: metaBarrier for docmgt");
        }
    } else {
        /* last active meta-information query releases meta-barrier 
         */
        BUN self_passed; /* this query uses meta-information functions? */
        BUN others_passed; /* there are multiple such queries active? */

        self_passed = BUNlocate(ws_overlaps_ws, wsid, &nil);
        BUNdelHead(ws_overlaps_ws, wsid, FALSE); /* release all active info on this query */
        others_passed = BUNfnd(BATmirror(ws_overlaps_ws), &nil);
        if (self_passed && !others_passed) {
            MT_up_sema(PF_META_BARRIER, "pflock_end: metaBarrier");
        }

        /* last active update query releases update-barrier 
         */
        if (pf_writer == *wsid) {
            /* finish in exclusive mode */
            pf_writer = -1;
            MT_up_sema(PF_UPDATE_BARRIER, "pflock_end: exclusive updateBarrier");
        } else if (mode != PF_MODE_RDONLY) { 
            /* finish in concurrent mode */
            if (--pf_nreaders == 0)
                MT_up_sema(PF_UPDATE_BARRIER, "pflock_end: updateBarrier");
        }
    }
    return GDK_SUCCEED;
}

/* we are going to use a meta-information function that returns all collections/documents
 * thus we must block all new docmgt queries and wait for all active ones to finish 
 */
int CMDpflock_meta(lng *wsid) 
{
    int mode = ((*wsid) & (3LL << 30)) >> 30;
    lng nil = lng_nil;
    BUN self_passed; /* this query uses meta-information functions? */
    BUN others_passed; /* there are multiple such queries active? */

    MT_set_lock(PF_SHORT_LOCK, "pflock_meta: shortLock precheck");
    self_passed = BUNlocate(ws_overlaps_ws, wsid, &nil);
    if (*wsid == pf_special) self_passed = (BUN) wsid;
    if (self_passed) {
        MT_unset_lock(PF_SHORT_LOCK, "pflock_meta: shortLock precheck");
        return GDK_SUCCEED; /* already got permission to proceed */
    }
    if (mode == PF_MODE_DOCMGT) {
        /* SPECIAL CASE: a meta function inside a docmgt function. 
         * now we must wait for potential other docmgts to finish first 
         */
        int reapply_barrier = (pf_ndocmgt > 1);
        if (reapply_barrier) pf_ndocmgt--; 
        MT_unset_lock(PF_SHORT_LOCK, "pflock_meta: metaLock precheck");
        if (reapply_barrier) {
            MT_down_sema(PF_META_BARRIER, "pflock_meta: metaBarrier for convert");
            MT_set_lock(PF_SHORT_LOCK, "pflock_meta: shortLock special");
            pf_special = *wsid;
            MT_unset_lock(PF_SHORT_LOCK, "pflock_meta: shortLock special");
        }
        return GDK_SUCCEED;
    }
    MT_unset_lock(PF_SHORT_LOCK, "pflock_meta: shortLock precheck");

    MT_set_lock(PF_META_LOCK, "pflock_meta: metaLock");
    MT_set_lock(PF_SHORT_LOCK, "pflock_meta: shortLock insert");
    others_passed = BUNfnd(BATmirror(ws_overlaps_ws), &nil);
    BUNins(ws_overlaps_ws, wsid, &nil, FALSE);
    MT_unset_lock(PF_SHORT_LOCK, "pflock_meta: shortLock insert");
    if (!others_passed) {
        /* first query with meta function: get barrier */
        MT_down_sema(PF_META_BARRIER, "pflock_meta: metaBarrier");
    }
    MT_unset_lock(PF_META_LOCK, "pflock_meta: metaLock");
    return GDK_SUCCEED;
}

int CMDpflock_free(bit *res, bit* doCommit) {
    /* assumption: short lock is taken!! */
    *res = FALSE;
    if (*doCommit == TRUE) {
        /* only OK if no active query called a meta-information function */
        lng nil = lng_nil;
        *res = (BUNfnd(BATmirror(ws_overlaps_ws), &nil) == NULL);
    }
    return GDK_SUCCEED;
}

#include "serialize.h"

int
xquery_print_result_loop (
    str  mode,
    str  moduleNS,
    str  method,
    BAT* ws,
    BAT* loop, 
    BAT* iter, 
    BAT* item, 
    BAT* kind,
    BAT* intVAL,
    BAT* dblVAL,
    BAT* strVAL)
{
    size_t niters = BATcount (loop);
    return xquery_print_result_DRIVER (
               mode,
               moduleNS,
               method,
               NULL, /* set of printing callback function */
               NULL, /* optional arguments for the callback functions */
               ws,
               niters?niters:1, /* number of iterations */
               loop, /* loop relation */
               iter, /* iter relation */
               item, /* item relation */
               kind, /* kind relation */
               intVAL,
               dblVAL,
               strVAL);
}

int
xquery_print_result_main (
    str  mode,
    BAT* ws,
    BAT* item, 
    BAT* kind,
    BAT* intVAL,
    BAT* dblVAL,
    BAT* decVAL,
    BAT* strVAL)
{
    (void) decVAL;
    
    return xquery_print_result_DRIVER (
               mode,
               NULL, /* module */
               NULL, /* method */
               NULL, /* set of printing callback function */
               NULL, /* optional arguments for the callback functions */
               ws,
               1, /* number of iterations */
               item, /* loop relation */
               item, /* iter relation */
               item, /* item relation */
               kind, /* kind relation */
               intVAL,
               dblVAL,
               strVAL);
}

int
xquery_print_result_file (
    str  file,
    str  mode,
    BAT* ws,
    oid* item, 
    int* kind,
    BAT* intVAL,
    BAT* dblVAL,
    BAT* decVAL,
    BAT* strVAL)
{
    int len = strlen(file);
    int ret = GDK_FAIL;
    (void) decVAL;
    if (DIR_SEP != '/') {
        char *s = file; /* normalize path on windows to windows DIR_SEP */
        do { if (*s == '/') *s = DIR_SEP; } while(*(++s));
    }
    if (*file == DIR_SEP) {
        GDKerror("xquery_print_result_file: %s cannot have an absolute name.\n", file);
    } else if (len < 4 || (strcmp(file+len-4, ".xml") && strcmp(file+len-4, ".XML"))) {
        GDKerror("xquery_print_result_file: %s name does not end in .xml\n", file);
    } else if (GDKcreatedir(file)) {
        FILE *fp = fopen(file, "wb");
        if (fp) {
            stream *s = file_wastream(fp, file);
            if (s) {
                BAT* i = BATnew(TYPE_void,TYPE_oid,1);
                if (i) {
                    BAT* k = BATnew(TYPE_void,TYPE_int,1);
                    if (k) {
                        BATseqbase(k, 0);
                        BATseqbase(i, 0);
                        BUNappend(k, kind, FALSE);
                        BUNappend(i, item, FALSE);
                        ret = xquery_print_result_driver (
                             s,
                             mode,
                             NULL, /* module */
                             NULL, /* method */
                             NULL, /* set of printing callback function */
                             NULL, /* optional arguments for the callback functions */
                             ws,
                             1, /* number of iterations */
                             i, /* loop relation */
                             i, /* iter relation */
                             i, /* item relation */
                             k, /* kind relation */
                             intVAL,
                             dblVAL,
                             strVAL);
                         stream_destroy(s);
                    }
                    BBPreclaim(k);
                }
                BBPreclaim(i);
            } 
            fclose(fp);
        } else {
            GDKsyserror("xquery_print_result_file: %s", file);
        }
    }
    return ret;
}

int
xquery_print_doc_main (str mode, BAT* ws, str docName)
{
    return xquery_print_doc_DRIVER(mode,NULL,NULL,ws,docName);
}

bat* pf_support_prelude() {
    MT_init_lock(pf_runtime_lock[0], "PF_SHORT_LOCK");
    MT_init_lock(pf_runtime_lock[1], "PF_WAL_LOCK");
    MT_init_lock(pf_runtime_lock[2], "PF_FREE_LOCK");
    MT_init_lock(pf_runtime_lock[3], "PF_EXTEND_LOCK");
    MT_init_lock(pf_runtime_lock[5], "PF_META_LOCK");
    MT_init_lock(pf_runtime_lock[4], "PF_UPDATE_LOCK");
    MT_init_sema(pf_runtime_sema[0],1, "PF_META_BARRIER");
    MT_init_sema(pf_runtime_sema[1],1, "PF_UPDATE_BARRIER");
    MT_init_sema(pf_runtime_sema[2],1, "PF_EXTEND_BARRIER");
    ws_overlaps_ws = BATnew(TYPE_lng, TYPE_lng, 1024);
    BBPrename(ws_overlaps_ws->batCacheid, "ws_overlaps_ws");
    (void) BATprepareHash(ws_overlaps_ws);
    (void) BATprepareHash(BATmirror(ws_overlaps_ws));
    BATset(ws_overlaps_ws, BOUND2BTRUE);
    pf_nreaders = pf_convoy = pf_ndocmgt = 0;
    pf_special = pf_writer = -1;
    return NULL;
}

void pf_support_epilogue() {
    MT_destroy_lock(pf_runtime_lock[0]);
    MT_destroy_lock(pf_runtime_lock[1]);
    MT_destroy_lock(pf_runtime_lock[2]);
    MT_destroy_lock(pf_runtime_lock[3]);
    MT_destroy_lock(pf_runtime_lock[4]);
    MT_destroy_lock(pf_runtime_lock[5]);
    MT_destroy_sema(pf_runtime_sema[0]);
    MT_destroy_sema(pf_runtime_sema[1]);
    MT_destroy_sema(pf_runtime_sema[2]);
}

@- PROCs required by the algebraic translation ###
@mil
PROC doc_tbl (BAT[void, BAT] ws, BAT[void, str] item) : BAT[void,BAT]
{
    # load all requested documents into the working set
    var r := ws_opendoc(ws, item);

    # pick the according cont value for each document requested
    var ret_cont := r.tmark(0@0);

    # pick the according root-pre value for each document requested
    var ret_item := r.hmark(0@0);

    # return result as a BAT of BATs
    return new (void, BAT).append(ws)
                          .append(ret_item)
                          .append(ret_cont)
                          .seqbase(0@0);
}

ADDHELP("doc_tbl", "teubner", "Aug 2005",
                "PARAMETERS:\n\
ws    current working set; will be modified\n\
item  list of documents to add to the working set\n\
DESCRIPTION:\n\
Implementation of the algebra operator `doc_tbl' that\n\
loads persistent documents into the working set.\n\
Input is a list of document names. Output is a BAT of\n\
BATs with the components\n\
(a) the modified working set,\n\
(b) an `item' column with the pre values of the\n\
document roots, and\n\
(c) a `cont' column that encodes document\n\
container (within the working set) according to our\n\
working set representation.",
                "pf_support");

# primitive for supporting highly specific XQuery functionality
PROC merge_adjacent_text_nodes (BAT[void,oid] iter,
                                BAT[void,oid] pre,
                                BAT[void,oid] pcont,
                                BAT[void,BAT] ws) : BAT[void,oid]
{
    var map := pre.ord_uselect(oid_nil,oid_nil).hmark(0@0);
    iter := map.leftfetchjoin(iter);
    pre := map.leftfetchjoin(pre);
    pcont := map.leftfetchjoin(pcont);

    var kind := mposjoin (pre, pcont, ws.fetch(PRE_KIND));
    var text := kind.[=](TEXT);
    var text_sel := text.select(true).hmark(0@0);
    var text_pre := text_sel.leftfetchjoin(pre).tmark(0@0);
    var text_cont := text_sel.leftfetchjoin(pcont).tmark(0@0);

    var text_prop := mposjoin (mposjoin (text_pre, text_cont, ws.fetch(PRE_PROP)),
                               mposjoin (text_pre, text_cont, ws.fetch(PRE_CONT)),
                               ws.fetch(PROP_TEXT));
    text_pre := nil;
    text_cont := nil;

    var pre_prop := pre.mirror()
                       .outerjoin(text_prop.reverse()
                                           .leftfetchjoin(text_sel)
                                           .reverse());
    var pre_enum := [oid](text);
    var res_size := (iter.tunique().count() 
                  + text.count() + 1)
                  - text_sel.count();

    var res_strs := combine_text_string (iter.chk_order(),
                                         pre_enum,
                                         pre_prop,
                                         res_size);
    iter := nil;
    pre_enum := nil;
    pre_prop := nil;
    res_size := nil;
    var res_texts := text_constr (res_strs.tmark(0@0), ws);
    ws := res_texts.fetch(0);
    var textnodes := res_texts.fetch(1);
    res_texts := nil;
    
    text_pre := pre.mirror()
                   .outerjoin(res_strs.mark(0@0)
                                      .leftfetchjoin(textnodes));
    text_cont := pre.mirror()
                    .outerjoin(res_strs.project(WS));
    res_strs := nil;
    textnodes := nil;
    pre := map.reverse()
              .leftfetchjoin([ifthenelse](text,text_pre,pre));
    pcont := map.reverse()
                .leftfetchjoin([ifthenelse](text,text_cont,pcont));

    # return result as a BAT of BATs
    return new (void, BAT).append (ws)
                          .append (pre)
                          .append (pcont)
                          .seqbase (0@0);
}
ADDHELP("merge_adjacent_text_nodes", "tsheyar", "Oct 2005",
        "PARAMETERS:\n\
BAT[void,str] iter : prefix of qname\n\
BAT[void,str] pre  : URI of the qname\n\
BAT[void,str] cont : local part of the qname\n\
BAT[void,BAT] ws   : working set that stores the qname\n\
DESCRIPTION:\n\
merge_adjacent_text_nodes takes an iter|pre|cont schema and\n\
combines within each iteration all adjacent text nodes.\n\
(Note that the order is given by the input order.)\n\
New textnodes are added into the working set ws and the result is\n\
a bat of bats (ws, modified_pres, modified_conts). The heads of the\n\
pre and cont are aligned to the input relations.",
        "pf_support");
                                
# add_qnames changes the working set as side effect
# without a 'prefix:uri:local' index this could be quite expensive
#
# it basically does:
# [ifthenelse]([isnil](local),local.project(nil),[add_qname](prefix,uri,local,local.project(ws)));
PROC add_qnames (BAT[void,str] prefix,
                 BAT[void,str] uri,
                 BAT[void,str] local,
                 BAT[void,BAT] ws) : BAT[void,oid]
{
    var pref_uri_loc := prefix.[+](NS_ACCEL_SEP).[+](uri).[+](NS_ACCEL_SEP).[+](local); # [void,str]
    pref_uri_loc := [isnil](local).ord_uselect(false).mirror().leftfetchjoin(pref_uri_loc); # [oid,str] (sorted subset)
    return mirror(local).outerjoin(find_qn_bulk(ws, WS, pref_uri_loc, true)).tmark(seqbase(local)); # [void,oid]
}
ADDHELP("add_qnames", "tsheyar", "Oct 2005",
        "PARAMETERS:\n\
BAT[void,str] prefix : prefix of qname\n\
BAT[void,str] uri    : URI of the qname\n\
BAT[void,str] local  : local part of the qname\n\
BAT[void,BAT] ws     : working set that stores the qname\n\
DESCRIPTION:\n\
add_qnames adds qnames consisting of the three strings prefix,\n\
uri, and local to the working set (ws). The return value is the\n\
identifier that corresponds to the qname in the qname container\n\
of the transient nodes.\n\
NOTE: the working set 'ws' is changed as side effect!",
        "pf_support");

# add_qname changes the working set as side effect
PROC add_qname (str prefix, str uri, str local, BAT[void,BAT] ws) : oid
{
    var props := ws.fetch(QN_PREFIX_URI_LOC).fetch(WS);
    var key := prefix + NS_ACCEL_SEP + uri + NS_ACCEL_SEP + local;
    var itemID := oid(count(props));
    if (not(isnil(CATCH(itemID := reverse(props).find(key))))) {
        props.insert(itemID, key);
        ws.fetch(QN_HISTOGRAM).fetch(WS).insert(itemID, 1LL);
        ws.fetch(QN_URI_LOC).fetch(WS).insert(itemID, uri + NS_ACCEL_SEP + local);
        ws.fetch(QN_URI).fetch(WS).insert(itemID, uri);
        ws.fetch(QN_PREFIX).fetch(WS).insert(itemID, prefix);
        ws.fetch(QN_LOC).fetch(WS).insert(itemID, local);
    }
    return itemID;
}
ADDHELP("add_qname", "tsheyar", "Oct 2005",
        "PARAMETERS:\n\
str           prefix : prefix of qname\n\
str           uri    : URI of the qname\n\
str           local  : local part of the qname\n\
BAT[void,BAT] ws     : working set that stores the qname\n\
DESCRIPTION:\n\
add_qname adds a qname consisting of the three strings prefix,\n\
uri, and local to the working set ws. The return value is the\n\
identifier that corresponds to the qname in the qname container\n\
of the transient nodes.\n\
NOTE: the working set 'ws' is changed as side effect!",
        "pf_support");

PROC text_constr (BAT[void, str] item, BAT[void, BAT] ws) : BAT[void,BAT]
{
    # find all strings that are not already in the working set ...
    var ws_prop_text := ws.fetch(PROP_TEXT).fetch(WS);
    var unq_str := item.tunique().hmark(0@0);
    var str_unq := unq_str.reverse().kdiff(ws_prop_text.reverse());
    unq_str := nil;
    # ... and add them to the PROP_TEXT container
    var seqb := oid(int(ws_prop_text.seqbase()) + ws_prop_text.count());
    unq_str := str_unq.hmark(seqb);
    str_unq := nil;
    ws_prop_text := ws_prop_text.insert(unq_str);
    unq_str := nil;

    # get the property values of the strings;
    # we invest in sorting ws_prop_text &  X_strings/item on the TYPE_str 
    # join columns as the mergejoin proved to be faster and more robust with
    # large BATs than a hashjoin
    var ws_text_prop := ws_prop_text.reverse().sort();
    var X_item := item.mark(0@0);
    var X_strings := item.tmark(0@0).tsort();
    var X_prop := X_strings.leftjoin(ws_text_prop);
    X_strings := nil;
    ws_text_prop := nil;
    var newProp := X_item.leftjoin(X_prop);
    X_item := nil;
    X_prop := nil;

    # add new text nodes to the working set
    var seqb := oid(count(ws.fetch(PRE_KIND).fetch(WS)) +
                    int(ws.fetch(PRE_KIND).fetch(WS).seqbase()));
    var newPre_prop := newProp.tmark(seqb).chk_order();
    newProp := nil;
    ws.fetch(PRE_PROP).fetch(WS).insert(newPre_prop);
    ws.fetch(PRE_SIZE).fetch(WS).insert(newPre_prop.project(0));
    ws.fetch(PRE_LEVEL).fetch(WS).insert(newPre_prop.project(chr(0)));
    ws.fetch(PRE_KIND).fetch(WS).insert(newPre_prop.project(TEXT));
    ws.fetch(PRE_CONT).fetch(WS).insert(newPre_prop.project(WS));
    ws.fetch(PRE_NID).fetch(WS).append(newPre_prop.mirror());
    ws.fetch(NID_RID).fetch(WS).append(newPre_prop.mirror());

    newPre_prop := nil;
    item := item.mark(seqb);
    seqb := nil;

    # return result as a BAT of BATs
    var res := new (void, BAT).append (ws)
                              .append (item)
                              .append (item.project(WS))
                              .seqbase (0@0);

    { # adding new fragments to the FRAG_ROOT bat
        ws.fetch(FRAG_ROOT).fetch(WS).insert(reverse(reverse(item).project(oid_nil)));
    }

    return res;
}
ADDHELP("text_constr", "tsheyar", "Oct 2005",
        "PARAMETERS:\n\
BAT[void,str] item : content of the text nodes\n\
BAT[void,BAT] ws   : working set that stores the document representations\n\
DESCRIPTION:\n\
text_constr is a generic text constructor that creates for each\n\
item a new textnode. These textnodes are added to the working\n\
set ws. Output is a BAT of BATs with the following components:\n\
(a) the modified working set,\n\
(b) the pre values of the textnodes, and\n\
(c) the cont values of the textnodes",
        "pf_support");

PROC attr_constr (BAT[void, oid] qn, BAT[void, str] item, BAT[void, BAT] ws) : BAT[void,BAT]
{
    # find all strings that are not already in the working set ...
    var ws_prop_val := ws.fetch(PROP_VAL).fetch(WS);
    var unq_str := item.tunique().hmark(0@0);
    var str_unq := unq_str.reverse().kdiff(ws_prop_val.reverse());
    unq_str := nil;
    # ... and add them to the PROP_VAL container
    var seqb := oid(int(ws_prop_val.seqbase()) + ws_prop_val.count());
    unq_str := str_unq.hmark(seqb);
    str_unq := nil;
    ws_prop_val := ws_prop_val.insert(unq_str);
    unq_str := nil;

    # get the property values of the strings;
    # we invest in sorting ws_prop_val &  X_strings/item on the TYPE_str 
    # join columns as the mergejoin proved to be faster and more robust with
    # large BATs than a hashjoin
    var ws_val_prop := ws_prop_val.reverse().sort();
    var X_item := item.mark(0@0);
    var X_strings := item.tmark(0@0).tsort();
    var X_prop := X_strings.leftjoin(ws_val_prop);
    X_strings := nil;
    ws_val_prop := nil;
    var newProp := X_item.leftjoin(X_prop);
    X_item := nil;
    X_prop := nil;

    # add new text nodes to the working set
    var seqb := oid(count(ws.fetch(ATTR_OWN).fetch(WS)) +
                    int(ws.fetch(ATTR_OWN).fetch(WS).seqbase()));
    var newAttr_prop := newProp.tmark(seqb);
    newProp := nil;
    ws.fetch(ATTR_PROP).fetch(WS).insert(newAttr_prop);
    ws.fetch(ATTR_OWN).fetch(WS).insert(newAttr_prop.project(oid_nil));
    ws.fetch(ATTR_QN).fetch(WS).insert(qn.tmark(seqb));
    ws.fetch(ATTR_CONT).fetch(WS).insert(newAttr_prop.project(WS));

    newAttr_prop := nil;
    item := item.mark(seqb);
    seqb := nil;

    # return result as a BAT of BATs
    var res := new (void, BAT).append (ws)
                              .append (item)
                              .append (item.project(WS))
                              .seqbase (0@0);

    return res;
}
ADDHELP("attr_constr", "tsheyar", "Oct 2005",
        "PARAMETERS:\n\
BAT[void,oid] qn   : names of the attributes\n\
BAT[void,str] item : values of the attributes\n\
BAT[void,BAT] ws   : working set that stores the document representations\n\
DESCRIPTION:\n\
attr_constr is a generic attribute constructor that creates for\n\
each aligned qn|item pair a new attribute. These attributes are\n\
added to the working set ws. Output is a BAT of BATs with the\n\
following components:\n\
(a) the modified working set,\n\
(b) the attribute ids, and\n\
(c) the cont values of the attributes",
        "pf_support");

PROC elem_constr_empty (BAT[void, oid] qn, BAT[void, BAT] ws) : BAT[void,BAT]
{
    # add new element nodes to the working set
    var seqb := oid(count(ws.fetch(PRE_KIND).fetch(WS)) +
                    int(ws.fetch(PRE_KIND).fetch(WS).seqbase()));
    var newPre_prop := qn.tmark(seqb).chk_order();
    ws.fetch(PRE_PROP).fetch(WS).insert(newPre_prop);
    ws.fetch(PRE_SIZE).fetch(WS).insert(newPre_prop.project(0));
    ws.fetch(PRE_LEVEL).fetch(WS).insert(newPre_prop.project(chr(0)));
    ws.fetch(PRE_KIND).fetch(WS).insert(newPre_prop.project(ELEMENT));
    ws.fetch(PRE_CONT).fetch(WS).insert(newPre_prop.project(WS));
    ws.fetch(PRE_NID).fetch(WS).append(newPre_prop.mirror());
    ws.fetch(NID_RID).fetch(WS).append(newPre_prop.mirror());

    newPre_prop := nil;
    qn := qn.mark(seqb);
    seqb := nil;

    # return result as a BAT of BATs
    var res := new (void, BAT).append (ws)
                              .append (qn)
                              .append (qn.project(WS))
                              .seqbase (0@0);

    { # adding new fragments to the WS_FRAG bat
        ws.fetch(FRAG_ROOT).fetch(WS).insert(reverse(reverse(qn).project(oid_nil)));
    }

    return res;
}
ADDHELP("elem_constr_empty", "tsheyar", "Oct 2005",
        "PARAMETERS:\n\
BAT[void,oid] qn   : names of the elements\n\
BAT[void,BAT] ws   : working set that stores the document representations\n\
DESCRIPTION:\n\
elem_constr_empty is a generic element constructor for empty\n\
elements that creates for each qname qn a new element. These\n\
elements are added to the working set ws. Output is a BAT of\n\
BATs with the following components:\n\
(a) the modified working set,\n\
(b) the pre values, and\n\
(c) the cont values of the elements",
        "pf_support");

PROC elem_constr (BAT[void, oid] qn_iter,
                  BAT[void, oid] qn_item,
                  BAT[void, oid] iter,
                  BAT[oid, oid] pre,
                  BAT[oid, oid] pcont,
                  BAT[void, oid] attr,
                  BAT[void, oid] acont,
                  BAT[void, BAT] ws) : BAT[void,BAT]
{
    var root_iter;
    var root_size;
    var root_prop;
    var root_kind;
    var root_cont;
    var root_level;
    # attr
        var root_pre;
        var root_pre_cont;
        
    # throw out nil values and generate iter|item|cont representation
    # for attributes
    var selected := pre.select(oid_nil,oid_nil);
    var piter := selected.hmark(0@0).leftfetchjoin(iter).tmark(0@0); # make it void
    pre := selected.tmark(0@0);
    pcont := pcont.select(oid_nil,oid_nil).tmark(0@0);
    selected := nil;

    # throw out nil values and generate iter|item|cont representation
    # for attributes
    selected := attr.select(oid_nil,oid_nil);
    var aiter := selected.hmark(0@0).leftfetchjoin(iter).tmark(0@0); # make it void
    attr := selected.tmark(0@0);
    acont := acont.select(oid_nil,oid_nil).tmark(0@0);
    selected := nil;

    if (pre.count() != 0) {

        # use head to avoid elimination of duplicates
        # (this is additionally used in the content level determination
        var iter_unq := piter.mirror();
        # get all subtree copies
        var res_scj := loop_lifted_descendant_or_self_step 
                           (iter_unq, pre, pcont, ws, 0);
        iter_unq := nil;

        # variables for the result of the scj 
        var res_iter := res_scj.fetch(0);
        var res_item := res_scj.fetch(1);
        # !be aware that res_cont is only a fake_project!
        var res_cont := res_scj.fetch(2);
        # !avoid being res_iter a fake_project!
        res_iter := materialize (res_iter, res_item);
        res_scj := nil;
            
        # create content_iter as sorting argument for the merged union
        var content_iter := res_iter.leftfetchjoin(piter).chk_order();
        # create subtree copies for all bats
        var content_size := mposjoin(res_item, res_cont, ws.fetch(PRE_SIZE));
        var content_prop := mposjoin(res_item, res_cont, ws.fetch(PRE_PROP));
        var content_kind := mposjoin(res_item, res_cont, ws.fetch(PRE_KIND));
        var content_cont := mposjoin(res_item, res_cont, ws.fetch(PRE_CONT));
        var content_level := mposjoin(res_item, res_cont, ws.fetch(PRE_LEVEL));
        # change the level of the subtree copies
        content_level := content_level.[+](chr(1));
        var contentRoot_level := mposjoin(pre, pcont, ws.fetch(PRE_LEVEL));
        # map Root_level to the result of the scj 
        #using the faked iteration values
        contentRoot_level := res_iter.leftfetchjoin(contentRoot_level);
        content_level := content_level.[-](contentRoot_level);
        content_level := content_level.tmark(0@0);
        contentRoot_level := nil;
            
        # attr
        # content_pre is needed for attribute subtree copies
        var content_pre := res_item;

        # as well as content_pre_cont
        var content_pre_cont := res_cont;
            
        root_iter := qn_iter.chk_order();
        # calculate the sizes for the root nodes
        root_size := {count}(content_iter.reverse(), qn_iter.reverse(), FALSE).tmark(seqbase(qn_iter));
        root_prop := qn_item;
        root_kind := constant2bat(ELEMENT);
        root_cont := constant2bat(WS);
        root_level := constant2bat(chr(0));

        # attr
            # root_pre is a dummy needed for merge union with content_pre 
            root_pre := constant2bat(oid_nil);
            # as well as root_cont_pre
            root_pre_cont := constant2bat(oid_nil);

        # merge union root and nodes
        {
        var merged_result := merged_union (
                                 root_iter, content_iter,
                                 root_size, content_size,
                                 root_level, content_level,
                                 root_kind, content_kind,
                                 root_prop, content_prop,
                                 root_cont, content_cont,
        # attr
                                 root_pre, content_pre,
                                 root_pre_cont, content_pre_cont);
        root_iter := nil;
        content_iter := nil;
        content_size := nil;
        content_level := nil;
        content_kind := nil;
        content_prop := nil;
        content_cont := nil;
        # attr
            content_pre := nil;
            content_pre_cont := nil;
        root_size := merged_result.fetch(1);
        root_level := merged_result.fetch(2);
        root_kind := merged_result.fetch(3);
        root_prop := merged_result.fetch(4);
        root_cont := merged_result.fetch(5);
        # attr
            root_pre := merged_result.fetch(6);
            root_pre_cont := merged_result.fetch(7);

        merged_result := nil;

        # printing output for debugging purposes
            # print("merged (root & content)");
            # print(root_size, [int](root_level), [int](root_kind), root_prop);
        }

    } else { # end of ``if (pre.count() != 0)''

        root_size := qn_iter.project(0);
        root_prop := qn_item; # !the seqbase of qn_item is later modified
        root_kind := qn_iter.project(ELEMENT);
        root_cont := qn_iter.project(WS);
        root_level := qn_iter.project(chr(0));
        # attr
            root_pre := qn_iter.project(oid_nil);
            root_pre_cont := qn_iter.project(oid_nil);

    }  # end of else in ``if (pre.count() != 0)''
        
    # set the offset for the new created trees
    {
        var seqb := oid(count(ws.fetch(PRE_SIZE).fetch(WS))
                        + int(ws.fetch(PRE_SIZE).fetch(WS).seqbase()));
        root_size := root_size.seqbase(seqb);
        root_prop := root_prop.reverse().mark(seqb).reverse();
        root_kind := root_kind.seqbase(seqb);
        root_cont := root_cont.seqbase(seqb);
        root_level := root_level.seqbase(seqb);
        # attr
            # get the new pre values
            root_pre := root_pre.seqbase(seqb);
            root_pre_cont := root_pre_cont.seqbase(seqb);
        seqb := nil;
    }

    # insert the new trees into the working set
    ws.fetch(PRE_SIZE).fetch(WS).insert(root_size);
    ws.fetch(PRE_KIND).fetch(WS).insert(root_kind);
    ws.fetch(PRE_PROP).fetch(WS).insert(root_prop);
    ws.fetch(PRE_CONT).fetch(WS).insert(root_cont);
    ws.fetch(PRE_LEVEL).fetch(WS).insert(root_level);
    ws.fetch(PRE_NID).fetch(WS).append(root_kind.mirror());
    ws.fetch(NID_RID).fetch(WS).append(root_kind.mirror());

    # save the new roots for creation of the intermediate result
    var roots := root_level.ord_uselect(chr(0));
    # (note that all operations are order preserving and ``mark''
    # aligns the key with the qn_iter input 
    roots := roots.hmark(0@0);

    # resetting the temporary variables
    root_size := nil;
    root_prop := nil;
    root_kind := nil;
    root_cont := nil;
    root_level := nil;
        
    # adding the new constructed roots to the WS_FRAG bat of the
    # working set, that a following (preceding) step can check
    # the fragment boundaries
    {
        ws.fetch(FRAG_ROOT).fetch(WS).insert(reverse(reverse(roots).project(oid_nil)));
    }

    # ----------------------------------
    # ----- ATTRIBUTE TRANSLATION ------
    # ----------------------------------
    # 1. step: add subtree copies of attributes
    if (pre.count() != 0) { # but only if there are any subtree nodes
        # lookup the affected attributes using the old pre values
        var preNew_attr := mvaljoin(root_pre, 
                                    root_pre_cont,
                                    ws.fetch(ATTR_OWN));
        # lookup the first free attr value
        var seqb := oid(ws.fetch(ATTR_QN).fetch(WS).count());
        # split up result of mvaljoin and mark them with the correct seqbase
        var attrNew_preNew := preNew_attr.mark(seqb).reverse();
        var attrNew_attrOld := preNew_attr.reverse().mark(seqb).reverse();
        preNew_attr := nil;
        var attrNew_pre_cont := attrNew_preNew.leftfetchjoin(root_pre_cont);
        # help MIL to keep head void
        attrNew_pre_cont := attrNew_pre_cont.reverse().mark(seqb).reverse();
        seqb := nil;

        # get the values of the QN/OID offsets for the reference to the
        # string values
        var attrNew_qn := mposjoin(attrNew_attrOld,
                                   attrNew_pre_cont,
                                   ws.fetch(ATTR_QN));
        var attrNew_prop := mposjoin(attrNew_attrOld,
                                     attrNew_pre_cont,
                                     ws.fetch(ATTR_PROP));
        # get container where values are stored (not where attribute is stored)
        var attrNew_cont := mposjoin(attrNew_attrOld,
                                     attrNew_pre_cont,
                                     ws.fetch(ATTR_CONT));
        attrNew_attrOld := nil;
        attrNew_pre_cont := nil;

        ws.fetch(ATTR_QN).fetch(WS).insert(attrNew_qn);
        ws.fetch(ATTR_PROP).fetch(WS).insert(attrNew_prop);
        ws.fetch(ATTR_OWN).fetch(WS).insert(attrNew_preNew);
        ws.fetch(ATTR_CONT).fetch(WS).insert(attrNew_cont);
        attrNew_qn := nil;
        attrNew_prop := nil;
        attrNew_preNew := nil;
        attrNew_cont := nil;
    }

    # 2. step: add attribute binding for new elements
    if (attr.count() != 0) { # but only if there are any top level attributes
        
        # use iter, qn and cont to find unique combinations
        var attr_qn := mposjoin(attr, acont, ws.fetch(ATTR_QN));
        var attr_cont := mposjoin(attr, acont, ws.fetch(ATTR_CONT));
        var sorting := aiter.tsort();
        sorting := sorting.CTrefine(mposjoin(attr_qn,
                                             attr_cont,
                                             ws.fetch(QN_URI_LOC)));
        var unq_attrs := sorting.tunique();
        sorting := nil;
        # test uniqueness
        if (unq_attrs.count() != aiter.count())
        {
           if (qn_item.count() > 0) {
               ERROR ("err:XQDY0025: attribute names are not unique in constructed element '%s'.",
                      qn_item.leftfetchjoin(ws.fetch(QN_LOC).fetch(WS))
                             .fetch(0));
           } else {
               ERROR ("err:XQDY0025: attribute names are not unique in constructed element.");
           }
        }
        unq_attrs := nil;

        var seqb := oid(ws.fetch(ATTR_QN).fetch(WS).count());
        attr_qn := attr_qn.seqbase(seqb);
        var attr_own := aiter.leftjoin(qn_iter.reverse())
                             .leftfetchjoin(roots)
                             .reverse().mark(seqb).reverse();
        var attr_prop := mposjoin(attr, acont, ws.fetch(ATTR_PROP));
        attr_prop := attr_prop.seqbase(seqb);
        attr_cont := attr_cont.seqbase(seqb);
        seqb := nil;

        ws.fetch(ATTR_QN).fetch(WS).insert(attr_qn);
        ws.fetch(ATTR_PROP).fetch(WS).insert(attr_prop);
        ws.fetch(ATTR_OWN).fetch(WS).insert(attr_own);
        ws.fetch(ATTR_CONT).fetch(WS).insert(attr_cont);
        attr_qn := nil;
        attr_prop := nil;
        attr_own := nil;
        attr_cont := nil;
    }

    # create result as a BAT of BATs
    var res := new (void, BAT).append (ws)
                              .append (roots)
                              .append (roots.project(WS))
                              .seqbase (0@0);
    roots := nil;

    return res;
}
ADDHELP("elem_constr", "tsheyar", "Oct 2005",
        "PARAMETERS:\n\
BAT[void, oid] qn_iter : iteration values of the qnames\n\
BAT[void, oid] qn_item : names of the elements\n\
BAT[void, oid] iter    : iteration values of the content\n\
BAT[oid, oid] pre      : pre values of the content (heads aligned with head of iter)\n\
BAT[oid, oid] pcont    : node cont values of the content (heads aligned with head of iter)\n\
BAT[void, oid] attr    : attribute ids of the content (heads aligned with head of iter)\n\
BAT[void, oid] acont   : attribute container-ids of the content (heads aligned with head of iter)\n\
BAT[void, BAT] ws      : working set that stores the document representations\n\
DESCRIPTION:\n\
elem_constr is a full featured element constructor that requires\n\
qn|iter pairs for the name part and iteration, attribute, and node\n\
information for the content. These elements are added to the working\n\
set ws. Output is a BAT of BATs with the following components:\n\
(a) the modified working set,\n\
(b) the pre values, and\n\
(c) the containder-ids of the elements",
        "pf_support");
