@' The contents of this file are subject to the Pathfinder Public License
@' Version 1.1 (the "License"); you may not use this file except in
@' compliance with the License.  You may obtain a copy of the License at
@' http://monetdb.cwi.nl/Legal/PathfinderLicense-1.1.html
@'
@' Software distributed under the License is distributed on an "AS IS"
@' basis, WITHOUT WARRANTY OF ANY KIND, either express or implied.  See
@' the License for the specific language governing rights and limitations
@' under the License.
@'
@' The Original Code is the Pathfinder system.
@'
@' The Original Code has initially been developed by the Database &
@' Information Systems Group at the University of Konstanz, Germany and
@' is now maintained by the Database Systems Group at the Technische
@' Universitaet Muenchen, Germany.  Portions created by the University of
@' Konstanz and the Technische Universitaet Muenchen are Copyright (C)
@' 2000-2005 University of Konstanz and (C) 2005-2008 Technische
@' Universitaet Muenchen, respectively.  All Rights Reserved.

@f pf_support
@a Stefan Manegold
@v 1.0
@t MIL primitives to support the XQuery front-end "Pathfinder"

@* Introduction
This module provides new MIL primitives to support the XQuery
implementation on top of MonetDB within the "Pathfinder" project.
@
@* Module Definition 
@m
.MODULE pf_support;

.USE malalgebra;
.USE lock;
.USE monettime;
.USE streams;

@- XML shredder (shredder.mx)
@m
.COMMAND shred_url(BAT[str,bat], str url, lng percentage, lock l, bit verbose) : void = CMDshred_url;
"PARAMETERS:
url - document locates at this uri is shredded. 
percentage - a number [0,100] indicating the amount of free space to reserve for updates. percentage=0 leads to a read-only document!" 

.COMMAND shred_str(BAT[str,bat], str buffer, lng percentage, lock l, bit verbose) : void = CMDshred_str;
"PARAMETERS:
buffer - the XML string to shred. 
percentage - a number [0,100] indicating the amount of free space to reserve for updates. percentage=0 leads to a read-only document!" 

.COMMAND shred_stream(BAT[str,bat], Stream s, lng percentage, lock l, bit verbose) : void = CMDshred_stream;
"PARAMETERS:
s - XML input stream. 
percentage - a number [0,100] indicating the amount of free space to reserve for updates. percentage=0 leads to a read-only document!" 


@- XML print functions (serialize.mx)
@m
.COMMAND print_doc(str,BAT[void,bat], str) : void = xquery_print_doc_main;
 "C interface to Workset print routine"

.COMMAND print_result(str mode, str moduleNS, str method,
                      BAT[oid,bat] ws,
                      BAT[oid,any] loop, BAT[void,oid] iter,
                      BAT[void,oid] item, BAT[void,int] kind, 
                      BAT[oid,lng] int_values, BAT[oid,dbl] dbl_values, 
                      BAT[oid,str] str_values) :
                            void = xquery_print_result_loop;
 "C interface to Workset result print routine, that can print multiple iters"

.COMMAND print_result(str mode, BAT[void,bat] ws,
                      BAT[void,oid] item, BAT[void,int] kind,
                      BAT[void,lng] int_values, BAT[void,dbl] dbl_values,
                      BAT[void,dbl] dec_values, BAT[void,str] str_values) :
                            void = xquery_print_result_main;
 "C interface to Workset result print routine"

.COMMAND print_result(str file, str mode, BAT[void,bat] ws,
                      oid item, int kind,
                      BAT[void,lng] int_values, BAT[void,dbl] dbl_values,
                      BAT[void,dbl] dec_values, BAT[void,str] str_values) :
                            void = xquery_print_result_file;
 "C interface to Workset result print routine, but print to a file"



@- Multi Join
@m
.COMMAND mposjoin( BAT[oid,oid] pre, BAT[oid,oid] cont, BAT[oid, bat] ws_item )
		: BAT[void,any] = CMDmposjoin;
"PARAMETERS:
BAT[oid,oid] - the values which have to be looked up
BAT[oid,oid] - the corresponding containers in which the values have to be looked up
BAT[oid,BAT[oid,any]] - the list of bats where the values are looked up
All BAT-heads must be dense.
DESCRIPTION:
looks up the values in a list of bats. The first argument specifies the
value to be looked up (joined) and the second one saves which bat contains
the value (see also 'mvaljoin').
The result is a bat with the tail values from the batlist (any) and the same
void column like the first two arguments"

.COMMAND mvaljoin( BAT[oid,oid] pre, BAT[oid,oid] cont, BAT[oid, bat] ws_item )
		: BAT[oid,oid] = CMDmvaljoin;
"PARAMETERS:
BAT[oid,oid] - the values which have to be joined
BAT[oid,oid] - the corresponding containers in which the values have to be looked up
BAT[oid,BAT[oid,oid]] - the list of bats where the tail values are joined
All BAT-heads must be dense.
DESCRIPTION:
joins the tail values of the first argument with the tail values of the bat
from one bat of the third argument. Which bat from the list is chosen is
specified by the second argument (see also 'mposjoin').
The result is a bat with the head values of the first two arguments in the head
and the head values from the batlist"


@- new algebraic primitives
@m
.COMMAND merged_union( any left, any right, ..any.. )
		: BAT[void,BAT] = CMDmerged_union;
"PARAMETERS:
Even number of BAT[oid,any] with dense heads and pairs of equal tail type;
all odd BATs must be head-aligned and all even BATs must be head-aligned;
first two BATs must be sorted on tail values.
DESCRIPTION:
Merges pairs of bats according to the order as defined by the first pair's tails."
@(
"Returns the union of two *tail-sorted* BATs. All BUNs of both BATs appear
 in the result, i.e., duplicates are not eliminated. As opposed to standard
 "union", the sortedness on the tail column is maintained in the result and
 all BUNs in the result appear in the same order as in their respective
 input."
@)

.COMMAND multi_merged_union (BAT[void, BAT] b)
		: BAT[void,BAT] = CMDmulti_merged_union;
"PARAMETERS:
BAT[void,BAT t[void,BAT c[VoID,any]]]:
a void-headed BAT of n void-headed BATs t such that each BAT t represents a
relational table t of m columns c, each represented by a dense-headed BAT c;
the first column of each table must be sorted.
DESCRIPTION: 
Merges the n relational tables t according to the value-order as defined by the
first columns of all tables."

.COMMAND ll_tokenize( BAT[void,str] strs, BAT[void,str] seps )
		: BAT[oid,str] = CMDll_strSplit;
"PARAMETERS:
BAT[void,str] - the strings which are going to be tokenized
BAT[void,str] - separators for the strings
DESCRIPTION:
ll_tokenize is a special version of [split](bat(void,str),bat(void,str)), which
tokenizes a string using the aligned separator string and adds a row for
each substring, with the oid of the input head identifying the original boundaries."

.COMMAND round_up(dbl x) : dbl = math_unary_up_ROUND;
"PARAMETERS:
dbl - the argument to round
DESCRIPTION:
round_up rounds to 0 digits after the point. The only difference to
round(dbl, 0) is that negative numbers with .5 are rounded up 
(e.g., round(-1.5) = -1.0)."

.COMMAND normSpace(str string)
		: str = CMDnormSpace;
"PARAMETERS:
BAT[void,str] - the strings which are going to be normalized
DESCRIPTION:
normSpace normalizes the whitespaces to a single space."

.COMMAND combine_text_string( BAT[void,oid] iter, BAT[void,oid] kind, BAT[void,str] str_value, int result_size )
		: BAT[oid,str] = CMDcombine_text_string;
"PARAMETERS:
BAT[void,oid] - iter values which have to be ordered
BAT[void,oid] - one of three different kinds (strings '2@0'; text-nodes '1@0' and other nodes '0@0')
BAT[void,str] - string values of the strings and text-nodes; an empty string for each other node
DESCRIPTION:
very specialized helper function for the translation of the item-sequence-to-node-sequence function in XQuery.
It expects three aligned columns and creates one string out of adjacent strings and text-nodes,
which can be translated into a text-node again. Every other node and every new iter divide the strings.
A space is inserted if two strings are adjacent, and no space is inserted if a text-node is in between.
Empty strings are not added to the output."

.COMMAND string_join( BAT[oid,str] iter_str, BAT[oid,str] separator )
		: BAT[oid,str] = CMDstring_join;
"PARAMETERS:
BAT[oid,str] - sorted iters in the head, string values in the tail
BAT[oid,str] - separator which is added between strings, within an iter
DESCRIPTION:
string_join constructs for each iter one strings by appending the strings
with the separator for this iter in between."

.COMMAND enumerate( BAT[void,lng] startval, BAT[void,lng] length) : BAT[oid,lng] = CMDenumerate;
"PARAMETERS:
BAT[void,oid] - list of start values
BAT[void,oid] - list of length values
DESCRIPTION:
enumerate creates for each input row #length rows with the values beginning
at startval and increasing by one with every row."

.COMMAND dbl(int,int,int,int,int,int,int,int) : dbl = CMDdblFromBytes;
"returns the double formed by concatenating 8 bytes (passed as integer) into its 64-bits IEEE double bit pattern.
 This function was introduced to eliminate rounding errors on doubles initially parsed in the XQuery
 parser, then passed as string to the MIL parser. No XQuery passes them as a 64-bits pattern identically."

.COMMAND ebv(BAT[oid, bit] b) : BAT[oid, bit] = CMDebv;
"PARAMETERS
BAT[oid, bit], sorted on head values.
DESCRIPTION:
This is a helper function to implement the XQuery ``effective boolean\n
value.'' Grouped by the head values, it will look at the tail values.\n
If for a head value there is exactly one BUN whose tail equals `true',\n
the result for this group will be `true' as well. In any other case\n
(a single `false' BUN, or multiple BUNs for this group), a `false'\n
tuple will be in the result for this group."

.COMMAND invalid_qname(BAT[any,str]) : str = CMDinvalid_qname;
"PARAMETERS
BAT[any, str] 
DESCRIPTION:
This is a helper function that tries to find an invalid qname
string. It returns the first invalid one it sees or str(nil) 
if all are ok." 

.COMMAND lastmod_time(str filename) : timestamp = CMDlastmod_time;
 "return the last modification time of a file"

.COMMAND correct_sizes(bat[void,oid] iter, bat[void,oid] item, bat[void,int] size)
                      : bat[void,int] = CMDcorrect_sizes;
 "correct subtree sizes when copying subtrees;
  silently modifies third argument (size-BAT) in place."

@- Value Hashing
@m
.COMMAND xquery_hash(str) : int = CMDxquery_hash; ""
.COMMAND [xquery_hash](BAT[void,str]) : BAT[void,int] = CMDxquery_hash_bat; ""

@- Constant Columns

In XQuery, we often have the case that BAT[void,T] all contain the same tail value
'constant columns'. In this case, we represent the column variable by a MIL
constant. The new 'constant' MIL module mimics the BAT algebra on such constants.

Regrettably, we cannot always represent constants as single MIL (non-bat) values.
Constants sometimes *need* to go into bats-of-bats. Particular cases in XQuery are 
the working set BAT[void,BAT] that contains a set of columns, and the result of staircase 
join (ditto). 

Our solution here is to represent constant columns inside a bat of bats as a 'fake' 
BAT[void,T] with a single entry [nil,c].  The routines constant2bat/bat2constant switch
between the MIL constant and the 'fake' BAT[nil,c] representation (obviously, non-constant
bats are unaffected by these calls). The switching is done automatically when 
bats fetched from a bat-of-bats (using an overloaded fetch) and put into them (with
an overloaded insert).
@m
.COMMAND is_constant(any) : bit = CMDisFakeProject;
"checks whether the parameter is a BAT[nil,c] constant column"

.COMMAND constant2bat(any col) : bat[any,any] = CMDfakeProject;
"if a column is a MIL constant c, change it into a BAT[nil,c] constant column."

.COMMAND bat2constant(any col) : any = CMDdeFakeProject;
"convert BAT[nil,c] constant column into a simple MIL constant 'c'"

.COMMAND fetch(BAT[any,bat] b, int pos) : any = CMDfetchConvert_int;
"fetches a bat from a bat-of-bats, and if it is a fake-bat converts it to a MIL constant"

.COMMAND fetch(BAT[any,bat] b, oid pos) : any = CMDfetchConvert_oid;
"fetches a bat from a bat-of-bats, and if it is a fake-bat converts it to a MIL constant"

.COMMAND insert(BAT[any::1,bat] b, any::1 head, any tail) : BAT[any::1,bat] = CMDinsertConvert;
"insert into a bat-of-bats; if a non-bat tail is inserted, it is inserted as a fake-bat."

.COMMAND append(BAT[any::1,bat] b, any tail) : BAT[any::1,bat] = CMDappendConvert;
"append into a bat-of-bats; if a non-bat tail is appended, it is appended as a fake-bat."

@- Path steps (staircase join) 
@m
@:ll_prec_foll_decl(following)@
@:ll_prec_foll_decl(preceding)@

@= ll_prec_foll_decl
.COMMAND ll_@1(BAT[oid,oid] iter,
               BAT[oid,oid] ctx,
               BAT[oid,oid] doc_pre,
               BAT[oid,int] pre_size,
               BAT[oid,chr] pre_kind,
               chr kind_test): BAT[oid,oid] = PFll_@1;
"PARAMETERS:
BAT[void,oid] iter (grouping relation; sorted on tail within each ctx group)
BAT[void,oid] ctx (context set; sorted on tail)
BAT[void,oid] doc_pre (table of document containers; doc id, preorder start value)
BAT[void,int] pre_size (from the working set)
BAT[void,chr] pre_kind (from the working set)
chr kind_test (kind to test for; chr_nil if no kind test)
DESCRIPTION:
returns all nodes on the @1 axis of the ctx-nodes duplicate free for each group."
@
@m
@:ll_sibling_decl(following)@

@= ll_sibling_decl
.COMMAND ll_@1_sibling(BAT[oid,oid] iter,
               BAT[oid,oid] ctx,
               BAT[oid,int] pre_size,
               BAT[oid,chr] pre_level,
               BAT[oid,chr] pre_kind,
               chr kind_test): BAT[oid,oid] = PFll_@1_sibling;
"PARAMETERS:
BAT[void,oid] iter (grouping relation; sorted on tail within each ctx group)
BAT[void,oid] ctx (context set; sorted on tail)
BAT[void,int] pre_size  (from the working set)
BAT[void,int] pre_level (from the working set)
BAT[void,chr] pre_kind  (from the working set)
chr kind_test (kind to test for; chr_nil if no kind test)
DESCRIPTION:
returns all nodes on the @1-sibling axis of the ctx-nodes duplicate free for each group."
@
@m
@:ll_cmd(descendant)@
@:ll_cmd(descendant_or_self)@
@:ll_cmd(child)@

@= ll_cmd
.COMMAND ll_@1(BAT[oid,oid] iter,
                   BAT[oid,oid] ctx,
                   BAT[oid,oid] end_ctx,
                   BAT[oid,int] pre_size,
                   BAT[void,any] cands,
                   bit one_iter, bit one_ctx,
                   oid min_iter, oid max_iter,
                   bit no_iter_order, chr kind_test): BAT[oid,oid] = PFll_@1;
"PARAMETERS:
BAT[void,oid] iter (grouping relation; sorted on tail within each ctx group)
BAT[void,oid] ctx (context set; sorted on tail)
BAT[void,oid] end_ctx (context end for 'early-exit' child step in preceding-sibling implementation;
                       head-aligned with iter & ctx or empty (=> no 'early-exit'))
BAT[void,int] pre_size (from the working set)
BAT[oid,oid]  cands (sorted list of result candidate OIDs in the tail)
bit           one_iter (only one iter?)
bit           one_ctx  (only one ctx node, i.e., the same for all iters?)
oid           min_iter,max_iter (smallest and largest iter id)
bit           no_iter_order (descendant & descendant_or_self, only: 
               result will be ordered on item, but not sub-ordered on iter)
DESCRIPTION:
returns all nodes on the @1 axis of the ctx-nodes duplicate free for each group."
@
@m
@:ll_upwards(parent,parent)@
@:ll_upwards(ancestor,ancestor)@
@:ll_upwards(ancestor_or_self,ancestor-or-self)@

@= ll_upwards
.COMMAND ll_@1(BAT[oid,oid] iter,
                   BAT[oid,oid] ctx,
                   BAT[oid,int] pre_size, BAT[oid,chr] pre_level): BAT[oid,oid] = PFll_@1;
"PARAMETERS:
BAT[void,oid] iter (grouping relation; sorted on tail within each ctx group)
BAT[void,oid] ctx (context set; sorted on tail)
BAT[void,int] pre_size (from the working set)
BAT[void,int] pre_level (from the working set)
DESCRIPTION:
returns all nodes on the @2 axis of the ctx-nodes duplicate free for each group."

@- string primitives
@m
.COMMAND splitbat(BAT[oid,str], str sep) : BAT[oid,BAT] = CMDSplitBat;
"Split the strings in the BAT on the separator sep which may not be empty.
Returns a BAT with as many BATs as the maximum number of substrings.
Each of those BATs is the same size as the input BAT and contains either
the next substring or str(nil) if there are no more substrings."

.COMMAND splitkind(BAT[oid,chr]) : BAT[oid,BAT] = CMDSplitKind;
"Split a \"kind\" BAT into separate BATs.
Returns a BAT of 5 BATs where the ith BAT is like the result of a uselect(i),
but does this in a single scan."

.COMMAND cleantmpdir(lng lim) : void = CMDcleantmpdir;
"quietly delete all files older than 'lim' (unix time) from the tmp dir (dbfarm/dbname/tmp)"

@- bit manipulations
@= nilarithMIL
.COMMAND nil@1(int v1, int v2) : int = CMDnil@1;
"Compute @1 on the two arguments, viewing nil as just another bit pattern."
@m
@:nilarithMIL(and)@
@:nilarithMIL(or)@
@:nilarithMIL(plus)@

@- transaction locking 
@m
.COMMAND pflock_get(int i) : lock = CMDpflock_get;
 "provide a pointer to a global lock"
.COMMAND pflock_begin(lng wsid) : void = CMDpflock_begin;
 "let a query (rdonly,docmgmt,update) enter the system"
.COMMAND pflock_end(lng wsid) : void = CMDpflock_end;
 "a query (rdonly,docmgmt,update) leaves the system.
  NOTE: you *must* have the short lock when executing this!!"
.COMMAND pflock_meta(lng wsid) : void = CMDpflock_meta;
 "alert that query (rdonly,docmgmt,update) reads meta information"
.COMMAND pflock_free(bit persistent) : bit = CMDpflock_free;
 "return false if there is an active document mgt query.
  NOTE: you *must* have the short lock when executing this!!"

@- retrieve BAT ws that belongs to wsid; note that you must be sure that it is still alive
@m
.COMMAND ws_bat(lng wsid) : BAT[void,bat] = CMDws_bat; 
 "retrieve BAT ws that belongs to wsid; note that you must be sure that it is still alive"

.COMMAND docsused(BAT[any,oid] any_pre, BAT[oid,oid] frag_root, BAT[void,oid] nid_rid,
                 BAT[void,oid] map_pid, BAT[void,int] pre_size) : BAT[oid,oid] = CMDdocsused;
 "find out the list of [rootpre,docid] in frag_root[docid,rootnid] that have nodes in any_pre (nids are swizzled on demand)"

.PRELUDE = pf_support_prelude;
.EPILOGUE = pf_support_epilogue;

.END pf_support;

@mil
module("xtables");
module("aggrX3");
module("bat_arith");
module("mmath");
module("pcre");
module("malalgebra");
module("monettime");
module("lock");
module("logger");

# global constants using in the MIL translation
const empty_kind_bat := bat(void,int,0).seqbase(0@0).rename("empty_kind_bat").access(BAT_READ);
const EMPTY_STRING := 0@0;
const OID_PAGE_BITS := isnil(lng(1LL << 32).oid()).ifthenelse(10,9);
const empty_dbl__bat := bat(void,dbl,0).seqbase(0@0).rename("empty_dbl__bat").access(BAT_READ);
const empty_dec__bat := empty_dbl__bat;
const empty_str__bat := bat(void,str,0).seqbase(0@0).rename("empty_str__bat").access(BAT_READ);
const empty_int__bat := bat(void,lng,0).seqbase(0@0).rename("empty_int__bat").access(BAT_READ);
const bool_not := bat(void,oid,2).append(1@0).append(0@0).rename("bool_not").seqbase(0@0).access(BAT_READ);
const bool_str := bat(void,str,2).append("false").append("true").rename("bool_str").seqbase(0@0).access(BAT_READ);

# nil constants (saves some run-time casting)
const bit_nil       := bit(nil);
const chr_nil       := chr(nil);
const int_nil       := int(nil);
const lng_nil       := lng(nil);
const dbl_nil       := dbl(nil);
const oid_nil       := oid(nil);
const str_nil       := str(nil);
const stream_nil    := Stream(nil);
const lock_nil      := lock(nil);
const sema_nil      := sema(nil);
const timestamp_nil := timestamp(nil);

PROC log2(any i) : int { return int(log(dbl(i))/M_LN2) + 1; }

# sort on [head,tail] and make globally unique 
PROC htordered_sunique(BAT[oid,any::1] b) : BAT[oid,any::1]
{
    b := b.sort(); # often a no-op
    # get kunique CTrefine numbers, and fetch the corresponding source positions 
    return b.fetch(reverse(reverse(CTrefine(b.hmark(0@0),b.tmark(0@0))).kunique()));
}

PROC addValues(bat[void,any::1] container, any::1 delta) : oid
{
    container.append(delta);
    return container.reverse().find(delta);
}

PROC addValues(bat[void,any::1] container, bat[oid,any::1] delta) : bat[oid,oid]
{
    container.append(delta);
    return delta.leftjoin(container.reverse());
}

PROC mposjoin( any pre, oid cont, BAT[oid, bat] ws_item ) : BAT[void,any] 
{
    return pre.leftfetchjoin(bat2constant(ws_item.find(cont)));
}

PROC mvaljoin( any pre, oid cont, BAT[oid, bat] ws_item ) : BAT[oid,oid] 
{
    return pre.leftjoin(ws_item.find(cont).reverse());
}

PROC tmark_unique( any::1 col, BAT[void,any] ipik) : BAT[oid,oid]
{
    return bat(void,oid).append(reverse(ipik).fetch(0)).seqbase(0@0).access(BAT_READ);
}

PROC tmark_unique( BAT[oid,any] col, BAT[oid,any] ipik) : BAT[oid,oid]
{
    return tmark(kunique(reverse(col)),0@0);
}

PROC tmark_grp_unique( any iter, BAT[oid,any] ipik) : BAT[oid,oid]
{
    return ipik.mark(1@0);
}

PROC tmark_grp_unique( BAT[any,any] iter, BAT[oid,any] ipik) : BAT[oid,oid]
{
    if (is_constant(iter)) return ipik.mark(1@0);
    return iter.mark_grp(iter.tunique(), 1@0);
}

PROC is_type (int kind, int type_) : bit
{
        return kind.and(63) = type_;
}

PROC get_type (bat[void,int] kind, int type_) : bat[oid,void]
{
        return kind.[and](63).ord_uselect(type_); # 63 = 2^6 - 1
}

PROC get_type_node (bat[void,int] kind) : bat[oid,void]
{
        return kind.ord_uselect(NODE, int_nil);
}

PROC get_type_atomic (bat[void,int] kind) : bat[oid,void]
{
        return kind.ord_uselect(int_nil, ATOMIC);
}

PROC get_container (int kind) : oid
{
        return kind.>>(6).oid();
}

PROC get_container (bat[oid,int] kind) : bat[oid,oid]
{
        return kind.[>>](6).[oid]();
}

PROC get_container (bat[void,int] kind) : bat[void,oid]
{
        return kind.[>>](6).[oid]();
}

PROC set_kind (oid cont, int type_) : int
{
        return cont.int().<<(6).or(type_);
}

PROC set_kind (bat[void,oid] cont, int type_) : bat[void,int]
{
        return cont.[int]().[<<](6).[or](type_);
}

PROC get_types (bat[void,int] kind) : bat[void,int]
{
        return kind.[and](63); # 63 = 2^6 - 1
}

PROC correct_sizes (oid iter, bat[void,oid] item, bat[void,int] size) : bat[void,int]
{
	return correct_sizes (constant2bat(iter), item, size);
}

@- get_root
We now have frag_root populated for all XML document collections (previously only
the transient document container). Finding the root of an item entails establishing
the largest root PRE that is smaller than that item. In the general case, we need
a thetajoin for that (root < item) with a per-item {max} aggregate.

The thetajoin can get quickly out of hand in MonetDB, so we actually do a 
chunked thetajoin A.K.A. blocked nested-loops (by hand, by taking slices).
@mil
PROC replace_root(BAT[void,oid] result, BAT[oid,oid] frag_item, BAT[oid,oid] frag_frag) : void
{
        # MonetDB bogus: avoid materialized cartesian product in thetajoin using blocked nested loop
        var delta := 100000 / count(frag_frag); # generate 100K intermediate tuples max
        var cur := 0, end := count(frag_item);
        while(cur < end) {
                var batch := frag_item.slice(cur, cur+delta);
                result.replace(leftthetajoin(batch, frag_frag, GE).{max}(), true);
                result.replace(batch.ord_select(0@0), true); # get_root() on collection super-root (0@0) returns self
                cur :+= delta + 1;
        }
}

PROC get_root(BAT[oid,bat] ws, oid item, int kind, oid cont) : oid
{
        if (item = 0@0) return 0@0; # get_root() on collection super-root (0@0) returns self
        var nid_rid   := ws.fetch(NID_RID).fetch(cont);
        var map_pid   := ws.fetch(MAP_PID).fetch(cont);
        var frag_root := ws.fetch(OPEN_DOCID).leftjoin(ws.fetch(FRAG_ROOT).fetch(cont));
            frag_root := frag_root.leftfetchjoin(nid_rid).[swizzle](map_pid).tsort().tmark(0@0);

        if (kind.is_type(ATTR)) { # convert attributes to a pre
                item := ws.fetch(ATTR_OWN).fetch(cont).find(item);
                item := nid_rid.find(item).swizzle(map_pid);
        }
        item := max(frag_root.select(oid_nil,item));
        return item;
}

PROC get_root(BAT[oid,bat] ws, BAT[void,oid] item, int kind, oid cont) : BAT[void,oid]
{
        if ((count(item) = 1)) {
        	# short-cut to avoid some extra work...
		var root := bat(void,oid).seqbase(seqbase(item));
		root.append(get_root(ws, item.fetch(0), kind, cont));
                return root.access(BAT_READ);
        }

        var seqbase := item.seqbase();
        var nid_rid   := ws.fetch(NID_RID).fetch(cont);
        var map_pid   := ws.fetch(MAP_PID).fetch(cont);
        var frag_root := ws.fetch(OPEN_DOCID).leftjoin(ws.fetch(FRAG_ROOT).fetch(cont));
            frag_root := frag_root.leftfetchjoin(nid_rid).[swizzle](map_pid).tsort().tmark(0@0);

        if (kind.is_type(ATTR)) { # convert attributes to a pre
                item := item.leftfetchjoin(ws.fetch(ATTR_OWN).fetch(cont));
                item := item.leftfetchjoin(nid_rid).[swizzle](map_pid);
                item := item.tmark(seqbase);
        }

        var result := item.copy().access(BAT_WRITE);
        replace_root(result, item, mirror(reverse(frag_root)));
        return result.access(BAT_READ).tmark(seqbase);
}

PROC get_root(BAT[oid,bat] ws, BAT[void,oid] item, BAT[void,int] kind, BAT[void,oid] cont) : BAT[void,oid] 
{
        if ((count(kind) = 1) and (count(cont) = 1)) {
        	# short-cut avoid some extra work...
                return get_root(ws, item, kind.fetch(0), cont.fetch(0));
        }

        # look up root fragment
        var seqbase := item.seqbase();
        var result := item.copy().access(BAT_WRITE);
        var attr_cont := kind.get_type(ATTR).mirror().leftfetchjoin(cont);
        var conts := cont.tunique();
        conts@batloop() {
                var nid_rid   := ws.fetch(NID_RID).fetch($h);
                var map_pid   := ws.fetch(MAP_PID).fetch($h);

                var frag_root := ws.fetch(OPEN_DOCID).leftjoin(ws.fetch(FRAG_ROOT).fetch($h));
                    frag_root := frag_root.leftfetchjoin(nid_rid).[swizzle](map_pid).tsort().tmark(0@0);
                var frag_item := cont.ord_uselect($h).mirror().leftfetchjoin(result);
                var attr_item := attr_cont.ord_uselect($h);
                if (count(attr_item) > 0) {
                        attr_item := attr_item.mirror().leftfetchjoin(item);
                        attr_item := attr_item.leftfetchjoin(ws.fetch(ATTR_OWN).fetch($h));
                        attr_item := attr_item.leftfetchjoin(nid_rid).[swizzle](map_pid);
                        result.replace(attr_item, true);
                }
                var frag_item := cont.ord_uselect($h).mirror().leftfetchjoin(result);
                replace_root(result, frag_item, mirror(reverse(frag_root)));
                var frag_item := cont.ord_uselect($h).mirror().leftfetchjoin(result);
        }
        return result.access(BAT_READ).tmark(seqbase);
}

PROC impl_attr_own(BAT[oid,bat] ws, any nids, any cont) : BAT[oid,oid]
{
    var private := mvaljoin(nids, cont, ws.fetch(ATTR_OWN_PRIVATE));
    var shared := mvaljoin(nids, cont, ws.fetch(ATTR_OWN_SHARED));

    var cnt_p := count(int(private));
    var cnt_s := count(int(shared));
    if (cnt_s = 0LL) return private; # private is all we got.

    var hshared := shared.hmark(0@0);
    var tshared := shared.tmark(0@0);
    shared := shared.fetch(mposjoin(tshared, hshared.leftfetchjoin(cont), ws.fetch(ATTR_OWN)).ord_uselect(oid_nil,oid_nil));
    cnt_s := count(int(shared));

    if (cnt_s = 0LL) return private; # private is all we got.
    if (cnt_p = 0LL) return shared; # shared is all we got.

    # merge-union shared and private
    var res := merged_union(shared.hmark(0@0), private.hmark(0@0), shared.tmark(0@0), private.tmark(0@0));
    return reverse(res.fetch(0)).leftfetchjoin(res.fetch(1));
}

PROC get_attr_own(BAT[oid,bat] ws, oid nid, any cont) : BAT[oid,oid]
{
    return impl_attr_own(ws, nid, cont);
}

PROC get_attr_own(BAT[oid,bat] ws, BAT[oid,oid] nids, any cont) : BAT[oid,oid]
{
    return nids.mark(0@0).leftjoin(impl_attr_own(ws, nids.tmark(0@0), cont));
}

PROC fn_put(BAT[oid,bat] ws, BAT[void,str] uri, BAT[void,oid] item, any kind,
            BAT[void,lng] int_values, BAT[void,dbl] dbl_values,
            BAT[void,dbl] dec_values, BAT[void,str] str_values) : void
{
    var protocol := [search](uri, "://");
    var sel := protocol.uselect(0,int_nil).hmark(0@0).leftfetchjoin(uri);
    var isfile := [or]([startsWith](sel, "file://"), [startsWith](sel, "FILE://"));
    var filepath := protocol.uselect(-1).hmark(0@0).leftfetchjoin(uri).access(BAT_WRITE);

    isfile := isfile.uselect(true).hmark(0@0).leftfetchjoin(sel);
    sel := isfile.copy().access(BAT_WRITE).append(filepath).access(BAT_READ);
    filepath.append([string](isfile,7));

    if (count(sel) != count(uri)) { # we only support file url's for the moment
       var nonfileurl := tdiff(uri, sel);
       ERROR("fn_put: URI '%s' not recognized (only file URLs are supported; %d such errors).", wrong.fetch(0), count(nonfileurl));
    }
    var absolute := [or](filepath.[startsWith]("/"), filepath.[startsWith]("\\")).uselect(true).mirror().leftfetchjoin(sel);
    if (count(absolute) > 0) {
       ERROR("fn_put: file URI '%s' must be a relative path (%d such errors).", absolute.fetch(0), count(absolute));
    }
    var backwards := filepath.[search]("..").uselect(0,int_nil).mirror().leftfetchjoin(sel);
    if (count(backwards) > 0) {
       ERROR("fn_put: file URI '%s' must is not allowed to contain '..' (%d such errors).", backwards.fetch(0), count(backwards));
    }
    [print_result](sel, "xml-noroot", 
                   const ws, item, kind, 
                   const int_values, 
                   const dbl_values, 
                   const dec_values, 
                   const str_values);
}

# print whitespaces (used for indenting)
PROC pws (int shift) : void
{
    while (shift > 0) {
        printf("  ");
        shift :-= 1;
    }
}

# main trace routine (returns an XML document representing the trace)
#
# The generated XML document should be aligned to the following DTD
# <!DOCTYPE scope [
# 
# <!ELEMENT scope (iteration*)>
# <!ELEMENT iteration (scope*, trace*)>
# <!ELEMENT trace (item*)>
# <!ELEMENT item ANY>
# 
# <!ATTLIST scope id CDATA #REQUIRED>
# 
# <!ATTLIST iteration no CDATA #REQUIRED>
# 
# <!ATTLIST trace msg CDATA #REQUIRED>
# <!ATTLIST trace id CDATA #REQUIRED>
# 
# <!ATTLIST item type (integer|string|decimal|
#                      double|boolean|qname|
#                      node|attribute) #REQUIRED>
# <!ATTLIST item pos CDATA #REQUIRED>
# <!ATTLIST item id CDATA #IMPLIED>
# <!ATTLIST item fragment CDATA #IMPLIED>
# <!ATTLIST item name CDATA #IMPLIED>
# <!ATTLIST item value CDATA #IMPLIED>
# 
# ]>
PROC trace (BAT[void,any] ws,
            BAT[oid,bat] trace_outer,
            BAT[oid,bat] trace_inner,
            BAT[oid,bat] trace_iter,
            BAT[oid,bat] trace_msg,
            BAT[oid,bat] trace_item,
            BAT[oid,int] trace_type,
            BAT[oid,oid] trace_rel) : void
{
    # shift/indent counter
    var s := 0;
    # initial trace id
    var init_id := 0@0;
    # initial outer relation
    var init_outer_val := 1@0;
    
    pws (s); printf("<scope id=\"%i\">\n", init_id); s :+= 1;
    pws (s); printf("<iteration no=\"%i\">\n", init_outer_val);  s :+= 1;

    # first print all trace information in nested scopes ...
    trace_scopes (ws,
                  trace_outer,
                  trace_inner,
                  trace_iter,
                  trace_msg,
                  trace_item,
                  trace_type,
                  trace_rel,
                  s,
                  init_id,
                  init_outer_val);
    # ... and then print the trace information for this scope.
    trace_ops (ws,
               trace_iter,
               trace_msg,
               trace_item,
               trace_type,
               trace_rel,
               s,
               init_id,
               init_outer_val);
           
    s :-= 1; pws (s); printf("</iteration>\n");
    s :-= 1; pws (s); printf("</scope>\n");
}

# worker for PROC trace. It recursively prints the structure of
# the scopes and initiates the printing of trace information (trace_ops).
PROC trace_scopes (BAT[void,any] ws,
                   BAT[oid,bat] trace_outer,
                   BAT[oid,bat] trace_inner,
                   BAT[oid,bat] trace_iter,
                   BAT[oid,bat] trace_msg,
                   BAT[oid,bat] trace_item,
                   BAT[oid,int] trace_type,
                   BAT[oid,oid] trace_rel,
                   int s, oid id, oid cur_outer) : void
{
    var rels := trace_rel.reverse().uselect(id).mirror();
    var outer_rels := trace_outer.reverse().leftjoin(rels).reverse();
    var inner_rels := trace_inner.reverse().leftjoin(rels).reverse();

    var index1 := 0;
    outer_rels@batloop () {
        pws (s); printf("<scope id=\"%i\">\n", $h); s :+= 1;

        var next_id := $h;
        var inner_rel := inner_rels.fetch(index1);
        var outer := $t.select(cur_outer);
        var inner := outer.mirror().leftjoin(inner_rel);
        
        # iterate over the tuples of the outer|inner relation
        inner@batloop () {
            pws (s); printf("<iteration no=\"%i\">\n", $t);  s :+= 1;
            
            # first print all trace information in nested scopes ...
            trace_scopes (ws,
                          trace_outer,
                          trace_inner,
                          trace_iter,
                          trace_msg,
                          trace_item,
                          trace_type,
                          trace_rel,
                          s,
                          next_id,
                          $t);
            # ... and then print the trace information for this scope.
            trace_ops (ws,
                       trace_iter,
                       trace_msg,
                       trace_item,
                       trace_type,
                       trace_rel,
                       s,
                       next_id,
                       $t);
                   
            s :-= 1; pws (s); printf("</iteration>\n");
        }
        
        s :-= 1; pws (s); printf("</scope>\n");
        index1 :+= 1;
    }
}

# worker for PROC trace. It prints all trace operators. A worker
# (trace_item) prints the item sequences that correspond to the trace
# operators and the current iteration.
PROC trace_ops (BAT[void,any] ws,
                BAT[oid,bat] trace_iter,
                BAT[oid,bat] trace_msg,
                BAT[oid,bat] trace_item,
                BAT[oid,int] trace_type,
                BAT[oid,oid] trace_rel,
                int s, oid id, oid cur_inner) : void
{
    var rels := trace_rel.reverse().uselect(id).mirror();
    var msg_rels := trace_msg.reverse().leftjoin(rels).reverse();

    # iterate over the available trace operators ...
    msg_rels@batloop () {
        # ... and pick the current iteration
        var msg_row := $t.reverse().select(cur_inner).reverse();
        if (msg_row.count() != 0) {
            pws (s); printf("<trace msg=\"%s\" id=\"%i\">\n",
                            msg_row.fetch(0),
                            $h); s :+= 1;
            
            var iter := trace_iter.reverse().select($h).reverse().fetch(0);
            if (iter.seqbase() != 0@0) {
                printf ("\n");
                ERROR ("Trace printing failed: wrong seqbase in trace_iter");
            }
            # Also make sure that we only use values from the current iteration
            var iter_sel := iter.select(cur_inner);
            var iter_count := iter_sel.count();
            if (iter_count != 0) {
                var item_set := trace_item.reverse().select($h).reverse();
                var types := trace_type.reverse().select($h).reverse();
                var iter_offset := iter_sel.reverse().min().int();
                # generate an item sequence
                trace_items (ws, item_set, types, iter_offset, iter_count);
            }

            s :-= 1; pws (s); printf("</trace>\n");
        }
    }
}

# worker for PROC trace_ops. It prints an item sequence starting
# from ``offset'' until ``offset + count''. For nodes, attributes,
# and qnames special printing is performed.
PROC trace_items (BAT[void,any] ws,
                  BAT[oid,bat] item_set,
                  BAT[oid,int] types,
                  int offset,
                  int count) : void
{
    var item_set_count := item_set.count();
    var i := 0;
    # iterate over the tuples that are in the current iteration
    while (i < count) {
        pws (s); printf("<item pos=\"%i\"", i + 1);

        # print item value
        var j := 0;
        var node_id := oid(nil);
        var frag := oid(nil);
        var kind;
        # iterate over the list of item relations
        # (to cope with polymorphic sequences)
        while (j < item_set_count) {
            var type_id := types.fetch(j);
            var item := item_set.fetch(j).fetch(i + offset);
            var type_str;
            # the types are aligned with the aat_* types in
            # struct PFalg_simple_type_t (compiler/include/algebra.h)
            if (type_id = 2) {
                type_str := "integer";
            } else if (type_id = 4) {
                type_str := "string";
            } else if (type_id = 8) {
                type_str := "decimal";
            } else if (type_id = 16) {
                type_str := "double";
            } else if (type_id = 32) {
                type_str := "boolean";
            } else if (type_id = 64) {
                type_str := "QName";
            } else if (type_id = 128) {
                type_str := "untypedAtomic";
            } else if (type_id = 256) {
                type_str := "node";
                frag := item;
            } else if (type_id = 512) {
                type_str := "node";
                node_id := item;
                kind := ELEM;
            } else if (type_id = 1024) {
                type_str := "attribute";
                frag := item;
            } else if (type_id = 2048) {
                type_str := "attribute";
                node_id := item;
                kind := ATTR;
            } else {
                printf("\n");
                ERROR ("unknown type (%i) appears in "
                       + "trace output generation",
                       type_id);
            }
            # we have found the correct type -- so generate output for it
            if (not(isnil (item))) {
                # qnames
                if (type_id = 64) {
                    printf (" type=\"%s\"", type_str);
                    var name := trace_qname_uri (item, WS);
                    printf (">%s", name);
                    break;
                # all primitive types
                } else if (and (isnil (node_id), isnil (frag))) {
                    printf (" type=\"%s\">", type_str);
                    printf ("%s", item.str());
                    break;
                # nodes
                } else if (and (not (isnil (node_id)),
                                not (isnil (frag)))) {
                    printf (" type=\"%s\"", type_str);
                    printf (" id=\"%i\"", node_id);
                    printf (" fragment=\"%i\"", frag);
                    if (kind = ELEM) {
                        # element, text, comment, pi, and doc nodes
                        printf (">");
                        print_result ("xml-noroot-noheader",
                                      ws,
                                      new (void,oid).seqbase(0@0)
                                                    .append(node_id),
                                      new (void,oid).seqbase(0@0)
                                                    .append(frag)
                                                    .set_kind(kind),
                                      new (void,lng).seqbase(0@0),
                                      new (void,dbl).seqbase(0@0),
                                      new (void,dbl).seqbase(0@0),
                                      new (void,str).seqbase(0@0));
                    } else {
                        # attribute nodes
                        var qn_id := mposjoin (node_id, frag, ws.fetch(ATTR_QN)).fetch(0);
                        var qn_frag := mposjoin (node_id, frag, ws.fetch(ATTR_CONT)).fetch(0);
                        var prop := mposjoin (node_id, frag, ws.fetch(ATTR_PROP)).fetch(0);
                        var val := mposjoin (prop, qn_frag, ws.fetch(PROP_VAL)).fetch(0);
                        var name := trace_qname_uri (qn_id, qn_frag);
                        printf (" name=\"%s\" value=\"%s\">", name, val);
                    }
                    break;
                }
            }
            j :+= 1;
        }
            
        printf("</item>\n");
        i :+= 1;
    }
}

# worker for PROC trace_ops. It looks up the qname values
# and prints a namespace attribute (as side effect) if a prefix
# is used. The return value is the string representation of the
# qname.
PROC trace_qname_uri (oid item, oid frag) : str
{
    var local := mposjoin (item, frag, ws.fetch(QN_LOC)).fetch(0);
    var prefix := mposjoin (item, frag, ws.fetch(QN_PREFIX)).fetch(0);
    var uri := mposjoin (item, frag, ws.fetch(QN_URI)).fetch(0);
    if (and (prefix = "", uri = "")) {
        return local;
    } else if (prefix = "") {
        printf (" xmlns:prefix=\"%s\"",
                uri);
        return "prefix:" + local;
    } else {
        printf (" xmlns:%s=\"%s\"",
                prefix, uri);
        return prefix + ":" + local;
    }
    break;
}


@- order-preserving xquery join with existential semantics

The below proc handles xquery theta-joins between l[oid,any::1] and r[oid,any::1]
(iter,value) combinations with a join predicate PRED = { LT, LE, EQ, GE, GT }.

The result are the [oid,oid] iter numbers that should be ordered on head, and within 
head on tail (hence the term 'htordered') to preserve proper xquery sequence order.

We may also get optional lx[void,any] and rx[void,any] bats that substitute the
head and tail result oid-s for something else (backmapping). These lx/rx are not always 
present though.  They are pushed inside the thetajoin when sampling indicates the result 
will become big, or when the any types are smaller than oid-s (thus always make the 
result smaller).  In this case it is better to go into the join with the substitution 
already performed.  In other cases, the  substitution is done after computing the join.

The join is *existential*, that is, each [iter_l,iter_r] is unique in the result,
and should be there iff there exists any [iter_l,v_l] and [iter_r,v_r] for which 
(v_l PRED v_r) holds. 

For equi-joins, we compute the full join and then eliminate
duplicate [oid,oid] combinations. Given the htordered result characteristic, this will 
use a merge-algorithm. However for <,>,<=,>= we even *avoid* duplicates by joining {min}(l) 
with {max}(r) for <,<= (and vice versa for >,>=). Note  that these {min}() and {max}() 
are also efficient as they also can use a merge-algorithm for aggregation.

For non-equi-joins, use use either sort-merge or nested-loop join. Note that the latter join 
does not require any reordering to achieve htordered-ness. This is a plus, but nested-loop is 
of course quadratic in complexity. On the other hand, non-equi-joins tend to be quadratic 
in their result size anyway. We check this using run-time sampling. Nested-loop is chosen if 
sampling indicates a large join result.

Finally, the theta-join is loop-lifted, in that it may be executed between two loop-lifted
expressions. In that case, only matching outer iterations (lo and ro) should be joined on value. 
In effect, it requires a join condition "(l_val PRED r_val) and (lo = ro)". As MonetDB cannot 
handle such multi-column theta-joins efficiently, the current solution exploits the fact
that the loop-lifted iterations appear contiguous in both inputs. Thus, each iteration
is a slice of both input BATs. We just go through all outer iterations, take slices, and
execute the join for corresponding iter slices on l and r; concatenating all results.
@mil
var pf_sample256 := bat(void,lng,256LL).rename("pf_sample256").seqbase(0@0); 
{ var i := 0LL; while(i < 256LL) { pf_sample256.append(i); i :+= 1LL; } } 

PROC pf_ttype(bat[any,any] b) : int {
    var t := b.ttype();
    return (t = void).ifthenelse(oid,t);
}


PROC htordered_unique_thetajoin( int mode,
                                 bat[oid,any::1] l, bat[oid,any::1] r,
                                 any lx, any rx) : bat[any,any]
{
    var lcnt := int(l).count();
    var rcnt := int(r).count();
    if ((lcnt = 0LL) or (rcnt = 0LL))
        return bat(oid,oid,1LL).access(BAT_READ);

    if (not(ordered(l)) or not(ordered(r)))
        ERROR("htordered_unique_thetajoin(): ordered left and right head columns (iters) expected.\n");

    if (rcnt = 1LL) {
        # it is a selection; not a join
        var v := r.fetch(0);
        var t := r.reverse().fetch(0);
        var lo := (mode >= EQ).ifthenelse(v,cast(nil,pf_ttype(l)));
        var hi := (mode <= EQ).ifthenelse(v,cast(nil,pf_ttype(l)));
        var sel;
        if ((mode = GT) or (mode = LT)) {
            sel := l.ord_select(lo,hi).[!=](v).ord_uselect(true).kunique();
        } else {
            sel := l.ord_uselect(lo,hi).kunique();
        }
        if (type(lx) = bat) sel := reverse(reverse(sel).leftfetchjoin(lx));
        if (type(rx) = bat) t := rx.find(t);
        return sel.project(t);
    }

    if (lcnt = 1LL) {
        return reverse(htordered_unique_thetajoin(-(mode), r, l, rx, lx));
    }
    if (mode != EQ) {
        # try to reduce footprint, first:
        # join with lx/rx only if smaller than l/r
        if (type(lx) = bat) {
            if (htype(lx) <= sht) { l := reverse(leftfetchjoin(reverse(l),lx)); lx := nil; }
        }
        if (type(rx) = bat) {
            if (htype(rx) <= sht) { r := reverse(leftfetchjoin(reverse(r),rx)); rx := nil; }
        }
        # trick: as we eliminate double matches anyway, let's not generate them to start with
        # pumps are efficient because merge-based
        if ((mode = GT) or (mode = GE)) {
            l := {max}(l);
            r := {min}(r);
        } else {
            l := {min}(l);
            r := {max}(r);
        }
        var samp := [oid]([*](pf_sample256, int(l).count()/256LL)).leftfetchjoin(l.reverse().mark(0@0).reverse());
        var res := nlthetajoin(samp, reverse(r), mode, int(r).count() * 64LL);
        if (((2LL * batsize(res) * int(l).count()) / (1LL + int(samp).count())) > mem_maxsize()) {
            var cnt := int(res).count(); res := nil;
            # if not done, yet, join with lx/rx
            if (type(lx) = bat) l := reverse(leftfetchjoin(reverse(l),lx));
            if (type(rx) = bat) r := reverse(leftfetchjoin(reverse(r),rx));
            # a large intermediate result is better handled with nested loop (no reordering necessary)
            return nlthetajoin(l, reverse(r), mode, ((cnt+cnt) / int(samp).count())*int(r).count());
        }
    }
    var join_order := leftthetajoin(l, reverse(r), mode);

    # avoid error (lng(max(bat(oid,oid))) = nil)
    if (join_order.count() = 0) { return join_order; }

    var snd_iter := join_order.reverse().mark(0@0).reverse();
    var fst_iter := join_order.mark(0@0).reverse();
    var sorting := fst_iter.CTrefine(snd_iter); # this may hurt
    if (lng(max(sorting)) != int(sorting).count()) {
        # the output of CTrefine allows to easily check if it is kunique
        sorting := sorting.reverse().kunique().reverse(); # merge-based kunique
    }
    join_order := join_order.fetch(sorting); # this may hurt as well
    # joins with lx/rx can only be done after the CTrefine() and kunique()
    if (type(lx) = bat) join_order := reverse(leftfetchjoin(reverse(join_order),lx));
    if (type(rx) = bat) join_order := leftfetchjoin(join_order,rx);
    return join_order;
}

var lng_oid := ifthenelse(isnil(oid(LNG_MAX)),lng,oid);

PROC join_test(int mode, bat[void,any::1] l, bat[void,any::2] r) : bat[void,bit] {
    if (mode = GT) return [>](l,r);
    if (mode = LT) return [<](l,r);
    if (mode = GE) return [>=](l,r);
    if (mode = LE) return [<=](l,r);
    return [=](l,r);
}

# loop-lifted variant; lo and ro are the outer iteration numbers that should match
PROC ll_htordered_unique_thetajoin( int mode,
                                    bat[oid,any::2] l, bat[oid,any::2] r,
                                    bat[void,oid] lo, bat[void,oid] ro,
                                    any lx, any rx) : bat[any,any]
{
    if (not(reverse(lo).ordered()) or not(reverse(ro).ordered()))
        ERROR("htordered_unique_thetajoin(): ordered left and right outer columns (iters) expected.\n");

    var lo_histo := histogram(lo), lr_histo := reverse(lo_histo);
    var ro_histo := histogram(ro), rr_histo := reverse(ro_histo);
    var li := 0LL, lp := 0LL, lc := count(int(lo_histo));
    var ri := 0LL, rp := 0LL, rc := count(int(ro_histo));
    var b := new(void,bat,min(lc,rc));
    var tpe := pf_ttype(l);

    # trivial case; not handled below as log2() cannot cope with max() of an empty BAT being NIL
    if ((lc = 0LL) or (rc = 0LL)) {
        return bat(oid, oid, 0).access(BAT_READ);
    }

    # constant outer columns can be handles by the basic thetajoin
    if ((lc = 1LL) and (rc = 1LL) and (lo.fetch(0) = ro.fetch(0))) {
        return htordered_unique_thetajoin(mode, l, r, lx, rx);
    }

    if (mode = EQ) {
        # equi-joins on constant value columns can also be handled by the basic thetajoin
        if (l.sample(100).tunique().count() = 1) if (l.tunique().count() = 1) {
            r := r.mark(0@0).fetch(r.tmark(0@0).ord_uselect(l.fetch(0))).leftfetchjoin(ro);
            l := l.mark(0@0).leftfetchjoin(lo);
            return htordered_unique_thetajoin(EQ, l, r, lx, rx);
        }
        if (r.sample(100).tunique().count() = 1) if (r.tunique().count() = 1) {
            l := l.mark(0@0).fetch(l.tmark(0@0).ord_uselect(r.fetch(0))).leftfetchjoin(lo);
            r := r.mark(0@0).leftfetchjoin(ro);
            return htordered_unique_thetajoin(EQ, l, r, lx, rx);
        }

        # in case of integer equi-join, we shift iter and value together in a lng and do a single join on that
        if (or(or((tpe = int), (tpe = oid)), (tpe = lng)))  {
            var iter_max := 32;
            var combine := true;
            if ((tpe = lng) or (lng_oid = lng)) {
                iter_max := log2(lng(max(max(lr_histo),max(rr_histo))));
                combine := (iter_max + log2(lng(max(max(l),max(r))))) < 64;
            }
            if (combine) {
                lo := [lng](lo).access(BAT_WRITE); 
                ro := [lng](ro).access(BAT_WRITE); 
	        [:+=](lo, [<<]([lng](l.tmark(0@0)), iter_max)).access(BAT_READ);
	        [:+=](ro, [<<]([lng](r.tmark(0@0)), iter_max)).access(BAT_READ);
	        l := l.mark(0@0).leftfetchjoin(lo); 
	        r := r.mark(0@0).leftfetchjoin(ro); 
                return htordered_unique_thetajoin(EQ, l, r, lx, rx);
            }
        }
    }

    # in some cases, the (left_iter = right_iter) restriction is more selective than (left_val OP right_val)
    if (sum([*](lo_histo,ro_histo)) > 6*min(count(ro),count(lo))) {
        var join_res := lo.leftmergejoin(reverse(ro)); # get cartesian product within each matching iter

        # select only matching tuples according to (left_val OP right_val)
        var l_val := join_res.hmark(0@0).leftfetchjoin(l.tmark(0@0));
        var r_val := join_res.tmark(0@0).leftfetchjoin(r.tmark(0@0));
        join_res := join_res.fetch(join_test(mode,l_val,r_val).ord_uselect(true));

        # map back to the iter values of lo and ro [iter,iter]
        join_res := join_res.leftfetchjoin(r.hmark(0@0));
        join_res := reverse(reverse(join_res).leftfetchjoin(l.hmark(0@0)));
        
        # sort (should already be the case) and unique
        join_res := htordered_sunique(join_res);

        # do not forget to apply any remaps 
        if (type(lx) = bat) join_res := reverse(leftfetchjoin(reverse(join_res),lx));
        if (type(rx) = bat) join_res := leftfetchjoin(join_res,rx);
        return join_res;
    }

    # otherwise iterate over the outer scopes; execute the join only for a single outer iter at a time
    while((lp < lc) and (rp < rc)) {
        var lv := lr_histo.fetch(lp);
        var rv := rr_histo.fetch(rp);
        var ln := lng(lo_histo.fetch(lp));
        var rn := lng(ro_histo.fetch(rp));
        if (lv = rv) {
            # only join corresponding slices of l an r
            var bn := htordered_unique_thetajoin(mode, l.slice(li,li+ln-1), r.slice(ri,ri+rn-1), lx, rx);
            if (count(int(bn)) > 0LL) b.append(bn);
        }
        if (lv <= rv) { lp :+= 1; li :+= ln; }
        if (rv <= lv) { rp :+= 1; ri :+= rn; }
    }

    # concatenate all results
    if (count(int(b)) = 1LL) return b.fetch(0);
    var bn := bat(oid, oid, sum_lng([count]([int](b)))).access(BAT_WRITE);
    if (count(int(b)) > 0LL) [insert](const bn,b);
    return bn.access(BAT_READ);
}

# procedure that squeezes two node sequences # (encoded in 3 bats: pre,
# frag, and attr) into two bats of type lng by using bit-shifting.
# The procedure needs both node sequences to align the bit-shifting
# and thus to guarantee that the lng values can be compared instead
# of the node representation.
PROC combine_node_info (any frag1, bat[void,oid] pre1, any attr1,
                        any frag2, bat[void,oid] pre2, any attr2) : bat[void,bat]
{
    var flen, plen, alen, maxlen, tmp1, tmp2;
    var res := bat (void, bat, 2).seqbase(0@0);

    # nothing to do if we have no nodes
    if (pre1.count() = 0) return res.append(pre1).append(pre1);
    if (pre2.count() = 0) return res.append(pre2).append(pre2);

    tmp1 := lng(max(pre1)); 
    tmp2 := lng(max(pre2));
    if (tmp1 > tmp2) {
        plen := tmp1;
    }
    else {
        plen := tmp2;
    }
    if (plen = 0LL) {
        plen := 0;
    }
    else {
        plen := log2 (plen);
    }
    
    if (type(frag1) = bat) {
        if ((frag1.count() != pre1.count()) or 
            (frag1.seqbase() != pre1.seqbase()))
            ERROR("combine_node_info(): pre and frag (of the first input) are not aligned.\n");
        tmp1 := lng(max(frag1));
        if (tmp1 = 0LL) {
            flen := 0;
        }
        else {
            flen := log2 (tmp1);
        }
    } else {
        if (type(frag1) = oid) {
            tmp1 := max(lng(frag1));
            if (tmp1 = 0LL) {
                flen := 0;
            }
            else {
                flen := log2 (tmp1);
            }
        } else {
            ERROR("combine_node_info(): frag value(s) expected.\n");
        }
    }
    
    if (type(attr1) = bat) {
        if ((pre1.count() != attr1.count()) or
            (pre1.seqbase() != attr1.seqbase()))
            ERROR("combine_node_info(): pre and attr (of the first input) are not aligned.\n");
        tmp1 := lng(max(attr1));
        if (tmp1 = 0LL) {
            alen := 0;
        }
        else {
            alen := log2 (tmp1);
        }
    } else {
        if (type(attr1) = oid) {
            tmp1 := max(lng(attr1));
            if (tmp1 = 0LL) {
                alen := 0;
            }
            else {
                alen := log2 (tmp1);
            }
        } else {
            alen := 0;
        }
    }
    
    if (type(frag2) = bat) {
        if ((frag2.count() != pre2.count()) or
             (frag2.seqbase() != pre2.seqbase()))
            ERROR("combine_node_info(): pre and frag (of the second input) are not aligned.\n");
        tmp1 := lng(max(frag2));
        if (tmp1 = 0LL) {
            tmp2 := 0;
        }
        else {
            tmp2 := log2 (tmp1);
        }
        flen := max (flen, tmp2);
    } else {
        if (type(frag2) = oid) {
            tmp1 := max(lng(frag2));
            if (tmp1 = 0LL) {
                tmp2 := 0;
            }
            else {
                tmp2 := log2 (tmp1);
            }
            flen := max (flen, tmp2);
        } else {
            ERROR("combine_node_info(): frag value(s) expected.\n");
        }
    }
    
    if (type(attr2) = bat) {
        if ((pre2.count() != attr2.count()) or
            (pre2.seqbase() != attr2.seqbase()))
            ERROR("combine_node_info(): pre and attr (of the second input) are not aligned.\n");
        tmp1 := lng(max(attr2));
        if (tmp1 = 0LL) {
            tmp2 := 0;
        }
        else {
            tmp2 := log2 (tmp1);
        }
        alen := max (alen, tmp2);
    } else {
        if (type(attr2) = oid) {
            tmp1 := max(lng(attr2));
            if (tmp1 = 0LL) {
                tmp2 := 0;
            }
            else {
                tmp2 := log2 (tmp1);
            }
            alen := max (alen, tmp2);
        } else {
            alen := max (alen, 0);
        }
    }
    
    maxlen := flen + plen + alen;
    if (maxlen > 64)
        ERROR("combine_node_info(): node compression failed.\n");
    
    var res1 := [<<]([lng](pre1), alen).access(BAT_WRITE);
    var res2 := [<<]([lng](pre2), alen).access(BAT_WRITE);
    if (alen > 0) {
        if (type(attr1) = bat) {
            [:+=](res1, [lng](attr1.tmark(0@0)));
        } else {
            [:+=](res1, const lng(attr1));
        }
        if (type(attr2) = bat) {
            [:+=](res2, [lng](attr2.tmark(0@0)));
        } else {
            [:+=](res2, const lng(attr2));
        }
    }
    if (type(frag1) = bat) {
        [:+=](res1, [<<]([lng](frag1.tmark(0@0)), alen + plen));
    } else {
        [:+=](res1, const << (lng(frag1), alen + plen));
    }
    if (type(frag2) = bat) {
        [:+=](res2, [<<]([lng](frag2.tmark(0@0)), alen + plen));
    } else {
        [:+=](res2, const << (lng(frag2), alen + plen));
    }
    return res.append (res1.access(BAT_READ))
              .append (res2.access(BAT_READ));
}

@- loop-lifted staircase join
@mil
#############################################
# MIL WRAPPER for AXIS STEPS 
#
# In order to simplify the invocation of the axis steps functions, this
# interface provides...
# 

@:wrap(descendant)@
@:wrap(descendant_or_self)@
@:wrap(child)@

@:upwards(parent)@
@:upwards(ancestor)@
@:upwards(ancestor_or_self)@

@:ll_sibling_impl_following@
@:ll_sibling_impl_preceding@

@:ll_prec_foll_impl(following)@
@:ll_prec_foll_impl(preceding)@

@= chk_order
	if ( and(order,1) = @2 ) {
		@3 := @3.chk_order(); # just in case...
	}
@= one_iter_many_items
	# 1 iter, n items
	one_iter := TRUE;
	@:chk_order(@1@2,0,item)@
	# =>  we do not need to sort the input
	order := or(order,1);
@= many_iters_one_item
	# n iters, 1 item 
	one_item := TRUE;
	@:chk_order(@1@2,1,iter)@
	# =>  we do not need to sort the input
	order := or(order,1);
@= unique_iters
	if (end_ctx.count() = 0) {
		# unique iters optimization;
		# must NOT be done in case we have end_ctx nodes, i.e., in
		# case we "mis-use" a limited child-step to evaluate
		# preceding-sibling as child(parent(ctx))!
                var unq_items;
                unq_items := item.reverse().kunique();
                if ((item.count() - unq_items.count()) > 1000) {
                        unq_iter := TRUE;
                        unq_iters := iter.reverse()
                                         .leftfetchjoin(item)
                                         .leftjoin(unq_items.mark(0@0));
                        item := unq_items.mark(0@0).reverse();
                        iter := item.mirror();
                        # we now certainly have iter|item ordering
                        order := and(order,2);
                }
	}
@= resolve_unique_iters
                # undo memoization and remap iter values
                if (unq_iter) {
                    # we assume descendant and child steps ALWAYS
                    # return their result in iter|item order
                    #
                    # Otherwise we need to correct the ordering if not already present
                    # if (not (ordered (res))) {
                    #     var ord := res.mark(0@0).sort().reverse();
                    #         ord := ord.CTrefine(res.reverse().mark(0@0).reverse())
                    #                   .mark(0@0).reverse();
                    #     iter := ord.leftfetchjoin(res.mark(0@0).reverse());
                    #     item := ord.leftfetchjoin(res.reverse().mark(0@0).reverse());
                    #     res := iter.reverse().leftfetchjoin(item);
                    # }
                    res := unq_iters.lefthashjoin(res.access(BAT_WRITE).revert());
                    res.chk_order();
                }
@= wrap
PROC ll_@1(BAT[oid,oid] iter, BAT[oid,oid] ctx, BAT[oid,int] pre_size, BAT[void,any] cands, bit one_iter, bit one_ctx, oid min_iter, oid max_iter, bit no_iter_order, chr kind_test) : BAT[oid,oid]
{
        var end_ctx := bat(void,oid,0).seqbase(0@0).access(BAT_READ);
	return ll_@1(iter, ctx, end_ctx, pre_size, cands, one_iter, one_ctx, min_iter, max_iter, no_iter_order, kind_test);
}
ADDHELP("ll_@1", "manegold", "Aug 2007",
"PARAMETERS:\n\
BAT[void,oid] iter (grouping relation; sorted on tail within each ctx group)\n\
BAT[void,oid] ctx (context set; sorted on tail)\n\
BAT[void,int] pre_size (from the working set)\n\
BAT[oid,oid]  cands (sorted list of result candidate OIDs in the tail)\n\
bit           one_iter (only one iter?)\n\
bit           one_ctx  (only one ctx node, i.e., the same for all iters?)\n\
oid           min_iter,max_iter (smallest and largest iter id)\n\
bit           no_iter_order (descendant & descendant_or_self, only: \n\
               result will be ordered on item, but not sub-ordered on iter)\n\
DESCRIPTION:\n\
returns all nodes on the @1 axis of the ctx-nodes duplicate free for each group.",
"pf_support");

PROC @1 (BAT[oid,oid] iter, BAT[oid,oid] item, oid cont, BAT[oid,bat] ws, int order, BAT[void,any] cands, chr kind_test) : BAT[void,bat]
{
        var end_ctx := bat(void,oid,0).seqbase(0@0).access(BAT_READ);
	return @1(iter, item, end_ctx, cont, ws, order, cands, kind_test);
}
ADDHELP("@1", "tsheyar", "Sep 2004",
"PARAMETERS:\n\
BAT[void,oid] iter (grouping relation)\n\
BAT[void,oid] item (context set)\n\
oid cont (the current container of the ws)\n\
BAT[void,bat] ws (working set)\n\
int order (input & output order properties:\n\
           bit 0: input is sorted on iter(0) or item(1)\n\
           bit 1: output must be sorted on iter(0) or item(1))\n\
BAT[oid,oid] cands (sorted list of result candidate OIDs in the tail)\n\
DESCRIPTION:\n\
returns all nodes on the @1 axis of the ctx-nodes duplicate free for each group.",
"pf_support");

PROC @1 (BAT[oid,oid] iter, BAT[oid,oid] item, BAT[oid,oid] end_ctx, oid cont, BAT[oid,bat] ws, int order, BAT[void,any] cands, chr kind_test) : BAT[void,bat]
{
	var result := nil;
	var one_iter := FALSE;
	var one_item := FALSE;
	var unq_iter := FALSE;
        var unq_iters := nil;
	var min_iter := oid_nil;
	var max_iter := oid_nil;

	# check consistency
	if ( isnil(seqbase(iter)) or isnil(seqbase(item)) ) {
		ERROR("@1(0): heads of iter & item/ctx must not be NIL!");
	}
	if ( (count(iter) != count(item)) or (seqbase(iter) != seqbase(item)) ) {
		ERROR("@1(1): heads of iter & item/ctx must be aligned (count(iter)="+str(count(iter))+", count(item)="+str(count(item))+", seqbase(iter)="+str(seqbase(iter))+", seqbase(item)="+str(seqbase(item))+") !");
	}
	@:chk_order(@1(3),0,iter)@
	@:chk_order(@1(4),1,item)@
	cands := cands.chk_order(); # just in case...

	var pre_size := ws.fetch(PRE_SIZE).fetch(cont);

	# trivial cases	
	if ( (count(item) = 0) or (count(cands) = 0) or (count(pre_size) = 0) ) {
		result := new(void,bat,2).seqbase(0@0)
		          .append(bat(void,oid,0).seqbase(0@0).access(BAT_READ))
		          .append(bat(void,oid,0).seqbase(0@0).access(BAT_READ))
		          .access(BAT_READ);
		return result;
	}
	
	# special cases
	if ( count(item) = 1 ) {
		# 1 iter, 1 item
		one_iter := TRUE;
		one_item := TRUE;
		# =>  we do not need to sort neither input nor output
		order := 3;
		iter := iter.chk_order(); # just in case...
		item := item.chk_order(); # just in case...
	} else {
	# first: try cheap min==max checks on ordered columns
	if ( ordered(reverse(iter)) and (min(iter) = max(iter)) ) {
		@:one_iter_many_items(@1,(6))@
	} else {
	if ( ordered(reverse(item)) and (min(item) = max(item)) ) {
		@:many_iters_one_item(@1,(7))@
	} else {
	# then: invest in one scan to check order to save two scans for min/max
	iter := iter.chk_order(); # just in case...
	if ( ordered(reverse(iter)) and (min(iter) = max(iter)) ) {
		@:one_iter_many_items(@1,(8))@
	} else {
	item := item.chk_order(); # just in case...
	if ( ordered(reverse(item)) and (min(item) = max(item)) ) {
		@:many_iters_one_item(@1,(9))@
	} else {
	if ( ordered(reverse(iter)) and (key(reverse(iter)))) {
		@:unique_iters()@
	}}}}}}

	min_iter := min(iter);
	max_iter := max(iter);

	@:pre_sort_input@

	# the actual location step
	if ( isnil(result) ) {
		var res := ll_@1 (iter, item, end_ctx, pre_size, cands, one_iter, one_item, 
		                   min_iter, max_iter, (and(order,2) = 0), kind_test);
                               
                @:resolve_unique_iters()@

		result := new(void,bat,2).seqbase(0@0)
		          .append(res.mark(0@0).reverse())
		          .append(res.reverse().mark(0@0).reverse())
		          .access(BAT_READ);
	}

        @:post_sort_output@
	
	return result;
}
ADDHELP("@1", "tsheyar", "Sep 2004",
"PARAMETERS:\n\
BAT[void,oid] iter (grouping relation)\n\
BAT[void,oid] item (context set)\n\
BAT[void,oid] end_ctx (context end for 'early-exit' child step in preceding-sibling implementation;\n\
                       head-aligned with iter & item or empty (=> no 'early-exit'))\n\
oid cont (the current container of the ws)\n\
BAT[void,bat] ws (working set)\n\
int order (input & output order properties:\n\
           bit 0: input is sorted on iter(0) or item(1)\n\
           bit 1: output must be sorted on iter(0) or item(1))\n\
BAT[oid,oid] cands (sorted list of result candidate OIDs in the tail)\n\
DESCRIPTION:\n\
returns all nodes on the @1 axis of the ctx-nodes duplicate free for each group.",
"pf_support");
@
@= pre_sort_input
	# pre-sort input
	if ( (and(order,1) = 0) and not(ordered(reverse(item)))) {
		var ord := item.tsort();
		    ord := ord.CTrefine(iter).mark(0@0).reverse();
		iter := ord.leftfetchjoin(iter).chk_order();
		item := ord.leftfetchjoin(item).chk_order();
		order := or(order,1);
	}
@
@= post_sort_output
	# post-sort output
	if ( (and(order,2) = 2) and not(ordered(reverse(result.fetch(1)))) ) {
		iter := result.fetch(0);
		item := result.fetch(1);
		var ord := item.tsort();
		    ord := ord.CTrefine(iter).mark(0@0).reverse();
		result := new(void,bat,2).seqbase(0@0)
		          .append(ord.leftfetchjoin(iter).chk_order())
		          .append(ord.leftfetchjoin(item).chk_order())
		          .access(BAT_READ);
	}
	
	# post-sort output
	if ( (and(order,2) = 0) and not(ordered(reverse(result.fetch(0)))) ) {
		iter := result.fetch(0);
		item := result.fetch(1);
		var ord := iter.tsort();
		    ord := ord.CTrefine(item).mark(0@0).reverse();
		result := new(void,bat,2).seqbase(0@0)
		          .append(ord.leftfetchjoin(iter).chk_order())
		          .append(ord.leftfetchjoin(item).chk_order())
		          .access(BAT_READ);
	}
@

@= upwards
PROC @1(BAT[void,oid] iter, BAT[void,oid] item, oid cont, BAT[void,bat] ws, int order) : BAT[void,bat]
{
    var pre_sizes := ws.fetch(PRE_SIZE).fetch(cont);
    var pre_levels := ws.fetch(PRE_LEVEL).fetch(cont);

    iter := iter.chk_order();
    item := item.chk_order();
    @:pre_sort_input@

    var res := ll_@1(iter, item, pre_sizes, pre_levels);
    var result := new(void,bat,2).seqbase(0@0).append(hmark(res,0@0)).append(tmark(res,0@0)).access(BAT_READ);

    @:post_sort_output@
    
    return result;
}
@

@= ll_sibling_impl_following
PROC following_sibling(BAT[void,oid] iter, BAT[void,oid] item, oid cont, BAT[void,bat] ws, int order, chr kind_test) : BAT[void,bat]
{
    var pre_levels := ws.fetch(PRE_LEVEL).fetch(cont);
    var pre_sizes := ws.fetch(PRE_SIZE).fetch(cont);
    var pre_kinds;
    if (isnil(kind_test)) {
        # no kind test
        pre_kinds := new(void,chr,0).seqbase(0@0).access(BAT_READ);
    } else {
        pre_kinds := ws.fetch(PRE_KIND).fetch(cont);
    }

    iter := iter.chk_order();
    item := item.chk_order();
    @:pre_sort_input@

    var res := ll_following_sibling([oid](iter), [oid](item), pre_sizes, pre_levels, pre_kinds, kind_test);
    var result := new(void,bat,2).seqbase(0@0).append(hmark(res,0@0)).append(tmark(res,0@0)).access(BAT_READ);

    @:post_sort_output@
    
    return result;
}
@

@= ll_sibling_impl_preceding
#PROC preceding_sibling(BAT[void,oid] iter, BAT[void,oid] item, oid cont, BAT[void,bat] ws, int order, BAT[void,any] cands, chr kind_test) : BAT[void,bat]
PROC preceding_sibling(BAT[void,oid] iter, BAT[void,oid] item, oid cont, BAT[void,bat] ws, int order, chr kind_test) : BAT[void,bat]
{
    var cands;
    if (isnil(kind_test)) {
        cands := ws.fetch(PRE_SIZE).fetch(cont).mirror();
    } else {
        cands := ws.fetch(PRE_KIND).fetch(cont);
    }
    var itr, par;
    var pre_levels := ws.fetch(PRE_LEVEL).fetch(cont);
    var pre_sizes := ws.fetch(PRE_SIZE).fetch(cont);

    iter := iter.chk_order();
    item := item.chk_order();
    @:pre_sort_input@

    # use one iter per item to get parent of each item
    itr := iter.mirror();
    par := ll_parent(itr, item, pre_sizes, pre_levels).chk_order();
    if (count(par) < count(itr)) {
        # nodes that don't have a parent also don't have (preceding) siblings
        itr  := par.hmark(0@0);
        par  := par.tmark(0@0);
        iter := itr.leftfetchjoin(iter);
        item := itr.leftfetchjoin(item);
    }

    # ensure that intermediate [iter,par,item] result is ordered on [par(asc),iter(asc),item(desc)]
    if (not(ordered(reverse(par)))) {
            var ord := par.tsort().CTrefine(iter).CTrefine_rev(item).hmark(0@0);
            par  := ord.leftfetchjoin(par ).chk_order();
            iter := ord.leftfetchjoin(iter).chk_order();
            item := ord.leftfetchjoin(item).chk_order();
    }
    # no further sorting required in child()
    order := or(order,1);

    # child-step from parents with original items as context-ends yields the desired result
    return child(iter, par, item, cont, ws, order, cands, kind_test);
}
@

@= ll_prec_foll_impl
PROC @1(BAT[void,oid] iter, BAT[void,oid] item, oid cont, BAT[void,bat] ws, int order, chr kind_test) : BAT[void,bat]
{
    # find the document root-pre's for prec_foll
    var nid_rid := ws.fetch(NID_RID).fetch(cont);
    var map_pid := ws.fetch(MAP_PID).fetch(cont);
    var doc_pre := ws.fetch(FRAG_ROOT).fetch(cont).tmark(0@0).leftfetchjoin(nid_rid).[swizzle](map_pid).chk_order();
    var pre_sizes := ws.fetch(PRE_SIZE).fetch(cont);
    var pre_kinds;
    if (isnil(kind_test)) {
        # no kind test
        pre_kinds := new(void,chr,0).seqbase(0@0).access(BAT_READ);
    } else {
        pre_kinds := ws.fetch(PRE_KIND).fetch(cont);
    }

    iter := iter.chk_order();
    item := item.chk_order();
    @:pre_sort_input@

    var res := ll_@1([oid](iter.chk_order()), [oid](item.chk_order()), doc_pre, pre_sizes, pre_kinds, kind_test);
    var result := new(void,bat,2).seqbase(0@0).append(hmark(res,0@0)).append(tmark(res,0@0)).access(BAT_READ);

    @:post_sort_output@
    
    return result;
}
@

@mil
CONST AXIS_ancestor           :=  0;
CONST AXIS_ancestor_or_self   :=  1;
CONST AXIS_child              :=  2;
CONST AXIS_descendant         :=  3;
CONST AXIS_descendant_or_self :=  4;
CONST AXIS_following          :=  5;
CONST AXIS_following_sibling  :=  6;
CONST AXIS_parent             :=  7;
CONST AXIS_preceding          :=  8;
CONST AXIS_preceding_sibling  :=  9;
CONST AXIS_self               := 10;
CONST AXIS_attribute          := 11;
CONST TEST_none   := 0;
CONST TEST_kind   := 1;
CONST TEST_ns     := 2;
CONST TEST_loc    := 3;
CONST TEST_nsloc  := 4;
CONST TEST_target := 5;

PROC step (int axis,
           int test,
           bat[void,oid] iter,
           bat[void,oid] cont,
           bat[void,oid] pre,
           any           attr,
           bat[void,bat] ws,
           int order,
           chr kind,
           str ns,
           str loc,
           str tgt) : bat[void,bat]
{
    var step_res,
        attr_iter, attr_cont, attr_pre,
        self_iter, self_cont, self_pre, self_attr,
        merge := false;
                
    if (isnil(order)) {
        ERROR ("PROC step(): Unknown order '%d' !", order);
    }
    if ((test < TEST_none) or (test > TEST_target)) {
        ERROR ("PROC step(): Unknown test '%d' !", test);
    }

    # Our input comes with attribute nodes.
    #
    # Here we have to make sure that attribute nodes are treated
    # correctly for
    #  - upward axes (parent, ancestor, ancestor-or-self) and
    #  - *self axes (self, ancestor-or-self, descendant-or-self).
    if (type(attr) = bat) {
        # consistency check to ensure that attr is of type bat[void,oid]
        if ((attr.count() != iter.count()) or 
            (attr.htype() != void) or
            ((attr.ttype() != void) and (attr.ttype() != oid)) or
            (attr.seqbase() != iter.seqbase()))
            ERROR("PROC step(): attr relation is not aligned.\n");

        # select all attribute nodes
        var attr_tuple := [isnil](attr);
        var map := attr_tuple.ord_uselect(false).mirror();
        var map_count := map.count();

        if (map_count = iter.count()) { # only attr values
            if (axis = AXIS_parent) {
                # return the owner (stored in the pre column)
@= PROC_step_attr_parent
                map := leftfetchjoin (map, pre).select(oid(nil),oid(nil))
                                               .hmark(0@0);
                attr_iter := leftfetchjoin (map, iter);
                attr_cont := leftfetchjoin (map, cont);
                attr_pre  := leftfetchjoin (map, pre);
@mil
                @:PROC_step_attr_parent()@
                @:PROC_step_self_nametest(attr_)@

@= PROC_step_sort_distinct
                # remove duplicates (as a side effect ensure the output order)
                if (and(order,2) = 2) {
                    map := @1cont.tsort()
                               .CTrefine(@1pre)
                               # '@2' is used to disable the attribute column
                               @2.CTrefine(@1attr)
                               .CTrefine(@1iter)
                               .reverse()
                               .kunique()
                               .tmark(0@0);
                }
                else if (and(order,2) = 0) {
                    map := @1iter.tsort()
                               .CTrefine(@1cont)
                               .CTrefine(@1pre)
                               # '@2' is used to disable the attribute column
                               @2.CTrefine(@1attr)
                               .reverse()
                               .kunique()
                               .tmark(0@0);
                }
                else {
                    ERROR ("PROC step(): Unknown order '%d' !", order);
                }
                @1iter := leftfetchjoin (map, @1iter);
                @1cont := leftfetchjoin (map, @1cont);
                @1pre  := leftfetchjoin (map, @1pre);
                # '@2' is used to disable the attribute column
                @2@1attr := leftfetchjoin (map, @1attr);
@mil
                @:PROC_step_sort_distinct(attr_,#)@
                
                return bat (void,bat,3).seqbase(0@0)
                                       .append(attr_iter.chk_order()) # iter
                                       .append(attr_cont.chk_order()) # cont
                                       .append(attr_pre.chk_order() ) # pre
                                       .access(BAT_READ);
            }
            else if (axis = AXIS_ancestor) {
                # lookup the owner (stored in the pre column) ...
                @:PROC_step_attr_parent()@
                # ... and start an ancestor-or-self from the owner
                # (the parents also belong into the result)
                axis := AXIS_ancestor_or_self;
                
                # apply the ancestor-or-self path step
                iter := attr_iter;
                cont := attr_cont;
                pre  := attr_pre;
            }
            else if (axis = AXIS_ancestor_or_self) {
                # lookup the owner (stored in the pre column)
                # and start an ancestor-or-self from the owner
                @:PROC_step_attr_parent()@
                
                # apply the kind and nametest to the attribute context
                # nodes and later merge them with the result of path step
@= PROC_step_attr_self
                merge := true; # see macro PROC_step_merge() 

                # apply the kind test
                if (not (isnil(kind))) { # test for non-attribute nodes
                    self_iter := bat(void,oid,0).seqbase(0@0).access(BAT_READ);
                    self_cont := bat(void,oid,0).seqbase(0@0).access(BAT_READ);
                    self_pre  := bat(void,oid,0).seqbase(0@0).access(BAT_READ);
                    self_attr := bat(void,oid,0).seqbase(0@0).access(BAT_READ);
                }
                else {
                    self_iter := iter;
                    self_cont := cont;
                    self_pre  := pre;
                    self_attr := attr;

                    # apply the nametest
                    @:PROC_step_attr_nametest()@
                }
@mil
                @:PROC_step_attr_self()@

                # apply the ancestor-or-self path step
                iter := attr_iter;
                cont := attr_cont;
                pre  := attr_pre;
            }
            else if (or ((axis = AXIS_self), 
                         (axis = AXIS_descendant_or_self))) {
                # apply the kind and nametest to the attribute context
                # nodes and return them (as we had only attribute context
                # nodes)
                @:PROC_step_attr_self()@
                
                # remove duplicates
                @:PROC_step_sort_distinct(self_,)@
                
                return bat (void,bat,4).seqbase(0@0)
                                       .append(self_iter.chk_order()) # iter
                                       .append(self_cont.chk_order()) # cont
                                       .append(self_pre.chk_order() ) # pre
                                       .append(self_attr.chk_order()) # attr
                                       .access(BAT_READ);
            }
            else {
                # In all other cases a step starting from only attribute
                # context nodes generates an empty result.
                attr_iter := bat(void,oid,0).seqbase(0@0).access(BAT_READ);
                attr_cont := bat(void,oid,0).seqbase(0@0).access(BAT_READ);
                attr_pre  := bat(void,oid,0).seqbase(0@0).access(BAT_READ);
                return bat (void,bat,3).seqbase(0@0)
                                       .append(attr_iter.chk_order()) # iter
                                       .append(attr_cont.chk_order()) # cont
                                       .append(attr_pre.chk_order() ) # pre
                                       .access(BAT_READ);
            }
        }
        else if (map_count != 0) { # mixed pre and attr values
            if (axis = AXIS_parent) {
                # we need to merge the step result
                # and the attribute owners afterwards
                merge := true; # see macro PROC_step_merge() 
                @:PROC_step_attr_parent()@
                @:PROC_step_self_nametest(attr_)@
                
                # filter out the attribute nodes
                @:PROC_step_pre@
            }
            else if (axis = AXIS_ancestor) {
                # we need to merge the step result
                # and the attribute owners afterwards
                merge := true; # see macro PROC_step_merge()
                @:PROC_step_attr_parent()@
                @:PROC_step_self_nametest(attr_)@
                
                # filter out the attribute nodes ...
                @:PROC_step_pre@
                # ... and add the owners to the non-attribute context nodes
@= PROC_step_attr_merge
                iter.access(BAT_APPEND).append(attr_iter).access(BAT_READ);
                cont.access(BAT_APPEND).append(attr_cont).access(BAT_READ);
                pre.access(BAT_APPEND).append(attr_pre).access(BAT_READ);
                map := cont.tsort().CTrefine(pre).CTrefine(iter).hmark(0@0);
                iter := leftfetchjoin (map, iter);
                cont := leftfetchjoin (map, cont);
                pre  := leftfetchjoin (map, pre);
                order := or (order, 1); # now ordered by item, iter
@mil
                @:PROC_step_attr_merge()@
            }
            else if (axis = AXIS_ancestor_or_self) {
                # lookup the owner (stored in the pre column)
                # and start an ancestor-or-self from the owner
                @:PROC_step_attr_parent()@
                
                # (Recreate the content of map as it was overwritten
                # by the macro PROC_step_attr_parent() and we want
                # to use the orginal content once more.)
                map := attr_tuple.ord_uselect(false);
                
                # apply the kind and nametest to the attribute context
                # nodes and later merge them with the result of path step
@= PROC_step_attr_self_filtered
                merge := true; # see macro PROC_step_merge() 
                
                # apply the kind test
                if (not (isnil(kind))) { # test for non-attribute nodes
                    self_iter := bat(void,oid,0).seqbase(0@0).access(BAT_READ);
                    self_cont := bat(void,oid,0).seqbase(0@0).access(BAT_READ);
                    self_pre  := bat(void,oid,0).seqbase(0@0).access(BAT_READ);
                    self_attr := bat(void,oid,0).seqbase(0@0).access(BAT_READ);
                }
                else {
                    # filter the attribute nodes
                    map := map.hmark(0@0);
                    self_iter := leftfetchjoin (map, iter);
                    self_cont := leftfetchjoin (map, cont);
                    self_pre  := leftfetchjoin (map, pre);
                    self_attr := leftfetchjoin (map, attr);

                    # apply the nametest
                    @:PROC_step_attr_nametest()@
                }
@mil
                @:PROC_step_attr_self_filtered()@

                # filter out the attribute nodes ...
                @:PROC_step_pre@
                # ... and add the owners to the non-attribute context nodes
                @:PROC_step_attr_merge()@
            }
            else if (or ((axis = AXIS_self), 
                         (axis = AXIS_descendant_or_self))) {
                # apply the kind and nametest to the attribute context
                # nodes and later merge them with the result of path step
                @:PROC_step_attr_self_filtered()@

                # filter out the attribute nodes ...
                @:PROC_step_pre@
            }
            else {
                # ignore attr start nodes for non-upward axis
@= PROC_step_pre
                map := attr_tuple.ord_uselect(true).hmark(0@0);
                iter := leftfetchjoin (map, iter);
                cont := leftfetchjoin (map, cont);
                pre  := leftfetchjoin (map, pre);
@mil
            }
        }
        # else iter, cont, and pre are only represent pre values
    }
    
    if ((axis < AXIS_ancestor) or (axis > AXIS_attribute)) {
        ERROR ("PROC step(): Unknown axis '%d' !", axis);
@= PROC_step
    } else if (axis = AXIS_@1) {
               if (test = TEST_none  ) {
            step_res := loop_lifted_@1_step                  (iter, pre, cont, ws, order);
        } else if (isnil (kind)) { # either node() (tested already above) or attribute()
            iter     := bat(void,oid,0).seqbase(0@0).access(BAT_READ);
            cont     := bat(void,oid,0).seqbase(0@0).access(BAT_READ);
            pre      := bat(void,oid,0).seqbase(0@0).access(BAT_READ);
            step_res := bat (void,bat,3).seqbase(0@0)
                                        .append(iter.chk_order()) # iter
                                        .append(pre.chk_order() ) # pre
                                        .append(cont.chk_order()) # cont
                                        .access(BAT_READ);
        } else if (test = TEST_kind  ) {
            step_res := loop_lifted_@1_step_with_kind_test   (iter, pre, cont, ws, order, kind);
        } else if (test = TEST_ns    ) {
            step_res := loop_lifted_@1_step_with_ns_test     (iter, pre, cont, ws, order, ns);
        } else if (test = TEST_loc   ) {
            step_res := loop_lifted_@1_step_with_loc_test    (iter, pre, cont, ws, order, loc);
        } else if (test = TEST_nsloc ) {
            step_res := loop_lifted_@1_step_with_nsloc_test  (iter, pre, cont, ws, order, ns, loc);
        } else if (test = TEST_target) {
            step_res := loop_lifted_@1_step_with_target_test (iter, pre, cont, ws, order, tgt);
        }
@= PROC_step_merge
        # merge the result of the step with the attribute owners
        if (merge) {
            var map;
            pre  := step_res.fetch(1);
            iter := materialize (step_res.fetch(0), pre);
            cont := materialize (step_res.fetch(2), pre);
            # '@2' is used to disable the attribute column
            @2attr := pre.project(oid(nil));
            
            iter.access(BAT_APPEND).append(@1_iter).access(BAT_READ);
            cont.access(BAT_APPEND).append(@1_cont).access(BAT_READ);
            pre.access(BAT_APPEND).append(@1_pre).access(BAT_READ);
            # '@2' is used to disable the attribute column
            @2attr.access(BAT_APPEND).append(@1_attr).access(BAT_READ);
            
            # remove duplicates
            @:PROC_step_sort_distinct(,@2)@
                
            # We overestimate the size with 4 result tuples.
            # In some situations the result will only contain 3 tuples.
            return bat (void,bat,4).seqbase(0@0)
                                   .append(iter.chk_order()) # iter
                                   .append(cont.chk_order()) # cont
                                   .append(pre.chk_order())  # pre
                                   # '@2' is used to disable the attribute column
                                   @2.append(attr.chk_order()) # attr
                                   .access(BAT_READ);
        }
@= PROC_step_result
        # change the column order to (iter, cont, pre)
        return bat (void,bat,3).seqbase(0@0)
                               .append(step_res.fetch(0)) # iter
                               .append(step_res.fetch(2)) # cont
                               .append(step_res.fetch(1)) # pre
                               .access(BAT_READ);
@mil
    @:PROC_step(ancestor)@           @:PROC_step_merge(attr,#)@ @:PROC_step_result()@
    @:PROC_step(ancestor_or_self)@   @:PROC_step_merge(self,)@  @:PROC_step_result()@
    @:PROC_step(child)@                                         @:PROC_step_result()@
    @:PROC_step(descendant)@                                    @:PROC_step_result()@
    @:PROC_step(descendant_or_self)@ @:PROC_step_merge(self,)@  @:PROC_step_result()@
    @:PROC_step(following)@                                     @:PROC_step_result()@
    @:PROC_step(following_sibling)@                             @:PROC_step_result()@
    @:PROC_step(parent)@             @:PROC_step_merge(attr,#)@ @:PROC_step_result()@
    @:PROC_step(preceding)@                                     @:PROC_step_result()@
    @:PROC_step(preceding_sibling)@                             @:PROC_step_result()@
    } else if (axis = AXIS_self) {
        var map;
@= PROC_step_self_nametest
               if (test = TEST_none  ) {
            # nothing to do
        } else if (isnil (kind)) { # either node() (tested already above) or attribute()
            @1iter := bat(void,oid,0).seqbase(0@0).access(BAT_READ);
            @1cont := bat(void,oid,0).seqbase(0@0).access(BAT_READ);
            @1pre  := bat(void,oid,0).seqbase(0@0).access(BAT_READ);
        } else if (test = TEST_kind  ) {
            map    := mposjoin (@1pre, @1cont, ws.fetch (PRE_KIND));
            map    := map.ord_uselect(kind).hmark(0@0);
            @1iter := leftfetchjoin (map, @1iter);
            @1cont := leftfetchjoin (map, @1cont);
            @1pre  := leftfetchjoin (map, @1pre);
        } else if (test = TEST_ns    ) {
            map    := mposjoin (@1pre, @1cont, ws.fetch (PRE_KIND));
            map    := map.ord_uselect(ELEMENT).hmark(0@0);
            @1iter := leftfetchjoin (map, @1iter);
            @1cont := leftfetchjoin (map, @1cont);
            @1pre  := leftfetchjoin (map, @1pre);
            map    := mposjoin (mposjoin (@1pre, @1cont, ws.fetch (PRE_PROP)),
                                mposjoin (@1pre, @1cont, ws.fetch (PRE_CONT)),
                                ws.fetch (QN_URI));
            map    := map.ord_uselect(ns).hmark(0@0);
            @1iter := leftfetchjoin (map, @1iter);
            @1cont := leftfetchjoin (map, @1cont);
            @1pre  := leftfetchjoin (map, @1pre);
        } else if (test = TEST_loc   ) {
            map    := mposjoin (@1pre, @1cont, ws.fetch (PRE_KIND));
            map    := map.ord_uselect(ELEMENT).hmark(0@0);
            @1iter := leftfetchjoin (map, @1iter);
            @1cont := leftfetchjoin (map, @1cont);
            @1pre  := leftfetchjoin (map, @1pre);
            map    := mposjoin (mposjoin (@1pre, @1cont, ws.fetch (PRE_PROP)),
                                mposjoin (@1pre, @1cont, ws.fetch (PRE_CONT)),
                                ws.fetch (QN_LOC));
            map    := map.ord_uselect(loc).hmark(0@0);
            @1iter := leftfetchjoin (map, @1iter);
            @1cont := leftfetchjoin (map, @1cont);
            @1pre  := leftfetchjoin (map, @1pre);
        } else if (test = TEST_nsloc ) {
            map    := mposjoin (@1pre, @1cont, ws.fetch (PRE_KIND));
            map    := map.ord_uselect(ELEMENT).hmark(0@0);
            @1iter := leftfetchjoin (map, @1iter);
            @1cont := leftfetchjoin (map, @1cont);
            @1pre  := leftfetchjoin (map, @1pre);
            map    := mposjoin (mposjoin (@1pre, @1cont, ws.fetch (PRE_PROP)),
                                mposjoin (@1pre, @1cont, ws.fetch (PRE_CONT)),
                                ws.fetch (QN_URI_LOC));
            map    := map.ord_uselect(ns + NS_ACCEL_SEP + loc).hmark(0@0);
            @1iter := leftfetchjoin (map, @1iter);
            @1cont := leftfetchjoin (map, @1cont);
            @1pre  := leftfetchjoin (map, @1pre);
        } else if (test = TEST_target) {
            map    := mposjoin (@1pre, @1cont, ws.fetch (PRE_KIND));
            map    := map.ord_uselect(PI).hmark(0@0);
            @1iter := leftfetchjoin (map, @1iter);
            @1cont := leftfetchjoin (map, @1cont);
            @1pre  := leftfetchjoin (map, @1pre);
            map    := mposjoin (mposjoin (@1pre, @1cont, ws.fetch (PRE_PROP)),
                                mposjoin (@1pre, @1cont, ws.fetch (PRE_CONT)),
                                ws.fetch (PROP_TGT));
            map    := map.ord_uselect(tgt).hmark(0@0);
            @1iter := leftfetchjoin (map, @1iter);
            @1cont := leftfetchjoin (map, @1cont);
            @1pre  := leftfetchjoin (map, @1pre);
        }
@mil
        @:PROC_step_self_nametest()@
        
        if (not (merge)) {
            # remove duplicates
            @:PROC_step_sort_distinct(,#)@
            
            return bat (void,bat,3).seqbase(0@0)
                                   .append(iter.chk_order()) # iter
                                   .append(cont.chk_order()) # cont
                                   .append(pre.chk_order() ) # pre
                                   .access(BAT_READ);
        }
                                   
        # Add the result in the ``wrong'' (iter|pre|cont) order.
        # (This way the following macro works correctly.)
        step_res := bat (void,bat,3).seqbase(0@0)
                                    .append(iter)
                                    .append(pre)
                                    .append(cont)
                                    .access(BAT_READ);
        # add the filtered attribute context nodes
        @:PROC_step_merge(self,,4)@
        
        ERROR ("PROC step(): problem in self step (AXIS_self).");
    } else if (axis = AXIS_attribute) {
        var map;
        
        # non-matching kind tests result in an empty result
        if (not (isnil(kind))) { # non-attribute nodes
            iter := bat(void,oid,0).seqbase(0@0).access(BAT_READ);
            cont := bat(void,oid,0).seqbase(0@0).access(BAT_READ);
            pre  := bat(void,oid,0).seqbase(0@0).access(BAT_READ);
            attr := bat(void,oid,0).seqbase(0@0).access(BAT_READ);
            return bat (void,bat,4).seqbase(0@0)
                                   .append(iter.chk_order()) # iter
                                   .append(cont.chk_order()) # cont
                                   .append(pre.chk_order() ) # pre
                                   .append(attr.chk_order()) # attr
                                   .access(BAT_READ);
        }

        # lookup the attribute ids
        map  := get_attr_own (ws, mposjoin(pre, cont, ws.fetch(PRE_NID)), cont);
        
        self_attr := map.tmark(0@0);
        map       := map.hmark(0@0);
        self_iter := leftfetchjoin (map, iter);
        self_cont := leftfetchjoin (map, cont);
        self_pre  := leftfetchjoin (map, pre);
@= PROC_step_attr_nametest
        # apply the nametest
        if (test = TEST_ns) {
            map  := mposjoin (mposjoin (self_attr, self_cont, ws.fetch (ATTR_QN)),
                              mposjoin (self_attr, self_cont, ws.fetch (ATTR_CONT)),
                              ws.fetch (QN_URI));
            map  := map.ord_uselect(ns).hmark(0@0);

            self_iter := leftfetchjoin (map, self_iter);
            self_cont := leftfetchjoin (map, self_cont);
            self_pre  := leftfetchjoin (map, self_pre);
            self_attr := leftfetchjoin (map, self_attr);
        }
        else if (test = TEST_loc) {
            map  := mposjoin (mposjoin (self_attr, self_cont, ws.fetch (ATTR_QN)),
                              mposjoin (self_attr, self_cont, ws.fetch (ATTR_CONT)),
                              ws.fetch (QN_LOC));
            map  := map.ord_uselect(loc).hmark(0@0);
                  
            self_iter := leftfetchjoin (map, self_iter);
            self_cont := leftfetchjoin (map, self_cont);
            self_pre  := leftfetchjoin (map, self_pre);
            self_attr := leftfetchjoin (map, self_attr);
        }
        else if (test = TEST_nsloc) {
            map  := mposjoin (mposjoin (self_attr, self_cont, ws.fetch (ATTR_QN)),
                              mposjoin (self_attr, self_cont, ws.fetch (ATTR_CONT)),
                              ws.fetch (QN_URI_LOC));
            map  := map.ord_uselect(ns + NS_ACCEL_SEP + loc).hmark(0@0);

            self_iter := leftfetchjoin (map, self_iter);
            self_cont := leftfetchjoin (map, self_cont);
            self_pre  := leftfetchjoin (map, self_pre);
            self_attr := leftfetchjoin (map, self_attr);
        }
@mil
        @:PROC_step_attr_nametest()@
        
        # Sort output as we don't know anything about the attribute
        # output order of mvaljoin (called by get_attr_own()).
        # This is done as a side effect of the following duplicate elimination
        
        # remove duplicates
        @:PROC_step_sort_distinct(self_,)@
            
        return bat (void,bat,4).seqbase(0@0)
                               .append(self_iter.chk_order()) # iter
                               .append(self_cont.chk_order()) # cont
                               .append(self_pre.chk_order() ) # pre
                               .append(self_attr.chk_order()) # attr
                               .access(BAT_READ);
    }
}

@:loop_lifted_scj_step1(parent)@
@:loop_lifted_scj_step1(ancestor)@
@:loop_lifted_scj_step1(ancestor_or_self)@

@:loop_lifted_scj_wrap1(child)@
@:loop_lifted_scj_wrap1(descendant)@
@:loop_lifted_scj_wrap1(descendant_or_self)@

@:loop_lifted_scj_step1x(following)@
@:loop_lifted_scj_step1x(preceding)@

@:loop_lifted_scj_step1x(following_sibling)@
@:loop_lifted_scj_step1x(preceding_sibling)@
@
#==================================================================
# expansions of the loop lifted scj
# kind argument
@= kind_args
, chr kind
@
@= kind_params
, kind
@
# tagname argument
@= ns_args
, str ns
@
@= ns_params
, ns
@
@= loc_args
, str loc
@
@= loc_params
, loc
@
@= tgt_args
, str tgt
@
@= tgt_params
, tgt
@
@= nsloc_args
, str ns, str loc
@
@= nsloc_params
, ns, loc
@
@= params1
, kind_test
@
@= params2
, cands, kind_test
@
@= postfilter
if (postfilter) {
	var pre_kind := nil;
	if (isnil(kind_test)) {
	    pre_kind := ws.fetch(PRE_KIND).fetch(contID);
	}
	var pre_cont := ws.fetch(PRE_CONT).fetch(contID);
	var pre_prop := ws.fetch(PRE_PROP).fetch(contID);
	if (is_constant(pre_cont)) {
                # determine a sel [qnid,qnid] of qualifying nslocs (i.e. prop IDs)
		var sel := ws.fetch(@1).fetch(bat2constant(pre_cont)).ord_uselect(@2);

                # first join on prop, possibly getting some false hits for non ELEMENT nodes
                tmp_res := result_part_item.leftfetchjoin(pre_prop).leftjoin(sel).hmark(0@0);

                if (isnil(kind_test)) {
                    # remove false hits;
                    # only necessary if we haven't done the kind-test as pushed-down pre-test
                    tmp_res := tmp_res.leftfetchjoin(result_part_item).leftfetchjoin(pre_kind).ord_uselect(@3).hmark(0@0).leftfetchjoin(tmp_res);
                }
        } else {
                # the cont value refers back to multiple different containers (XPath step in transient doc container)

                var X := result_part_item;
                if (isnil(kind_test)) {
                    # first select the element nodes;
                    # only necessary if we haven't done the kind-test as pushed-down pre-test
                    X := X.leftfetchjoin(pre_kind).ord_uselect(@3);
                }
                X := X.hmark(0@0);

                # fetch cont and prop values
                tmp_res := X.leftfetchjoin(result_part_item);
                var X_cont  := tmp_res.leftfetchjoin(pre_cont);
                var X_prop  := tmp_res.leftfetchjoin(pre_prop);

                # get qnames using mposjoin from the source containers
		var X_nsloc := mposjoin(X_prop, X_cont, ws.fetch(@1));

                # final select
		tmp_res := X_nsloc.ord_uselect(@2).hmark(0@0).leftfetchjoin(X);
        } 
	@:mapping_code@
}
@= mapping_code
	result_part_iter := tmp_res.leftfetchjoin(result_part_iter);
	result_part_item := tmp_res.leftfetchjoin(result_part_item);
	tmp_res := nil;
@= nsloc_post
      { var nsloc := ns + NS_ACCEL_SEP + loc; @:postfilter(QN_URI_LOC,nsloc,ELEMENT)@ }
@= loc_post
        @:postfilter(QN_LOC,loc,ELEMENT)@
@= ns_post
        @:postfilter(QN_URI,ns,ELEMENT)@
@= target_post
        @:postfilter(PROP_TGT,tgt,PI)@
@= kind_post
        if (postfilter and isnil(kind_test)) {
      		var pre_kind := ws.fetch(PRE_KIND).find(contID);
               	tmp_res := result_part_item.leftfetchjoin(pre_kind).ord_uselect(@1).hmark(0@0);
	       	@:mapping_code@
        }
@= no_pre
	cands := ws.fetch(PRE_SIZE).fetch(contID).mirror(); # no selection: cands is everything
@= kind_pre
      { cands := ws.fetch(PRE_KIND).fetch(contID); kind_test := @1; }
@= nsloc_pre
      { var nsloc := ns + NS_ACCEL_SEP + loc; @:prefilter(nsloc,QN_URI_LOC,ELEMENT)@ }
@= loc_pre
      { @:prefilter(loc,QN_LOC,ELEMENT)@ }
@= ns_pre
      { @:prefilter(ns,QN_URI,ELEMENT)@ }
@= prefilter
	       cands := ws.fetch(PRE_SIZE).fetch(contID).mirror();
	var pre_cont := ws.fetch(PRE_CONT).fetch(contID);
	if (is_constant(pre_cont)) {
		var qn_sel := ws.fetch(@2).fetch(bat2constant(pre_cont)).ord_uselect(@1);
                if (isnil(CATCH(cands := ws_lookup(ws, contID, qn_sel.mirror())))) {
	                if ( (count(cands) > 0) and (count(result_part_item) > 0) ) {
                                var min_cand := min(cands);
                                var min_item := min(result_part_item);
                                if ( min_cand < min_item ) {
                                        cands := cands.ord_select(min_item,oid_nil);
                                }
                        }
                        if ( (count(cands) > 0) and (count(result_part_item) > 0) ) {
                                var max_cand := max(cands);
                                var max_item := max(result_part_item);
                                if ( max_cand < max_item ) {
                                        tmp_res := result_part_item.ord_uselect(oid_nil,max_cand).hmark(0@0);
                                        @:mapping_code@
                                }
                        }
                	postfilter := false; # we have a true candidate list
                }
        } else {
                # we use postfilter (after SCJ) for ns/loc/nsloc/target,
                # but prefilter for kind
                @:kind_pre(@3)@
        }

# expanding the scj for the different tests
@= loop_lifted_scj_wrap1
@:loop_lifted_scj_step2(@1,,,,,                                                               @:params2@,@:no_pre@)@
@:loop_lifted_scj_step2(@1,_with_kind_test,  @:kind_args@, @:kind_params@,,                   @:params2@,@:kind_pre(kind)@)@
@:loop_lifted_scj_step2(@1,_with_nsloc_test, @:nsloc_args@,@:nsloc_params@,@:nsloc_post@,     @:params2@,@:nsloc_pre@)@
@:loop_lifted_scj_step2(@1,_with_ns_test,    @:ns_args@,   @:ns_params@,   @:ns_post@,        @:params2@,@:ns_pre@)@
@:loop_lifted_scj_step2(@1,_with_loc_test,   @:loc_args@,  @:loc_params@,  @:loc_post@,       @:params2@,@:loc_pre@)@
@:loop_lifted_scj_step2(@1,_with_target_test,@:tgt_args@,  @:tgt_params@,  @:target_post@,    @:params2@,@:no_pre@)@
@
@= loop_lifted_scj_step1
@:loop_lifted_scj_step2(@1,,,,,,                                                               )@
@:loop_lifted_scj_step2(@1,_with_kind_test,  @:kind_args@, @:kind_params@, @:kind_post(kind)@,,)@
@:loop_lifted_scj_step2(@1,_with_nsloc_test, @:nsloc_args@,@:nsloc_params@,@:nsloc_post@     ,,)@
@:loop_lifted_scj_step2(@1,_with_ns_test,    @:ns_args@,   @:ns_params@,   @:ns_post@        ,,)@
@:loop_lifted_scj_step2(@1,_with_loc_test,   @:loc_args@,  @:loc_params@,  @:loc_post@       ,,)@
@:loop_lifted_scj_step2(@1,_with_target_test,@:tgt_args@,  @:tgt_params@,  @:target_post@    ,,)@
@
@= loop_lifted_scj_step1x
@:loop_lifted_scj_step2(@1,,,,,                                                               @:params1@,)@
@:loop_lifted_scj_step2(@1,_with_kind_test,  @:kind_args@, @:kind_params@,,                   @:params1@,kind_test := kind;)@
@:loop_lifted_scj_step2(@1,_with_nsloc_test, @:nsloc_args@,@:nsloc_params@,@:nsloc_post@,     @:params1@,kind_test := ELEMENT;)@
@:loop_lifted_scj_step2(@1,_with_ns_test,    @:ns_args@,   @:ns_params@,   @:ns_post@,        @:params1@,kind_test := ELEMENT;)@
@:loop_lifted_scj_step2(@1,_with_loc_test,   @:loc_args@,  @:loc_params@,  @:loc_post@,       @:params1@,kind_test := ELEMENT;)@
@:loop_lifted_scj_step2(@1,_with_target_test,@:tgt_args@,  @:tgt_params@,  @:target_post@,    @:params1@,kind_test := PI;)@
@
#==================================================================
# actual definition of the scj proc
@= loop_lifted_scj_per_cont
	result_part_iter := result_part_iter.chk_order();
	result_part_item := result_part_item.chk_order();
        result_part_cont := nil;

	# pre-test
	@4

        var result := @1 (result_part_iter, result_part_item, contID, ws, order @3);
	result_part_iter := result.fetch(0);
	result_part_item := result.fetch(1);
        result_part_cont := constant2bat(contID);
	cands := nil;

	# post-test
	@2
@
@= loop_lifted_scj_step2
PROC loop_lifted_@1_step@2 (bat[void, oid] iter, bat[void, oid] item, bat[void, oid] cont, bat[void, bat] ws @3) : bat[void,bat]
{
     return loop_lifted_@1_step@2 (iter, item, cont, ws, 0 @4);
}
PROC loop_lifted_@1_step@2 (bat[void, oid] iter, bat[void, oid] item, bat[void, oid] cont, bat[void, bat] ws, int order @3) : bat[void,bat]
{
    # handle empty results correctly
    if (iter.count() = 0) {
	var empty := bat(void,oid,0).seqbase(0@0).access(BAT_READ);
	return bat(void,bat,3).seqbase(0@0)
			    .append(empty)
			    .append(empty)
			    .append(empty)
			    .access(BAT_READ);
    }

    var result;
    var result_iter;
    var result_item;
    var result_cont;
    var tmp_res;
    var cands;
    var kind_test := chr_nil;
    var postfilter := true;

    var uniqueCont := cont.tunique().sort();
    var contID := uniqueCont.reverse().fetch(0);
    if (uniqueCont.count() = 1) {
        var result_part_cont := oid_nil;
        var result_part_iter := iter;
        var result_part_item := item;

        @:loop_lifted_scj_per_cont(@1,@5,@6,@7)@

        result_iter := result_part_iter;
        result_item := result_part_item;
        result_cont := result_part_cont;
        result_part_iter := nil;
        result_part_item := nil;
        result_part_cont := nil;
    } else {
        var result_part_cont := cont.ord_uselect(contID).hmark(0@0);
        var result_part_iter := result_part_cont.leftfetchjoin(iter);
        var result_part_item := result_part_cont.leftfetchjoin(item);

        @:loop_lifted_scj_per_cont(@1,@5,@6,@7)@

        result_iter := result_part_iter;
        result_item := result_part_item;
        result_cont := result_part_cont;
        result_part_iter := nil;
        result_part_item := nil;
        result_part_cont := nil;

        var res_mu;
	uniqueCont.slice(1,uniqueCont.count() - 1)@batloop () {
	    contID := $h;
            result_part_cont := cont.ord_uselect(contID).hmark(0@0);
            result_part_iter := result_part_cont.leftfetchjoin(iter);
            result_part_item := result_part_cont.leftfetchjoin(item);

            @:loop_lifted_scj_per_cont(@1,@5,@6,@7)@

            if ( and(order,2) = 2 )
	    {
	         res_mu := merged_union(result_item, result_part_item,
	             		   result_iter, result_part_iter,
	             		   result_cont, result_part_cont);
                 result_part_iter := nil;
                 result_part_item := nil;
                 result_part_cont := nil;
                 result_item := res_mu.fetch(0);
                 result_iter := res_mu.fetch(1);
                 result_cont := res_mu.fetch(2);
                 res_mu := nil;
	    }
	    else
	    {
	         res_mu := merged_union(result_iter, result_part_iter,
	             		   result_item, result_part_item,
	             		   result_cont, result_part_cont);
                 result_part_iter := nil;
                 result_part_item := nil;
                 result_part_cont := nil;
                 result_iter := res_mu.fetch(0);
                 result_item := res_mu.fetch(1);
                 result_cont := res_mu.fetch(2);
                 res_mu := nil;
	    }
        }
    }
    
    result_iter.access(BAT_READ);
    result_item.access(BAT_READ);
    result_cont.access(BAT_READ);
    var result_scj := bat(void,bat,3).seqbase(0@0);
    result_scj.append(result_iter);
    result_scj.append(result_item);
    result_scj.append(result_cont);

    return result_scj.access(BAT_READ);
}
@


@- update primitives
@mil

#################################################################
#
#
#
#                           /-------------\
#                          /               \
#                         /                 \
#                        /                   \
#                        |   XXXX     XXXX   |
#                        |   XXXX     XXXX   |
#                        |   XXX       XXX   |
#                        \         X        /
#                         --\     XXX     /--
#                          | |    XXX    | |
#                          | |           | |
#                          | I I I I I I I |
#                          |  I I I I I I  |
#                           \             /
#                            --         --
#                              \-------/
#                      XXX                    XXX
#                     XXXXX                  XXXXX
#                     XXXXXXXXX         XXXXXXXXXX
#                            XXXXX   XXXXX
#                               XXXXXXX
#                            XXXXX   XXXXX
#                     XXXXXXXXX        XXXXXXXXXX
#                     XXXXX                  XXXXX
#                      XXX                    XXX
#
#                            **************
#                            *  BEWARE!!  *
#                            **************
#
#                        All ye who enter here:
#                   Most of the code in this module
#                      is twisted beyond belief!
#
#                           Tread carefully.
#
#                   If you think you understand it,
#                              You Don't,
#                            So Look Again.
#
#################################################################

#
# the UPDATE_INSERT_*, UPDATE_REPLACENODE and UPDATE_REPLACECONTENT commands MUST be
# consecutive, the UPDATE_INSERT_LAST and UPDATE_INSERT_BEFORE
# commands also MUST be consecutive
const UPDATE_INSERT_FIRST := 1LL;
const UPDATE_INSERT_LAST := 2LL;
const UPDATE_INSERT_BEFORE := 3LL;
const UPDATE_INSERT_AFTER := 4LL;
const UPDATE_REPLACECONTENT := 5LL;
const UPDATE_REPLACENODE := 6LL;
const UPDATE_DELETE := 7LL;
const UPDATE_RENAME := 8LL;
const UPDATE_REPLACE := 9LL;

PROC myupdate(bat[any::1,any::2] oldbat, bat[any::1,any::2] newbat) : bat[any::1,any::2]
{
  if (newbat.count() = 0) {
    return oldbat;
  }
  if (oldbat.htype() = void) {
    var mx := newbat.reverse().max().wrd();
    var x;
    if (oldbat.count() = 0) {
      x := wrd(-1);
    } else {
      x := oldbat.reverse().max().wrd();
    }
    if (x < mx) {
      oldbat.append(densebat(wrd(mx - x)).project(cast(nil, oldbat.ttype())), true);
    }
    oldbat := oldbat.replace(newbat, true);
  } else {
    if (ordered(oldbat)) newbat := newbat.chk_order().sort();
    var newvals := kdiff(newbat, oldbat);
    var replvals := kdiff(newbat, newvals);
    oldbat := oldbat.replace(replvals, true).insert(newvals);
  }
  return oldbat;
}
ADDHELP("myupdate", "sjoerd", "Apr 13 2006",
"Combination of insert and replace: insert new values and replace existing ones.",
"pf_support");

PROC myupdate(bat[any::1,any::2] oldbat, any::1 key, any::2 val) : bat[any::1,any::2]
{
  if (oldbat.htype() = void) {
    # we want to maintain the void head if at all possible
    # therefore we check whether the key already exists and do a
    # replace if it does
    if (not(oldbat.exist(key))) {
      var x;
      if (oldbat.count() = 0) {
        x := wrd(-1);
      } else {
        x := oldbat.reverse().max().wrd();
      }
      var xkey := wrd(key);
      if (x < xkey) {
        oldbat.append(densebat(wrd(xkey - x)).project(cast(nil, oldbat.ttype())), true);
      }
    }
    oldbat.replace(key, val, true);
  } else {
    oldbat.insert(key, val);
  }
  return oldbat;
}
ADDHELP("myupdate", "sjoerd", "Apr 13 2006",
"Combination of insert and replace: insert new values and replace existing ones.",
"pf_support");

PROC findupdate(BAT[any::1,any::2] oldbat, BAT[any::1,any::2] newbat, any::1 key): any::2
{
  if (newbat.exist(key)) {
    return newbat.find(key);
  } else {
    return oldbat.find(key);
  }
}

# convert a PRE value to a RID value using the given map
PROC antiswizzle(oid o, BAT[oid,oid] map) : oid
{
  var pid := oid(lng(o) >> REMAP_PAGE_BITS);
  var revmap := map.reverse();
  if (revmap.exist(pid))
    return oid(<<(lng(revmap.find(pid)),REMAP_PAGE_BITS) + and(lng(o),REMAP_PAGE_MASK));
  return o;
}

# Find the ancestors of a given node in a document that is being modified.
PROC mil_ancestor(bat[void,bat] ws, oid cont, oid newpre) : bat[void,oid]
{
  var ancestors := new(void, oid).seqbase(0@0); # the result
  var pre_size := ws.fetch(PRE_SIZE).find(cont);
  var rid_size := ws.fetch(_RID_SIZE).find(cont);
  var rid_size_update := ws.fetch(RID_SIZE_UPDATE).find(cont);
  var map_pid := ws.fetch(MAP_PID).find(cont);
  var map_pid_update := ws.fetch(MAP_PID_UPDATE).find(cont);
  var pid_map_update := map_pid_update.select(oid_nil, oid_nil).reverse().sort().tmark(0@0);

  var p := 0@0;
  extend_protect(ws, cont);
  while (p < newpre) {
    var pageid := pid_map_update.find(oid(lng(p) >> REMAP_PAGE_BITS));
    var isoldpage := false;
    if (map_pid.exist(pageid)) {
      isoldpage := not(isnil(map_pid.find(pageid)));
    }
    var r := swizzle(p, pid_map_update);
    var size;
    if (isoldpage) {
      if (rid_size_update.exist(r)) {
        size := rid_size_update.find(r);
      } else {
        size := pre_size.find(swizzle(r, map_pid));
      }
    } else {
      size := rid_size.find(r); # PETER: looks like this code path was untested
    }
    size := niland(size, INT_MAX);
    var e := oid(lng(p) + size);
    if (e >= newpre) {
      ancestors.append(p);
      p := oid(lng(p) + 1);
    } else {
      p := oid(lng(e) + 1);
    }
  }
  extend_unprotect(ws, cont);
  return ancestors;
}

# # Find the parent of a given node in a document that is being modified.
# # this function is called with extend_protect(ws, cont) set.
# PROC mil_parent(bat[void,bat] ws, oid cont, oid newpre) : oid
# {
#   var parent := oid_nil;	# the result
#   var pre_size := ws.fetch(PRE_SIZE).find(cont);
#   var rid_size := ws.fetch(_RID_SIZE).find(cont);
#   var rid_size_update := ws.fetch(RID_SIZE_UPDATE).find(cont);
#   var map_pid := ws.fetch(MAP_PID).find(cont);
#   var map_pid_update := ws.fetch(MAP_PID_UPDATE).find(cont);
#   var pid_map_update := map_pid_update.select(oid_nil, oid_nil).reverse().sort().tmark(0@0);

#   var p := 0@0;
#   while (p < newpre) {
#     var pageid := pid_map_update.find(oid(lng(p) >> REMAP_PAGE_BITS));
#     var isoldpage := false;
#     if (map_pid.exist(pageid)) {
#       isoldpage := not(isnil(map_pid.find(pageid)));
#     }
#     var r := swizzle(p, pid_map_update);
#     var size;
#     if (isoldpage) {
#       if (rid_size_update.exist(r)) {
#         size := rid_size_update.find(r);
#       } else {
#         size := pre_size.find(swizzle(r, map_pid));
#       }
#     } else {
#       size := rid_size.find(r); # PETER: looks like this code path was untested
#     }
#     size := niland(size, INT_MAX);
#     var e := oid(lng(p) + size);
#     if (e >= newpre) {
#       parent := p;
#       p := oid(lng(p) + 1);
#     } else {
#       p := oid(lng(e) + 1);
#     }
#   }
#   return parent;
# }

# Move data starting at "from" of length "size" by "delta".  The
# destination *must* be a hole (XXX check this).
# size *must* be smaller than a page
# from + size + delta (delta > 0) *must be on the same page as from
# deom + size *must be on the same page as from + delta (delta < 0)
# Delta can be positive (move data down) or negative (move data up).
PROC movedata(bat[void,bat] ws, oid cont, oid from, int size, int delta): void
{
  if (delta = 0)
    return;                     # not moving, so nothing to do

# The picture shows the layout of the tables and what the important
# variables represent.  When delta > 0, the data is moved down (and
# thus the hole is moved up).
#
#       delta > 0 (moving down)                delta < 0 (moving up)
#             |         |                            |         |
#             |         |                            |         |
# before ---->|         |                before ---->|         |
#             |---------|                            |---------|
# from ------>| ^       |<- newholefirst holefirst ->| ^       |
#             | |       |                            | |       |
#             | | size  |                            | | delta |
#             | |       |<- newholelast  holelast -->| |       |
#             | | - - - |                            |---------|
# last ------>| v       |                from ------>| ^       |
#             |---------|                            | | - - - |
# holefirst ->| |       |                            | |       |<- newholefirst
#             | | delta |                            | | size  |
#             | |       |                            | |       |
# holelast -->| v       |                last ------>| v       |<- newholelast
#             |---------|                            |---------|
# after ----->|         |                after ----->|         |
#             |         |                            |         |
#             |         |                            |         |
#
# The following areas are of interest:
# - data to be moved, this is the data with PRE values "from" to "last"
#   and given by the parameters "from" and "size";
# - hole to be overwritten, this is the hole after (if delta > 0) or
#   before (if delta < 0) the data to be moved with PRE values
#   "holefirst" to "holelast" and size "abs(delta)";
# - affected area, this is the combination of the above two areas;
# - the area before the affected area;
# - the area after the affected area.
#
# We distinguish the following nodes that each get their own treatment:
# - nodes in the area before the affected area that end inside the data
#   to be moved;
# - nodes in the area before the affected area that end inside the hole
#   to be overwritten;
# - nodes in the data to be moved that end in the hole to be overwritten
#   (only applicable if delta > 0);
# - nodes in the data to be moved that end in the area after the
#   affected area.

  var map_pid := ws.fetch(MAP_PID).find(cont);
  var map_pid_update := ws.fetch(MAP_PID_UPDATE).find(cont);

  # some helpful values
  var pageno := oid(lng(from) >> REMAP_PAGE_BITS); # page number where all this takes place
  var pageid := map_pid_update.reverse().find(pageno);
  ws.fetch(MODIFIED_PAGE).insert(cont, pageid); # we're modifying this page
  var isoldpage := false;
  if (map_pid.exist(pageid)) {
    isoldpage := not(isnil(map_pid.find(pageid)));
  }
  # we're using RID values exclusively in this function
  var from_rid := antiswizzle(from, map_pid_update);

# var delta;                    # displacement (< 0: up, > 0: down) (parameter)
  var holefirst;                # ID of start of hole
  var holelast;                 # ID of end of hole
  var before;                   # ID of last node before area being moved and hole
# var from;                     # ID of first node being moved (parameter)
  var last;                     # ID of last node being moved
# var size;                     # size of area being moved (parameter)
  var after;                    # ID of first node after area being moved and hole
  var newholefirst;             # ID if start of to-be-created hole
  var newholelast;              # ID if end of to-be-created hole

  last := oid((lng(from_rid) + size) - 1);
  if (delta < 0) {
    holefirst := oid(lng(from_rid) + delta);
    holelast := oid(lng(from_rid) - 1);
    before := oid(lng(holefirst) - 1);
    after := oid(lng(last) + 1);
    newholefirst := oid(lng(holefirst) + size);
    newholelast := oid(lng(holelast) + size);
  } else {
    holefirst := oid(lng(from_rid) + size);
    after := oid(lng(holefirst) + delta);
    holelast := oid(lng(after) - 1);
    before := oid(lng(from_rid) - 1);
    newholefirst := from_rid;
    newholelast := oid(lng(from_rid) + delta - 1);
  }

  var pre_size := ws.fetch(PRE_SIZE).find(cont);
  var rid_size := ws.fetch(_RID_SIZE).find(cont);
  var rid_size_update := ws.fetch(RID_SIZE_UPDATE).find(cont);
  var pre_level := ws.fetch(PRE_LEVEL).find(cont);
  var rid_level := ws.fetch(_RID_LEVEL).find(cont);
  var rid_level_update := ws.fetch(RID_LEVEL_UPDATE).find(cont);
  var pre_kind := ws.fetch(PRE_KIND).find(cont);
  var rid_kind := ws.fetch(_RID_KIND).find(cont);
  var rid_kind_update := ws.fetch(RID_KIND_UPDATE).find(cont);
  var pre_prop := ws.fetch(PRE_PROP).find(cont);
  var rid_prop := ws.fetch(_RID_PROP).find(cont);
  var rid_prop_update := ws.fetch(RID_PROP_UPDATE).find(cont);
  var pre_nid := ws.fetch(PRE_NID).find(cont);
  var rid_nid := ws.fetch(_RID_NID).find(cont);
  var rid_nid_update := ws.fetch(RID_NID_UPDATE).find(cont);
  var modified_nid := ws.fetch(MODIFIED_NID);
  var ancestor_nid := ws.fetch(ANCESTOR_NID);
  var pid_map_update := map_pid_update.select(oid_nil, oid_nil).reverse().sort().tmark(0@0);

  extend_protect(ws, cont);

  # for each page prior to the one we're actually moving in, and for
  # all nodes in the page we're moving in before the affected area,
  # find the nodes (holes or real nodes) that end inside the affected
  # area (i.e. inside the data being moved or the hole that is going
  # to be overwritten) and fix up the size

  # In this section of the code we're only dealing with "ancestor"
  # nodes.  Note that nodes that are on new pages are by definition
  # not ancestral and must already be in the MODIFIED_NID table.  Also
  # nodes that are already in an UPDATES bat must already be in the
  # MODIFIED_NID table.
  {
    var ancestors_newpre;
    var ancestors_newrid;
    var ancestors_isnewpage;
    var ancestors_nid;
    var ancestors_oldpre;
    {
      var from_nid;
      if (isoldpage) {
        if (rid_nid_update.exist(from_rid)) {
          from_nid := rid_nid_update.find(from_rid);
        } else {
          from_nid := pre_nid.find(swizzle(from_rid, map_pid));
        }
      } else {
        from_nid := rid_nid.find(from_rid);
      }
      var oldpre := oid_nil;
      if (nid_rid.exist(from_nid))
         oldpre := swizzle(nid_rid.find(from_nid), map_pid);
      if (not(isnil(oldpre))) {
        # it's an old node, we can use ll_ancestor to find ancestors
        ancestors_oldpre := new(void,oid).seqbase(0@0).append(0@0).append(ll_ancestor(new(void,oid).seqbase(0@0).append(0@0), new(void,oid).seqbase(0@0).append(oldpre), pre_size, pre_level));
        ancestors_nid := ancestors_oldpre.join(pre_nid);
        ancestors_newrid := ancestors_nid.join(nid_rid).access(BAT_WRITE).myupdate(ancestors_nid.join(nid_rid_update));
        ancestors_newpre := [swizzle](ancestors_newrid, map_pid_update);
        ancestors_isnewpage := [isnil](outerjoin([oid]([>>]([lng](ancestors_newrid), REMAP_PAGE_BITS)), map_pid));
      } else {
        ancestors_newpre := mil_ancestor(ws, cont, swizzle(from_rid, map_pid_update));
        ancestors_newrid := [swizzle](ancestors_newpre, pid_map_update);
        ancestors_isnewpage := [isnil](outerjoin([oid]([>>]([lng](ancestors_newrid), REMAP_PAGE_BITS)), map_pid));
        {
          # distinguish the three cases:
          # rid is on new page
          var a := ancestors_isnewpage.uselect(true).mirror().join(ancestors_newrid).join(rid_nid);
          # rid is on old page and was modified
          var b := ancestors_isnewpage.uselect(false).mirror().join(ancestors_newrid).join(rid_nid_update);
          # rid is on old page and was not modified
          var c := ancestors_isnewpage.uselect(false).kdiff(b).mirror().join(ancestors_newrid).[swizzle](map_pid).join(pre_nid);
          # combine
          ancestors_nid := a.access(BAT_WRITE).insert(b).insert(c).order().tmark(0@0);
        }
        ancestors_oldpre := ancestors_nid.join(nid_rid).[swizzle](map_pid);
      }
    }
    var ancestors_size := ancestors_oldpre.join(pre_size).access(BAT_WRITE).myupdate(ancestors_newrid.join(rid_size_update));
    # now figure out which ancestors *end* at the hole
    var ancestors_end := [oid]([+]([lng](ancestors_newpre), ancestors_size));
    # select those that end in the data being moved
    var cands := ancestors_end.uselect(swizzle(from_rid, map_pid_update), swizzle(last, map_pid_update));
    if (cands.count() > 0) {
      var new_size := [+](cands.mirror().join(ancestors_size), delta);
      var old_page := ancestors_isnewpage.uselect(false);
      if (old_page.count() > 0) {
         rid_size_update.myupdate(ancestors_newrid.reverse().join(old_page.mirror().join(new_size)));
      }
      var new_page := ancestors_isnewpage.uselect(true);
      if (new_page.count() > 0) {
        rid_size.myupdate(ancestors_newrid.reverse().join(new_page.mirror().join(new_size)));
      }
      # remember which ancestors were changed
      ancestor_nid.insert(cands.project(cont).reverse().join(ancestors_nid));
    }
    # select those that end in the hole being overwritten
    var cands := ancestors_end.uselect(swizzle(holefirst, map_pid_update), swizzle(holelast, map_pid_update));
    if (cands.count() > 0) {
      # clamp to edge
      var new_size := [int](cands.mirror().join(ancestors_newrid)); # not really, yet
      if (delta < 0) {
        new_size := [int]([-](int(before), new_size));
      } else {
        new_size := [int]([-](int(last), new_size));
      }
      var old_page := ancestors_isnewpage.uselect(false);
      if (old_page.count() > 0) {
         rid_size_update.myupdate(ancestors_newrid.reverse().join(old_page.mirror().join(new_size)));
      }
      var new_page := ancestors_isnewpage.uselect(true);
      if (new_page.count() > 0) {
         rid_size.myupdate(ancestors_newrid.reverse().join(new_page.mirror().join(new_size)));
      }
      # remember which ancestors were changed
      ancestor_nid.insert(cands.project(cont).reverse().join(ancestors_nid));
    }
  }

  # the PRE_SIZE table has the correct sizes, but we now need to
  # actually move the data in all PRE tables
  var update_hole;
  var from_pre := swizzle(from_rid, map_pid);
  var last_pre := swizzle(last, map_pid);

  {
    var data;
    if (isoldpage) {
      data := pre_size.reverse().select(from_pre, last_pre).reverse().copy().access(BAT_WRITE).seqbase(from_rid).key(true);
      data.myupdate(rid_size_update.reverse().select(from_rid, last).reverse());
    } else {
      data := rid_size.reverse().select(from_rid, last).reverse().copy().access(BAT_WRITE).key(true);
    }

    # update the sizes of the moved data

    # table with int(nil) for all RID values that represent holes and 0 otherwise
    var ishole := [niland](data, int_nil);
    var end_data := [oid]([+]([int](data.mirror()), [niland](data, INT_MAX)));
    # find the nodes that end inside the hole that is to disappear
    if (delta > 0) {
      var holeend := end_data.uselect(holefirst, holelast);
      var newsizes := [-](int(last), [int](holeend.mirror()));
      newsizes := [nilor](newsizes.mirror().join(ishole), newsizes);
      data.replace(newsizes);
    } # else
      # if moving backward (delta < 0) no nodes that are to be moved end
      # in the hole
    # find the nodes that end past the hole and compensate for the move
    var pastend := end_data.uselect(after, oid_nil);
    if (pastend.count() > 0) {
      var newsizes := [-](pastend.mirror().join(data), delta);
      data.replace(newsizes);
    }

    # now fix the head values (i.e. move the data)
    var update_data := data.reverse().mark(oid(lng(from_rid) + delta)).reverse();
    # we're not actually using the tail value here...
    update_hole := rid_size.reverse().select(newholefirst, newholelast).reverse();
    if (update_hole.count() < delta) {
      update_hole := update_hole.copy().access(BAT_WRITE).key(true);
      update_hole := update_hole.append(densebat(wrd(delta - update_hole.count())).project(int_nil));
    }
    update_hole := [nilor]([-](update_hole.project(int(newholelast)), [int](update_hole.mirror())), int_nil);

    if (isoldpage) {
      rid_size_update.myupdate(update_hole).myupdate(update_data);
    } else {
      rid_size.replace(update_hole, true).replace(update_data, true);
    }
  }

  {
    var update_data;
    if (isoldpage) {
      update_data := pre_level.reverse().select(from_pre, last_pre).reverse().copy().seqbase(from_rid);
      update_data := update_data.access(BAT_WRITE).key(true).myupdate(rid_level_update.reverse().select(from_rid, last).reverse());
    } else {
      update_data := rid_level.reverse().select(from_rid, last).reverse();
    }
    update_data := update_data.reverse().mark(oid(lng(from_rid) + delta)).reverse();
    update_hole := update_hole.project(cast(nil, rid_level.ttype()));
    if (isoldpage) {
      rid_level_update.myupdate(update_hole).myupdate(update_data);
    } else {
      rid_level.replace(update_hole, true).replace(update_data, true);
    }
  }

  {
    var update_data;
    if (isoldpage) {
      update_data := pre_kind.reverse().select(from_pre, last_pre).reverse().copy().seqbase(from_rid);
      update_data := update_data.access(BAT_WRITE).key(true).myupdate(rid_kind_update.reverse().select(from_rid, last).reverse());
    } else {
      update_data := rid_kind.reverse().select(from_rid, last).reverse();
    }
    update_data := update_data.reverse().mark(oid(lng(from_rid) + delta)).reverse();
    update_hole := update_hole.project(cast(nil, rid_kind.ttype()));
    if (isoldpage) {
      rid_kind_update.myupdate(update_hole).myupdate(update_data);
    } else {
      rid_kind.replace(update_hole, true).replace(update_data, true);
    }
  }

  {
    var update_data;
    if (isoldpage) {
      update_data := pre_prop.reverse().select(from_pre, last_pre).reverse().copy().seqbase(from_rid);
      update_data := update_data.access(BAT_WRITE).key(true).myupdate(rid_prop_update.reverse().select(from_rid, last).reverse());
    } else {
      update_data := rid_prop.reverse().select(from_rid, last).reverse();
    }
    update_data := update_data.reverse().mark(oid(lng(from_rid) + delta)).reverse();
    update_hole := update_hole.project(oid_nil);
    if (isoldpage) {
      rid_prop_update.myupdate(update_hole).myupdate(update_data);
    } else {
      rid_prop.replace(update_hole, true).replace(update_data, true);
    }
  }

  {
    var update_data;
    if (isoldpage) {
      update_data := pre_nid.reverse().select(from_pre, last_pre).reverse().copy().seqbase(from_rid);
      update_data := update_data.access(BAT_WRITE).key(true).myupdate(rid_nid_update.reverse().select(from_rid, last).reverse());
    } else {
      update_data := rid_nid.reverse().select(from_rid, last).reverse();
    }
    update_data := update_data.reverse().mark(oid(lng(from_rid) + delta)).reverse();
    update_hole := update_hole.project(oid_nil);
    if (isoldpage) {
      rid_nid_update.myupdate(update_hole).myupdate(update_data);
    } else {
      rid_nid.replace(update_hole, true).replace(update_data, true);
    }
    var update_nid := update_data.select(oid_nil, oid_nil).reverse();
    var nid_rid_update := ws.fetch(NID_RID_UPDATE).find(cont);
    nid_rid_update.myupdate(update_nid);
    modified_nid.reverse().accbuild("hash");
    modified_nid.insert([lng](update_nid.project(cont).reverse()).[>>](OID_PAGE_BITS));
  }
  extend_unprotect(ws, cont);
}
ADDHELP("movedata", "sjoerd", "Oct 4 2005",
"Move data starting in WS for document CONT, starting at FROM of size
SIZE by DELTA. Delta may be positive (move data down) or
negative. Data should only be moved over an existing hole (not
currently checked). Returns the first argument which details the
changed already made to the BATs in the working set.",
"pf_support");


PROC UpdateTape(
bat[void, bat] ws,
bat[void, lng] kind,
bat[void, oid] pre_tgt, bat[void,int] pre_count_tgt,
bat[void, oid] attr_tgt, bat[void,int] attr_count_tgt,
bat[void, str] replace_strings, 
bat[void, oid] replace_qn_uristrings, bat[void,oid] replace_qn_loc_strings, 
bat[void, oid] pre_ins, bat[void,int] pre_count_ins,
bat[void, oid] attr_ins, bat[void,int] attr_count_ins): void
{
 kind.print();
 pre_tgt.print();
 attr_tgt.print();
 replace_strings.print();
 replace_qn_uristrings.print();
 replace_qn_loc_strings.print();
 pre_ins.print();
 attr_ins.print();
}

PROC play_update_tape(bat[void, bat] ws, bat[void, oid] item, bat[void, int] kind, bat[void,lng] int_values, bat[void,str] str_values) : void
{
  # [void,oid] list of all conts of affected documents
  var affected_conts := [and]([lng](item.mirror()), 3LL).ord_uselect(1LL).mirror().leftfetchjoin(kind).get_container().reverse().kunique().hmark(0@0);  # [i,CONT]

  if (affected_conts.count() = 0) {
    # nothing to do
    # this is actually not expected to happen, but it doesn't do any harm
    return;
  }

  # check that we're not trying to update the transient container
  if (affected_conts.uselect(0@0).count() > 0) {
    ERROR("updating transient container.\n");
  }

  # we currently cannot update collections that have a pftijah text index
  var cont_names := affected_conts.leftjoin(ws.fetch(CONT_NAME)).reverse().mirror();
  var conflict := [tj_is_indexed](cont_names).uselect(true);
  if (count(conflict) > 0) {
    ERROR("cannot update text-indexed collection %s (%s such errors).\n", reverse(conflict).fetch(0), count(confict));
  }

  # check that all containers are updatable (i.e. that none are read-only)
  if ([ttype](affected_conts.join(ws.fetch(MAP_PID))).uselect(void).count() > 0) {
    ERROR("updating read-only document.\n");
  }
  if (ws_log_active)
    ws_log(ws, "===================== START OF UPDATE");

  # set-wise unique bats, but better build the hash table on tail (head=cont != selective)
  ws.fetch(MODIFIED_NID).reverse().accbuild("hash");
  ws.fetch(ANCESTOR_NID).reverse().accbuild("hash");
  ws.fetch(MODIFIED_ATTR).reverse().accbuild("hash");
  ws.fetch(MODIFIED_PAGE).reverse().accbuild("hash");
  ws.fetch(NEW_PAGE).reverse().accbuild("hash");
  ws.fetch(DELETED_NID).reverse().accbuild("hash");
  ws.fetch(ADDED_NID).reverse().accbuild("hash");
  ws.fetch(ADDED_ATTR).reverse().accbuild("hash");

  affected_conts@batloop() {
    ws.fetch(MAP_PID_UPDATE).insert($t, ws.fetch(MAP_PID).find($t).copy().access(BAT_WRITE).key(true));
    ws.fetch(RID_SIZE_UPDATE).insert($t, new(oid,int).key(true));
    ws.fetch(RID_LEVEL_UPDATE).insert($t, new(oid,chr).key(true));
    ws.fetch(RID_KIND_UPDATE).insert($t, new(oid,chr).key(true));
    ws.fetch(RID_PROP_UPDATE).insert($t, new(oid,oid).key(true));
    ws.fetch(RID_NID_UPDATE).insert($t, new(oid,oid).key(true));
    ws.fetch(NID_RID_UPDATE).insert($t, new(oid,oid).key(true));
    ws.fetch(ATTR_QN_UPDATE).insert($t, new(oid,oid).key(true));
    ws.fetch(ATTR_PROP_UPDATE).insert($t, new(oid,oid).key(true));
    ws.fetch(ATTR_OWN_UPDATE).insert($t, new(oid,oid).key(true));
    ws.fetch(PROP_VAL_UPDATE).insert($t, new(oid,str).key(true));
    ws.fetch(PROP_TEXT_UPDATE).insert($t, new(oid,str).key(true));
    ws.fetch(PROP_COM_UPDATE).insert($t, new(oid,str).key(true));
    ws.fetch(PROP_INS_UPDATE).insert($t, new(oid,str).key(true));
    ws.fetch(PROP_TGT_UPDATE).insert($t, new(oid,str).key(true));
    ws.fetch(QN_LOC_UPDATE).insert($t, new(oid,str).key(true));
    ws.fetch(QN_URI_UPDATE).insert($t, new(oid,str).key(true));
    ws.fetch(QN_PREFIX_UPDATE).insert($t, new(oid,str).key(true));
    ws.fetch(QN_URI_LOC_UPDATE).insert($t, new(oid,str).key(true));
    ws.fetch(QN_PREFIX_URI_LOC_UPDATE).insert($t, new(oid,str).key(true));
    ws.fetch(QN_HISTOGRAM_UPDATE).insert($t, new(oid,lng).key(true));
    ws.fetch(NID_QN_INS_UPDATE).insert($t, new(oid,oid).key(true));
    ws.fetch(NID_QN_DEL_UPDATE).insert($t, new(oid,oid).key(true));
  }

  # extract the commands from the update tape 
  var update_cmd := [and]([lng](item.mirror()), 3LL).ord_uselect(0LL).mirror().leftfetchjoin(item).leftfetchjoin(int_values);

  {
    var rename_update := update_cmd.ord_uselect(UPDATE_RENAME);
    if (rename_update.count() > 0) {
      var update_node_item := [oid]([+]([lng](rename_update.mirror()), 1)).leftfetchjoin(item);
      var update_node_kind := [oid]([+]([lng](rename_update.mirror()), 1)).leftfetchjoin(kind);
      var update_rename := [oid]([+]([lng](rename_update.mirror()), 2)).leftfetchjoin(item).leftfetchjoin(str_values);

      # now select all the unique nodes (both item and kind must be
      # the same), and when there are multiple occurences, select the
      # *last*.
      # note that order doesn't matter once this is done.
      if (update_node_item.count() > 1) {
        var update_node_unique := update_node_item.CTgroup().CTderive(update_node_kind).CTmap().reverse().{max}().reverse().mirror();
        update_node_item := update_node_unique.leftjoin(update_node_item);
        update_node_kind := update_node_unique.leftjoin(update_node_kind);
        update_rename := update_node_unique.leftjoin(update_rename);
      }

      # now make the three BATs void-headed
      update_node_item := update_node_item.tmark(0@0);
      update_node_kind := update_node_kind.tmark(0@0);
      update_rename := update_rename.tmark(0@0);

      # do the work
      do_update_rename(ws, update_node_item, update_node_kind, update_rename);
    }
  }

  {
    var replace_update := update_cmd.ord_uselect(UPDATE_REPLACE);
    if (replace_update.count() > 0) {
      var update_node_item := [oid]([+]([lng](replace_update.mirror()), 1)).leftfetchjoin(item);
      var update_node_kind := [oid]([+]([lng](replace_update.mirror()), 1)).leftfetchjoin(kind);
      var update_replace := [oid]([+]([lng](replace_update.mirror()), 2)).leftfetchjoin(item).leftfetchjoin(str_values);

      # now select all the unique nodes (both item and kind must be
      # the same), and when there are multiple occurences, select the
      # *last*.
      # note that order doesn't matter once this is done.
      if (update_node_item.count() > 1) {
        var update_node_unique := update_node_item.CTgroup().CTderive(update_node_kind).CTmap().reverse().{max}().reverse().mirror();
        update_node_item := update_node_unique.leftjoin(update_node_item);
        update_node_kind := update_node_unique.leftjoin(update_node_kind);
        update_replace := update_node_unique.leftjoin(update_replace);
      }

      # now make the three BATs void-headed
      update_node_item := update_node_item.tmark(0@0);
      update_node_kind := update_node_kind.tmark(0@0);
      update_replace := update_replace.tmark(0@0);

      # do the work
      do_update_replace(ws, update_node_item, update_node_kind, update_replace);
    }
  }

  {
    var insert_update := update_cmd.ord_select(UPDATE_INSERT_FIRST, UPDATE_REPLACENODE);
    if (insert_update.count() > 0) {
      var update_node_item := [oid]([+]([lng](insert_update.mirror()), 1)).leftfetchjoin(item);
      var update_node_kind := [oid]([+]([lng](insert_update.mirror()), 1)).leftfetchjoin(kind);
      var update_insert_node_item := [oid]([+]([lng](insert_update.mirror()), 2)).leftfetchjoin(item);
      var update_insert_node_kind := [oid]([+]([lng](insert_update.mirror()), 2)).leftfetchjoin(kind);

      # UPDATE_INSERT_LAST and UPDATE_INSERT_BEFORE commands must be executed in reverse order
      var order := insert_update.ord_uselect(UPDATE_INSERT_LAST, UPDATE_INSERT_BEFORE).copy().mark(0@0);
      if (order.count() > 1) {
        update_node_item.access(BAT_WRITE).replace(order.leftjoin(order.mirror().leftjoin(update_node_item).access(BAT_WRITE).revert().tmark(0@0)));
        update_node_kind.access(BAT_WRITE).replace(order.leftjoin(order.mirror().leftjoin(update_node_kind).access(BAT_WRITE).revert().tmark(0@0)));
        update_insert_node_item.access(BAT_WRITE).replace(order.leftjoin(order.mirror().leftjoin(update_insert_node_item).access(BAT_WRITE).revert().tmark(0@0)));
        update_insert_node_kind.access(BAT_WRITE).replace(order.leftjoin(order.mirror().leftjoin(update_insert_node_kind).access(BAT_WRITE).revert().tmark(0@0)));
        insert_update.access(BAT_WRITE).replace(order.leftjoin(order.mirror().leftjoin(insert_update).access(BAT_WRITE).revert().tmark(0@0)));
      }

      # do double elimination on UPDATE_REPLACECONTENT commands
      # this is done by writing nil values in the insert_update column for eliminated rows.
      var replace := insert_update.ord_uselect(UPDATE_REPLACECONTENT).mirror();
      if (replace.count() > 1) {
        insert_update.access(BAT_WRITE).replace(replace.kdiff(replace.leftjoin(update_node_item).CTgroup().CTderive(update_node_kind).CTmap().reverse().{max}().reverse()).project(lng_nil));
      }

      # now make the five BATs void-headed
      update_node_item := update_node_item.tmark(0@0);
      update_node_kind := update_node_kind.tmark(0@0);
      update_insert_node_item := update_insert_node_item.tmark(0@0);
      update_insert_node_kind := update_insert_node_kind.tmark(0@0);
      insert_update := insert_update.tmark(0@0);

      # do the work
      do_update_insert(ws, insert_update, update_node_item, update_node_kind, update_insert_node_item, update_insert_node_kind);
    }
  }

  {
    var delete_update := update_cmd.ord_uselect(UPDATE_DELETE);
    if (delete_update.count() > 0) {
      var update_node_item := [oid]([+]([lng](delete_update.mirror()), 1)).leftfetchjoin(item);
      var update_node_kind := [oid]([+]([lng](delete_update.mirror()), 1)).leftfetchjoin(kind);

      # now select all the unique nodes (both item and kind must be
      # the same).
      if (update_node_item.count() > 1) {
        var update_node_unique := update_node_item.CTgroup().CTderive(update_node_kind).CTmap().tunique().mirror();
        update_node_item := update_node_unique.leftjoin(update_node_item);
        update_node_kind := update_node_unique.leftjoin(update_node_kind);
      }

      # now make the two BATs void-headed
      update_node_item := update_node_item.tmark(0@0);
      update_node_kind := update_node_kind.tmark(0@0);

      # do the work
      do_update_delete(ws, update_node_item, update_node_kind);
    }
  }

  var texts := ws.fetch(UPDATED_TEXT);
  #fix_consecutive_texts(ws, texts);

  # prepare for postcommit

  var deleted_nid := ws.fetch(DELETED_NID);
  # note that all modified and deleted attributes end up in MODIFIED_ATTR
  var modified_attr := ws.fetch(MODIFIED_ATTR);

  var deleted_text_nid := ws.fetch(DELETED_TEXT_NID);

  var old_attr := new(oid,bat);
  var old_text := new(oid,bat);
  var new_text := new(oid,bat);

  affected_conts@batloop() {
    var cont := $t;

    extend_protect(ws, cont);

    # first do attributes
    var nid_rid := ws.fetch(NID_RID).find(cont);
    var map_pid := ws.fetch(MAP_PID).find(cont);
    var pre_prop := ws.fetch(PRE_PROP).find(cont);
    {
      var attr_own := ws.fetch(ATTR_OWN).find(cont);
      var attr_qn := ws.fetch(ATTR_QN).find(cont);
      var attr_prop := ws.fetch(ATTR_PROP).find(cont);
      var prop_val := ws.fetch(PROP_VAL).find(cont);

      var nids := deleted_nid.reverse().uselect(cont).reverse(); # [nil,NID]

      var elemqns := nids.leftjoin(nid_rid).select(oid_nil, oid_nil).[swizzle](map_pid).leftjoin(pre_prop); # [nil,QNID]
      var attrids := nids.leftjoin(attr_own.reverse()); # [nil,ATID]
      # combine with deleted attributes (with double elimination)
      var attrs := modified_attr.reverse().uselect(cont); # [ATID,nil]
      attrids := attrids.reverse().kunion(attrs).hmark(0@0); # [j,ATID]

      var attrqns := attrids.leftjoin(attr_qn); # [j,ATQN]
      var attrvals := attrids.leftjoin(attr_prop).leftjoin(prop_val); # [j,ATVAL(str)]
      var elemnids := attrids.leftjoin(attr_own); # [j,NID]
      var elemqns := elemnids.leftjoin(nid_rid).[swizzle](map_pid).leftjoin(pre_prop); # [j,QNID]

      old_attr.insert(cont, vx_maintain(elemnids, elemqns, attrqns, attrvals));
    }

    # then do text
    {
      var prop_text := ws.fetch(PROP_TEXT).find(cont);
      var nids := deleted_text_nid.reverse().uselect(cont).hmark(0@0); # [i,NID]
      var texts := nids.leftjoin(nid_rid).select(oid_nil, oid_nil) # [i,RID]
        .[swizzle](map_pid) # [i,PRE]
        .leftjoin(pre_prop) # [i,PROP]
        .leftjoin(prop_text); # [i,TEXT]
      old_text.insert(cont, vx_maintain(nids, texts));
    }

    extend_unprotect(ws, cont);
  }

  # get containers in the order in which we must commit them ([COLL_ID,CONT])
  var cont_order := affected_conts.reverse().mirror().leftfetchjoin(ws.fetch(CONT_COLL)).reverse().sort();

  var ws_logstarttime := usec(); # this timer is used assuming a single collection per query

  var map_pid_changed_bat := new(oid, bit); # record which MAP_PID bats were updated

  # get the collection locks, re-read the master ancestor sizes, and check for conflicts (ws_precommit)
  cont_order@batloop() {
    var cont := $t;

    coll_lock_set(ws, cont, COLL_LONGLOCK, "===================== COMMIT START"); 
    var ws_logtime := usec();

    # add ancestor nids on modified pages to modified nids list and remove them from ancestor nids list
    var modified_nid := ws.fetch(MODIFIED_NID); # [cont,NID]
    var ancestor_nid := ws.fetch(ANCESTOR_NID); # [cont,NID]
    var sel_modified_page := ws.fetch(MODIFIED_PAGE).reverse().uselect(cont).hmark(0@0); # [i,PGID]
    var sel_modified_attr := ws.fetch(MODIFIED_ATTR).reverse().uselect(cont).hmark(0@0); # [i,ATID]
    var sel_ancestor_nid := ancestor_nid.reverse().uselect(cont).hmark(0@0); # [i,NID]
    var sel_ancestor_rid := sel_ancestor_nid.join(ws.fetch(NID_RID).find(cont)); # [i,RID]
    var del_ancestor_nid := new(void, oid);
    sel_modified_page@batloop() {
      var start := oid(lng($t) << REMAP_PAGE_BITS);
      var end := oid(lng(start) + REMAP_PAGE_MASK);
      var rids := sel_ancestor_rid.reverse().select(start, end).reverse();
      var nids := reverse([lng](rids.mirror().join(sel_ancestor_nid)).[>>](OID_PAGE_BITS));
      modified_nid.reverse().insert(nids.project(cont));
      del_ancestor_nid.append(sel_ancestor_nid.tintersect(rids.mirror().join(sel_ancestor_nid)));
    }
    # we change modified_nid + ancestor_nid here; this info is later needed by ws_isolate()
    reverse(ancestor_nid).deleteBuns(del_ancestor_nid.tunique().project(cont));

    # figure out whether we added or deleted pages
    var map_pid := ws.fetch(MAP_PID).find(cont);
    var map_pid_update := ws.fetch(MAP_PID_UPDATE).find(cont);
    # if lengths differ, there was a difference (he, he)
    var map_pid_changed := count(map_pid) != count(map_pid_update);
    if (not(map_pid_changed)) {
      # same length, calculate difference (using -1 for nil)
      map_pid := [ifthenelse]([isnil](map_pid),-1,[int](map_pid));
      map_pid_update := [ifthenelse]([isnil](map_pid_update),-1,[int](map_pid_update));
      # if number of 0 results differs from length of original, there was a difference
      map_pid_changed := count([-](map_pid, map_pid_update).uselect(0)) != count(map_pid);
    }
    map_pid_changed_bat.insert(cont, map_pid_changed);

    if (ws_log_active)
      ws_log(ws, "commit-reread exec" + str(ws_logtime - usec())); 

    # _ws_precommit gives an ERROR if a conflicting transaction has committed already
    lock_set(pf_short);
    var err := CATCH(_ws_precommit(ws, cont, sel_modified_page, sel_modified_attr, map_pid_changed));
    lock_unset(pf_short);
    if (not(isnil(err))) ERROR(err);
  }

  var ws_logtime := usec();

  # WAL all changes: no error means a succesful commit
  lock_set(pf_wal);
  if (ws_log_active) 
    ws_log(ws, "===================== WAL LOCKED wal_lock" + str(ws_logtime - (ws_logtime := usec()))); 
  log_trans_start(pf_logger);
  var err := CATCH(do_log_updates(ws, cont_order, map_pid_changed_bat));
  lock_unset(pf_wal);
  if (not(isnil(err))) ERROR(err);

  if (ws_log_active)
    ws_log(ws, "===================== WAL RELEASED exec" + str(ws_logtime - (ws_logtime := usec()))); 


  var added_nid := ws.fetch(ADDED_NID);
  var added_attr := ws.fetch(ADDED_ATTR);
  var added_text_nid := ws.fetch(ADDED_TEXT_NID);

  ws.seqbase(oid_nil); # indicate that we are done reading our snapshot

  # upgrade locks, isolate the masters, apply deltas to them, maintain indices, and release all coll-lock(s)
  cont_order@batloop() {
    var cont := $t;

    coll_lock_set(ws, cont, COLL_SHORTLOCK, "===================== MASTER UPDATES"); 

    # isolation: which attrs, nids and pages were modified?
    var modified_page := ws.fetch(MODIFIED_PAGE).reverse().uselect(cont).hmark(0@0); # [i,PGID]
    var modified_attr := ws.fetch(MODIFIED_ATTR).reverse().uselect(cont).hmark(0@0); # [i,ATID]
    var modified_nid := ws.fetch(MODIFIED_NID).reverse().uselect(cont).hmark(0@0); # [i,NID]
    var ancestor_nid := ws.fetch(ANCESTOR_NID).reverse().uselect(cont).hmark(0@0); # [i,NID]

    # ws_isolate isolates concurrent snapshots, before we modify the masters 
    __ws_isolate(ws, cont, modified_page, ancestor_nid, [oid]([<<](modified_nid, OID_PAGE_BITS)), modified_attr);

    if (ws_log_active)
      ws_log(ws, "commit-isolate exec" + str(ws_logtime - (ws_logtime := usec()))); 

    # apply the logged changes to the masters
    pf_assert(CATCH(do_commit_updates(ws, cont, map_pid_changed_bat.find(cont))), "master update failed (commit_updates)");

    if (ws_log_active)
      ws_log(ws, "commit-apply exec" + str(ws_logtime - (ws_logtime := usec()))); 

    var newattr;
    var nid_rid := ws.fetch(_NID_RID).find(cont);
    var rid_prop := ws.fetch(_RID_PROP).find(cont);
    {
      var attr_own := ws.fetch(_ATTR_OWN).find(cont);
      var attr_qn := ws.fetch(_ATTR_QN).find(cont);
      var attr_prop := ws.fetch(_ATTR_PROP).find(cont);
      var prop_val := ws.fetch(_PROP_VAL).find(cont);

      var nids := added_nid.reverse().uselect(cont).reverse(); # [nil,NID]

      var elemqns := nids.leftjoin(nid_rid).select(oid_nil, oid_nil).leftjoin(rid_prop); # [nil,QNID]
      var attrids := nids.leftjoin(attr_own.reverse()); # [nil,ATID]
      # combine with deleted attributes (with double elimination)
      var attrs := added_attr.reverse().uselect(cont); # [ATID,nil]
      attrids := attrids.reverse().kunion(attrs).hmark(0@0); # [j,ATID]

      var attrqns := attrids.leftjoin(attr_qn); # [j,ATQN]
      var attrvals := attrids.leftjoin(attr_prop).leftjoin(prop_val); # [j,ATVAL(str)]
      var elemnids := attrids.leftjoin(attr_own); # [j,NID]
      var elemqns := elemnids.leftjoin(nid_rid).leftjoin(rid_prop); # [j,QNID]

      newattr := vx_maintain(elemnids, elemqns, attrqns, attrvals);
    }

    var newtext;
    {
      var prop_text := ws.fetch(_PROP_TEXT).find(cont);
      var nids := added_text_nid.reverse().uselect(cont).hmark(0@0); # [i,NID]
      var texts := nids.leftjoin(nid_rid).select(oid_nil, oid_nil) # [i,RID]
        .leftjoin(rid_prop) # [i,PROP]
        .leftjoin(prop_text); # [i,TEXT]
      newtext := vx_maintain(nids, texts);
    }

    # ws_postcommit maintains the indices and releases the collection lock
    __ws_postcommit(ws, cont,
                    newattr,
                    old_attr.find(cont),
                    newtext,
                    old_text.find(cont),
                    ws.fetch(NID_QN_INS_UPDATE).find(cont).reverse(), 
                    ws.fetch(NID_QN_DEL_UPDATE).find(cont).reverse(), 
                    ws.fetch(QN_URI_UPDATE).find(cont).hmark(0@0),
                    ws.fetch(DEL_PAGE).reverse().uselect(cont).hmark(0@0));

    if (ws_log_active)
      ws_log(ws, "commit-idx exec" + str(ws_logtime - usec())); 

    coll_lock_unset(ws, cont, COLL_BOTHLOCK, "===================== COMMIT END", ws_logstarttime); 
  }
}

PROC do_commit_updates(BAT[void,bat] ws, oid cont, bit map_pid_changed) : void
{
  if (map_pid_changed)
    ws.fetch(_MAP_PID).find(cont).myupdate(ws.fetch(MAP_PID_UPDATE).find(cont));
  ws.fetch(_RID_SIZE).find(cont).myupdate(ws.fetch(RID_SIZE_UPDATE).find(cont));
  ws.fetch(_RID_LEVEL).find(cont).myupdate(ws.fetch(RID_LEVEL_UPDATE).find(cont));
  ws.fetch(_RID_KIND).find(cont).myupdate(ws.fetch(RID_KIND_UPDATE).find(cont));
  ws.fetch(_RID_PROP).find(cont).myupdate(ws.fetch(RID_PROP_UPDATE).find(cont));
  ws.fetch(_RID_NID).find(cont).myupdate(ws.fetch(RID_NID_UPDATE).find(cont));
  ws.fetch(_NID_RID).find(cont).myupdate(ws.fetch(NID_RID_UPDATE).find(cont));
  ws.fetch(_ATTR_QN).find(cont).myupdate(ws.fetch(ATTR_QN_UPDATE).find(cont));
  ws.fetch(_ATTR_PROP).find(cont).myupdate(ws.fetch(ATTR_PROP_UPDATE).find(cont));
  ws.fetch(_ATTR_OWN).find(cont).myupdate(ws.fetch(ATTR_OWN_UPDATE).find(cont));
}

PROC mylog_delta(logger l, BAT[any,any] b, str nme) : void
{
    log_delta(l, b, nme);
b.col_name(nme).print();
}

PROC do_log_updates(BAT[void,bat] ws, BAT[any,any] cont_order, BAT[oid,bit] map_pid_changed_bat) : void
{
  var ws_logtime := usec(); 

  # first do the main logging work without the collection locks
  cont_order@batloop() {
    var cont := $t;
    var rid_size := ws.fetch(_RID_SIZE).find(cont);
    var rid_level := ws.fetch(_RID_LEVEL).find(cont);
    var rid_kind := ws.fetch(_RID_KIND).find(cont);
    var rid_prop := ws.fetch(_RID_PROP).find(cont);
    var rid_nid := ws.fetch(_RID_NID).find(cont);
    if (map_pid_changed_bat.find(cont))
      log_delta(pf_logger, ws.fetch(MAP_PID_UPDATE).find(cont), ws.fetch(_MAP_PID).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(RID_SIZE_UPDATE).find(cont), rid_size.bbpname());
    log_delta(pf_logger, ws.fetch(RID_LEVEL_UPDATE).find(cont), rid_level.bbpname());
    log_delta(pf_logger, ws.fetch(RID_KIND_UPDATE).find(cont), rid_kind.bbpname());
    log_delta(pf_logger, ws.fetch(RID_PROP_UPDATE).find(cont), rid_prop.bbpname());
    log_delta(pf_logger, ws.fetch(RID_NID_UPDATE).find(cont), rid_nid.bbpname());
    log_delta(pf_logger, ws.fetch(NID_RID_UPDATE).find(cont), ws.fetch(_NID_RID).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(ATTR_QN_UPDATE).find(cont), ws.fetch(_ATTR_QN).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(ATTR_PROP_UPDATE).find(cont), ws.fetch(_ATTR_PROP).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(ATTR_OWN_UPDATE).find(cont), ws.fetch(_ATTR_OWN).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(PROP_VAL_UPDATE).find(cont), ws.fetch(_PROP_VAL).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(PROP_TEXT_UPDATE).find(cont), ws.fetch(_PROP_TEXT).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(PROP_COM_UPDATE).find(cont), ws.fetch(_PROP_COM).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(PROP_INS_UPDATE).find(cont), ws.fetch(_PROP_INS).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(PROP_TGT_UPDATE).find(cont), ws.fetch(_PROP_TGT).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(QN_LOC_UPDATE).find(cont), ws.fetch(_QN_LOC).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(QN_URI_UPDATE).find(cont), ws.fetch(_QN_URI).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(QN_PREFIX_UPDATE).find(cont), ws.fetch(_QN_PREFIX).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(QN_URI_LOC_UPDATE).find(cont), ws.fetch(_QN_URI_LOC).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(QN_PREFIX_URI_LOC_UPDATE).find(cont), ws.fetch(_QN_PREFIX_URI_LOC).find(cont).bbpname());
    log_delta(pf_logger, ws.fetch(QN_HISTOGRAM_UPDATE).find(cont), ws.fetch(_QN_HISTOGRAM).find(cont).bbpname());
    var new_page := ws.fetch(NEW_PAGE).reverse().ord_uselect(cont).sort();
    new_page@batloop() {
      var start := oid(lng($h) << REMAP_PAGE_BITS);
      var end := oid(lng(start) + REMAP_PAGE_MASK);
      log_delta(pf_logger, rid_size.reverse().select(start, end).reverse(), rid_size.bbpname());
      log_delta(pf_logger, rid_level.reverse().select(start, end).reverse(), rid_level.bbpname());
      log_delta(pf_logger, rid_kind.reverse().select(start, end).reverse(), rid_kind.bbpname());
      log_delta(pf_logger, rid_prop.reverse().select(start, end).reverse(), rid_prop.bbpname());
      log_delta(pf_logger, rid_nid.reverse().select(start, end).reverse(), rid_nid.bbpname());
    }

    if (ws_log_active)
      ws_log(ws, "commit-LOG_DELTAS exec" + str(ws_logtime - (ws_logtime := usec()))); 

    # compute the size-delta on the ancestors of modified nodes, and apply those to the recent masters (and LOG it) 
    var sel_ancestor_nid := ws.fetch(ANCESTOR_NID).reverse().uselect(cont).hmark(0@0); # [i,NID]
    var anc_rid := sel_ancestor_nid.leftjoin(ws.fetch(NID_RID).find(cont));
    # filter out ancestors that are on modified pages
    var modified_page := ws.fetch(MODIFIED_PAGE).reverse().select(cont); # [PGID,cont]
    var anc_pgid := [oid]([>>]([lng](anc_rid), REMAP_PAGE_BITS));
    var anc := anc_pgid.outerjoin(modified_page).uselect(oid_nil).mirror();
    sel_ancestor_nid := anc.leftfetchjoin(sel_ancestor_nid).tmark(0@0);
    anc_rid := anc.leftfetchjoin(anc_rid).tmark(0@0);

    var anc_oldsize := [swizzle](anc_rid, ws.fetch(MAP_PID).find(cont)).join(ws.fetch(PRE_SIZE).find(cont));
        anc_rid.access(BAT_WRITE).replace(sel_ancestor_nid.join(ws.fetch(NID_RID_UPDATE).find(cont)));
    var anc_newsize := anc_rid.leftjoin(ws.fetch(RID_SIZE_UPDATE).find(cont));
    var anc_mstsize := anc_rid.leftjoin(ws.fetch(_RID_SIZE).find(cont));
    var anc_updsize := [+]([-](anc_newsize, anc_oldsize), anc_mstsize);
    var rid_updsize := reverse(anc_rid).leftfetchjoin(anc_updsize);
    log_delta(pf_logger, rid_updsize, ws.fetch(_RID_SIZE).find(cont).bbpname()); 
    ws.fetch(RID_SIZE_UPDATE).find(cont).myupdate(rid_updsize);

    if (ws_log_active)
      ws_log(ws, "commit-LOG_SIZE exec" + str(ws_logtime - (ws_logtime := usec()))); 
  }
  log_trans_end(pf_logger); # write commit record in WAL: ==> THIS IS THE COMMIT POINT <==

  if (ws_log_active)
    ws_log(ws, "commit-LOG_TRANS_END exec" + str(ws_logtime - usec()));
}

# due to the order in which updates are executed, do_update_rename and
# do_update_replace don't have to contend with inserted pages or with
# nodes having changed their PRE value
PROC do_update_rename(bat[void, bat] ws, bat[void,oid] update_node_item, bat[void,int] update_node_kind, bat[void,str] update_rename) : void
{
  var conts := update_node_kind.get_container();
  var types := update_node_kind.get_types();
  var attrs := types.ord_uselect(ATTR);
  var elems := types.ord_uselect(ELEM);

  if (attrs.count() > 0) {
    # do rename $elem/@attr as(into) "newname"
    # renaming of attributes, i.e., the attributes get a new QN but keep their value
    attrs := attrs.mirror();
    var aitem := attrs.leftjoin(update_node_item); # [i,ATID]
    var aconts := attrs.leftjoin(conts); # [i,CONT]
    {
      var contattr := aconts.reverse().join(aitem);
      ws.fetch(MODIFIED_ATTR).insert(contattr);
      ws.fetch(ADDED_ATTR).insert(contattr);
    }
    aconts.tunique()@batloop() {
      var cont := $h;
      var list := aconts.ord_uselect(cont).mirror(); # [i,i] (attributes in current container)
      var aqn := list.leftjoin(update_rename); # [i,newQName]
      var aqnid := find_qn_bulk(ws, cont, [+](NS_ACCEL_SEP + NS_ACCEL_SEP, aqn), true); # [i,newQNID]
      var attr_qn_updates := ws.fetch(ATTR_QN_UPDATE).find(cont);
      # list.leftjoin(aitem) [i,ATID]
      # list.leftjoin(aitem).reverse() [ATID,i]
      # list.leftjoin(aitem).reverse().leftjoin(aqnid) [ATID,QNID]
      attr_qn_updates.insert(list.leftjoin(aitem).reverse().leftjoin(aqnid));
    }
  }

  if (elems.count() > 0) {
    # do rename $elem as(into) "newname"
    # renaming an element or processing instruction
    elems := elems.mirror();
    var eitem := elems.leftjoin(update_node_item); # [i,PRE]
    var econts := elems.leftjoin(conts); # [i,CONT]
    var modified_nid := ws.fetch(MODIFIED_NID);
    var deleted_nid := ws.fetch(DELETED_NID);
    var added_nid := ws.fetch(ADDED_NID);
    econts.tunique()@batloop() {
      var cont := $h;

      var pre_kind := ws.fetch(PRE_KIND).find(cont);
      var rid_prop_update := ws.fetch(RID_PROP_UPDATE).find(cont);
      var pid_map := ws.fetch(PID_MAP).find(cont);

      var list := econts.ord_uselect(cont).mirror(); # [i,i] (nodes in current container)
      var epres := list.leftjoin(eitem); # [i,PRE]
      var ekinds := epres.leftfetchjoin(pre_kind); # [i,KIND]
      var esplit := ekinds.splitkind();
      var eelem := esplit.fetch(int(ELEMENT)).mirror(); # [i,i] element nodes in current container
      var epi := esplit.fetch(int(PI)).mirror(); # [i,i] processing instruction nodes in current container
      if ((esplit.fetch(int(TEXT)).count() > 0) or (esplit.fetch(int(COMMENT)).count() > 0) or (esplit.fetch(int(DOCUMENT)).count() > 0)) {
        ERROR("node must be of type ELEMENT, PI, or ATTRIBUTE\n");
      }
      var erids := [swizzle](epres, pid_map);
      var pre_nid := ws.fetch(PRE_NID).find(cont);
      {
        modified_nid.reverse().accbuild("hash");
        modified_nid.insert([lng](epres.join(pre_nid).reverse().project(cont).reverse()).[>>](OID_PAGE_BITS));
      }
      {
        var modified_page := ws.fetch(MODIFIED_PAGE);
        modified_page.insert(reverse(project(tunique([oid]([>>]([lng](erids), REMAP_PAGE_BITS))), cont)));
      }
      if (eelem.count() > 0) {
        var eqn := eelem.leftjoin(update_rename); # [i,newQName]
        var eqnid := find_qn_bulk(ws, cont, [+](NS_ACCEL_SEP + NS_ACCEL_SEP, eqn), true); # [i,newQNID]
        {
          var eelempre := eelem.leftjoin(epres); # [i,PRE]
          var eelemnid := eelempre.leftjoin(pre_nid); # [i,NID]
          var pre_prop := ws.fetch(PRE_PROP).find(cont);
          var eelemqnold := eelempre.leftjoin(pre_prop); # [i,oldQNID]
          var eoldnidqn := eelemnid.reverse().join(eelemqnold); # [NID,oldQNID]
          var enewnidqn := eelemnid.reverse().join(eqnid); # [NID,newQNID]
          ws.fetch(NID_QN_INS_UPDATE).find(cont).insert(enewnidqn);
          ws.fetch(NID_QN_DEL_UPDATE).find(cont).insert(eoldnidqn);
          var contnid := eelemnid.reverse().project(cont).reverse(); # [cont,NID]
          deleted_nid.insert(contnid);
          added_nid.insert(contnid);
        }
        var upd := eelem.leftjoin(erids).reverse().join(eqnid); # [RID,QNID]
        rid_prop_update.insert(upd);
      }
      if (epi.count() > 0) { 
        var prop_ins := ws.fetch(_PROP_INS).find(cont);
        # rename comes before replace, so we don't have to deal with updated properties
        var pre_prop := ws.fetch(PRE_PROP).find(cont);
        var etgt := epi.leftjoin(update_rename); # [i,tgt]
        var eins := epi.leftjoin(epres).leftjoin(pre_prop);
        var evalid := add_pi_bulk(ws, cont, etgt, eins.leftjoin(prop_ins)); # [i,PROPID]
        var upd := epi.leftjoin(erids).reverse().join(evalid); # [RID,PROPID]
        rid_prop_update.insert(upd);
      }
    }
  }
}

PROC do_update_replace(bat[void, bat] ws, bat[void,oid] update_node_item, bat[void,int] update_node_kind, bat[void,str] update_replace) : void
{
  var conts := update_node_kind.get_container();
  var types := update_node_kind.get_types();
  var attrs := types.ord_uselect(ATTR);
  var elems := types.ord_uselect(ELEM);
  if ((attrs.count() + elems.count()) != types.count()) {
    ERROR("node must be of type ELEMENT or ATTRIBUTE\n");
  }

  if (attrs.count() > 0) {
    # do replace value of $elem/@attr with "new value"
    # replacing the value of an attribute
    attrs := attrs.mirror(); # [i,i]
    var aitem := attrs.leftjoin(update_node_item); # [i,ATID]
    var aconts := attrs.leftjoin(conts); # [i,CONT]
    {
      var contattr := aconts.reverse().join(aitem);
      ws.fetch(MODIFIED_ATTR).insert(contattr);
      ws.fetch(ADDED_ATTR).insert(contattr);
    }
    aconts.tunique()@batloop() {
      var cont := $h;
      var list := aconts.ord_uselect(cont).mirror(); # [i,i]
      var aval := list.leftjoin(update_replace); # [i,update_replace]
      var avalid := add_string_bulk(ws, cont, _PROP_VAL, PROP_VAL_UPDATE, aval, true);
      var attr_prop_updates := ws.fetch(ATTR_PROP_UPDATE).find(cont);
      attr_prop_updates.insert(list.leftjoin(aitem).reverse().leftjoin(avalid));
    }
  }

  if (elems.count() > 0) {
    elems := elems.mirror(); # [i,i]
    var eitem := elems.leftjoin(update_node_item); # [i.PRE]
    var econts := elems.leftjoin(conts); # [i,CONT]
    var modified_nid := ws.fetch(MODIFIED_NID);
    var deleted_text_nid := ws.fetch(DELETED_TEXT_NID);
    var added_text_nid := ws.fetch(ADDED_TEXT_NID);
    econts.tunique()@batloop() {
      var cont := $h;

      var pre_kind := ws.fetch(PRE_KIND).find(cont);
      var rid_prop_update := ws.fetch(RID_PROP_UPDATE).find(cont);
      var pre_nid := ws.fetch(PRE_NID).find(cont);
      var pid_map := ws.fetch(PID_MAP).find(cont);

      var list := econts.ord_uselect(cont).mirror(); # [i,i] current container
      var epres := list.leftjoin(eitem); # [i,PRE]
      var ekinds := epres.leftfetchjoin(pre_kind); # [i,KIND]
      var esplit := ekinds.splitkind();
      var etext := esplit.fetch(int(TEXT)).mirror(); # [i,i] text node in current container
      var ecomment := esplit.fetch(int(COMMENT)).mirror(); # [i,i] comment node in current container
      var epi := esplit.fetch(int(PI)).mirror(); # [i,i] processing instruction node in current container
      if ((esplit.fetch(int(ELEMENT)).count() > 0) or (esplit.fetch(int(DOCUMENT)).count() > 0)) {
        ERROR("node must be of type TEXT, COMMENT, PI, or ATTRIBUTE\n");
      }
      {
        modified_nid.reverse().accbuild("hash");
        modified_nid.insert([lng](epres.join(pre_nid).reverse().project(cont).reverse()).[>>](OID_PAGE_BITS));
      }
      var erids := [swizzle](epres, pid_map); # [i,RID]
      {
        var modified_page := ws.fetch(MODIFIED_PAGE);
        modified_page.insert(reverse(project(tunique([oid]([>>]([lng](erids), REMAP_PAGE_BITS))), cont)));
      }
      if (etext.count() > 0) {
        var elval := etext.leftjoin(update_replace); # [i,update_replace]
        var elvalid := add_string_bulk(ws, cont, _PROP_TEXT, PROP_TEXT_UPDATE, elval, true); # [i,PROPID]
        var upd := etext.leftjoin(erids).reverse().join(elvalid); # [RID,PROPID]
        rid_prop_update.insert(upd);
        var contnid := etext.leftjoin(epres).leftjoin(pre_nid).reverse().project(cont).reverse(); # [cont,NID]
        deleted_text_nid.insert(contnid);
        added_text_nid.insert(contnid);
      }
      if (ecomment.count() > 0) {
        var elval := ecomment.leftjoin(update_replace); # [i,update_replace]
        var elvalid := add_string_bulk(ws, cont, _PROP_COM, PROP_COM_UPDATE, elval, true); # [i,PROPID]
        var upd := ecomment.leftjoin(erids).reverse().join(elvalid); # [RID,PROPID]
        rid_prop_update.insert(upd);
      }
      if (epi.count() > 0) {
        var prop_tgt := ws.fetch(_PROP_TGT).find(cont); 
        # replace comes after rename: deal with renamed processing instruction targets
        var props := epi.leftjoin(erids).leftjoin(rid_prop_update); # [i,old PROPID]
        if (props.count() > 0) {
          var eins := props.mirror().leftjoin(update_replace); # [i,ins]
          var elvalid := add_pi_bulk(ws, cont, props.leftjoin(prop_tgt), eins); # [i,new PROPID]
          var upd := epi.leftjoin(erids).reverse().join(elvalid); # [RID,PROPID]
          rid_prop_update.replace(upd, true);
          epi := epi.kdiff(props); # only do the remaining ones
        }
        if (epi.count() > 0) {
          var pre_prop := ws.fetch(PRE_PROP).find(cont);
          var eins := epi.leftjoin(update_replace); # [i,ins]
          var etgt := epi.leftjoin(epres).leftjoin(pre_prop);
          var elvalid := add_pi_bulk(ws, cont, etgt.leftjoin(prop_tgt), eins); # [i,PROPID]
          var upd := epi.leftjoin(erids).reverse().join(elvalid); # [RID,PROPID]
          rid_prop_update.insert(upd);
        }
      }
    }
  }
}

PROC do_update_insert(bat[void, bat] ws, bat[void,lng] insert_update, bat[void,oid] update_node_item, bat[void,int] update_node_kind, bat[void,oid] update_insert_node_item, bat[void,int] update_insert_node_kind) : void
{
  # extract container ID from update_insert_node_kind
  var batdoccont := update_node_kind.get_container();
  var batcont := update_insert_node_kind.get_container();

  # separate out attribute
  var types := update_insert_node_kind.get_types();
  var attributes := types.ord_uselect(ATTR);
  var elements := types.ord_uselect(ELEM);

  if ((attributes.count() + elements.count()) != types.count()) {
    ERROR("node must be of type ELEMENT or ATTRIBUTE\n");
  }

  if (attributes.count() > 0) {
    # insert attributes into node
    attributes := attributes.mirror(); # [i.i]
    var attrconts := attributes.leftjoin(batdoccont);
    attrconts.tunique()@batloop() {
      var cont := $h;

      var attr_prop_update := ws.fetch(ATTR_PROP_UPDATE).find(cont);
      var attr_own_update := ws.fetch(ATTR_OWN_UPDATE).find(cont);
      var attr_qn_update := ws.fetch(ATTR_QN_UPDATE).find(cont);
      var attr_qn := ws.fetch(ATTR_QN).find(cont);

      var list := attrconts.ord_uselect(cont).mirror(); # [i,i]
      var aconts := list.leftjoin(batcont); # [i,ATTR_CONT]
      var aitems := list.leftjoin(update_insert_node_item); # [i,ATTR_ATTRID]

      var mapping := aitems.mark(0@0); # [i,j] mposjoin wants dense heads, so add a mapping
      var taitems := aitems.tmark(0@0); # [j,ATTR_ATTRID]
      var taconts := aconts.tmark(0@0); # [j,ATTR_CONT]

      var aqns := mposjoin(taitems, taconts, ws.fetch(ATTR_QN)); # [j,ATTR_QNID]
      var aqncont := mposjoin(taitems, taconts, ws.fetch(ATTR_CONT)); # [j,ATTR_QN_CONT]
      var aprefuriloc := mposjoin(aqns, aqncont, ws.fetch(QN_PREFIX_URI_LOC)); # [j,ATTR_PREFIX_URI_LOC]
      var aqnid := find_qn_bulk(ws, cont, aprefuriloc, true); # [j,DOC_QNID]
      aqnid := mapping.leftjoin(aqnid); # [i,DOC_QNID] map back to original numbering

      var aprops := mposjoin(taitems, taconts, ws.fetch(ATTR_PROP)); # [j,ATTR_PROPID]
      var avals := mposjoin(aprops, taconts, ws.fetch(PROP_VAL)); # [j,ATTR_VAL]
      var avalid := add_string_bulk(ws, cont, _PROP_VAL, PROP_VAL_UPDATE, avals, true); # [j,DOC_VALID]
      avalid := mapping.leftjoin(avalid); # [i,DOC_VALID]

      var aownid := list.leftjoin(update_node_item).leftjoin(ws.fetch(PRE_NID).find(cont)); # [i,DOC_NID]

      # check whether attribute already occurs on element and if so, raise error
      var aownunique := aownid.tunique().mirror(); # [DOC_NID,DOC_NID] unique owners of to-be-inserted attributes
      # get the (updated) set of attributes that belong to affected owners
      var owners := get_attr_own(ws, aownunique, cont) # [DOC_NID,DOC_ATID]
        .reverse() # [DOC_ATID,DOC_NID]
        .kdiff(attr_own_update) # [DOC_ATID,DOC_NID]
        .access(BAT_WRITE).insert(attr_own_update.join(aownunique));
      # for those attributes, get the (updated) QNIDs
      var qnames := owners.mirror().leftjoin(attr_qn).kdiff(attr_qn_update) # [DOC_ATID,DOC_QNID]
        .copy().access(BAT_WRITE).insert(owners.mirror().leftjoin(attr_qn_update));
      # compbine the two
      var oldownqn := owners.reverse().join(qnames); # [DOC_NID,DOC_QNID]
      # same for the new combos
      var newownqn := aownid.reverse().join(aqnid); # [DOC_NID,newDOC_QNID]
      # intersection must be empty
      if (sintersect(oldownqn, newownqn).count() > 0) {
        ERROR("inserted attribute already present on element: use replace value\n");
      }

      # figure out an ID for the new attribute
      var attrid := add_attr_bulk(ws, cont, list.project(oid_nil));
      attr_own_update.insert(aownid.tmark(attrid));
      attr_qn_update.insert(aqnid.tmark(attrid));
      attr_prop_update.insert(avalid.tmark(attrid));
      var newattrs := list.hmark(attrid).project(cont).reverse(); # [cont,ATID]
      ws.fetch(ADDED_ATTR).insert(newattrs);
    }
  }

  if (elements.count() > 0) {
    # insert new nodes
    elements := elements.mirror(); # [i,i]
    var modified_nid := ws.fetch(MODIFIED_NID);
    var ancestor_nid := ws.fetch(ANCESTOR_NID);
    var new_page := ws.fetch(NEW_PAGE);
    var added_nid := ws.fetch(ADDED_NID);
    var added_text_nid := ws.fetch(ADDED_TEXT_NID);
    # one at a time
    elements.leftjoin(update_insert_node_item)@batloop() {
#       if (debug) printf("\nstart of loop for argument %d\n", int($h));
      # insertcont - container from which to insert an element
      # insertitem - item to insert
      var idx := $h; # to ease debugging
      var insertitem := $t;
      var insertcont := batcont.fetch(idx);

      if (not(isnil(ws.fetch(PRE_KIND).find(insertcont).find(insertitem)))) { # do not insert holes
        # insertsize - size of item to be inserted
        # doccont - container of document-to-be-modified
        var insertsize := ws.fetch(PRE_SIZE).find(insertcont).find(insertitem) + 1;
        var doccont := batdoccont.fetch(idx);
        var map_pid := ws.fetch(MAP_PID).find(doccont);
        var map_pid_update := ws.fetch(MAP_PID_UPDATE).find(doccont);

        # figure out where to insert the new element
        # the position is identified using two values: the ID of the node
        # *after* which the element is to be inserted, and the level at which
        # it is to be inserted.
        var docinsertafter_oldpre := update_node_item.fetch(idx);
        var pre_nid := ws.fetch(PRE_NID).find(doccont);
        var pre_kind := ws.fetch(PRE_KIND).find(doccont);
        if (pre_kind.find(docinsertafter_oldpre) >= DOCUMENT) {
          ERROR("cannot insert into a document node\n");
        }
        var nid_rid := ws.fetch(NID_RID).find(doccont);
        var nid_rid_update := ws.fetch(NID_RID_UPDATE).find(doccont);
        var docinsertafter_rid := findupdate(nid_rid, nid_rid_update, pre_nid.find(docinsertafter_oldpre));
        var docinsertlevel := ws.fetch(PRE_LEVEL).find(doccont).find(docinsertafter_oldpre);
        var docinsertcmd := insert_update.fetch(idx);
        if (not(isnil(docinsertcmd)) and not(isnil(docinsertafter_rid))) {
          if (docinsertcmd = lng(UPDATE_INSERT_FIRST)) {
            docinsertlevel :+= chr(1);
          } else if (docinsertcmd = lng(UPDATE_INSERT_LAST)) {
            docinsertlevel :+= chr(1);
            docinsertafter_oldpre := oid(lng(docinsertafter_oldpre) + lng(ws.fetch(PRE_SIZE).find(doccont).find(docinsertafter_oldpre)));
          } else if (docinsertcmd = lng(UPDATE_INSERT_BEFORE)) {
            docinsertafter_oldpre := oid(lng(docinsertafter_oldpre) - 1LL);
          } else if (docinsertcmd = lng(UPDATE_INSERT_AFTER)) {
            docinsertafter_oldpre := oid(lng(docinsertafter_oldpre) + lng(ws.fetch(PRE_SIZE).find(doccont).find(docinsertafter_oldpre)));
          } else if (docinsertcmd = lng(UPDATE_REPLACENODE)) {
            docinsertafter_oldpre := oid(lng(docinsertafter_oldpre) - 1LL);
          } else if (docinsertcmd = lng(UPDATE_REPLACECONTENT)) {
            docinsertlevel :+= chr(1);
          }
          var pre_size := ws.fetch(PRE_SIZE).find(doccont);
          var rid_size := ws.fetch(_RID_SIZE).find(doccont);
          var rid_size_update := ws.fetch(RID_SIZE_UPDATE).find(doccont);
          var pre_level := ws.fetch(PRE_LEVEL).find(doccont);
          var rid_level := ws.fetch(_RID_LEVEL).find(doccont);
          var rid_level_update := ws.fetch(RID_LEVEL_UPDATE).find(doccont);
          var pre_kind := ws.fetch(PRE_KIND).find(doccont);
          var rid_kind := ws.fetch(_RID_KIND).find(doccont);
          var rid_kind_update := ws.fetch(RID_KIND_UPDATE).find(doccont);
          var pre_prop := ws.fetch(PRE_PROP).find(doccont);
          var rid_prop := ws.fetch(_RID_PROP).find(doccont);
          var rid_prop_update := ws.fetch(RID_PROP_UPDATE).find(doccont);
          var rid_nid := ws.fetch(_RID_NID).find(doccont);
          var rid_nid_update := ws.fetch(RID_NID_UPDATE).find(doccont);

          var docsize;            # current size of document
          # if the size of item 0 was changed, used the new size, else use the original size
          # we assume that page 0 on which item 0 is located already exists...
          {
            var root_rid := antiswizzle(0@0, map_pid_update);
            if (rid_size_update.exist(root_rid)) {
              docsize := rid_size_update.find(root_rid);
            } else {
              docsize := pre_size.find(0@0);
            }
            docsize :+= 1;        # count element as well (not just size of descendants)
          }

          {
            # docinsertafter_oldpre may point to a hole, so back track to a non-hole
            # item; this uses the original (unmodified) document
            var cur := docinsertafter_oldpre;
            var ret := pre_kind.find(cur);
            while (isnil(ret)) {
              # look for non-empty nodes in semi-bulk fashion
              var lim := cur;
              cur := oid(max(0LL, lng(cur) - lng(REMAP_PAGE_SIZE)));
              ret := pre_kind.reverse().ord_select(cur, lim).reverse().ord_select(chr_nil,chr_nil).reverse().max();
              docinsertafter_oldpre := ret;
            }
          }

          # docinsertafter_oldpre uses the "old" numbering scheme (i.e. the unmodified
          # document), translate to the modified document numbering
          # also recalculate docinsertafter_rid since docinsertafter_oldpre may have changed
          docinsertafter_rid := findupdate(nid_rid, nid_rid_update, pre_nid.find(docinsertafter_oldpre));
          var docinsertafter_newpre := swizzle(docinsertafter_rid, map_pid_update);
          var docinsertbefore_newpre := oid(lng(docinsertafter_newpre) + 1);
          var docinsertbefore_rid := antiswizzle(docinsertbefore_newpre, map_pid_update);

          if (docinsertcmd = lng(UPDATE_REPLACECONTENT)) {
            var size;
            extend_protect(ws, doccont);
            var pageid := oid(lng(docinsertbefore_rid) >> REMAP_PAGE_BITS);
            var isoldpage := false;
            if (map_pid.exist(pageid)) {
              isoldpage := not(isnil(map_pid.find(pageid)));
            }
            if (isoldpage) {
              if (rid_size_update.exist(docinsertafter_rid)) {
                size := rid_size_update.find(docinsertafter_rid);
              } else {
                size := pre_size.find(docinsertafter_oldpre);
              }
            } else {
              size := rid_size.find(docinsertafter_rid);
            }
            if (size > 0) {
              do_delete_nodes(ws, doccont, oid(lng(docinsertafter_newpre) + 1), size - 1);
            }
            extend_unprotect(ws, doccont);
          }
          if (docinsertcmd = lng(UPDATE_REPLACENODE)) {
            var oldpre := update_node_item.fetch(idx); # the node to be replaced (i.e. deleted)
            var newrid := findupdate(nid_rid, nid_rid_update, pre_nid.find(oldpre));
            var newpre := swizzle(newrid, map_pid_update);
            var isoldpg := false;
            var pgid := map_pid_update.reverse().find(oid(lng(docinsertbefore_newpre) >> REMAP_PAGE_BITS));
            if (map_pid.exist(pgid)) {
              isoldpg := not(isnil(map_pid.find(pgid)));
            }
            var size;
            extend_protect(ws, doccont);
            if (isoldpg) {
              if (rid_size_update.exist(newrid)) {
                size := rid_size_update.find(newrid);
              } else {
                size := pre_size.find(oldpre);
              }
            } else {
              size := rid_size.find(newrid);
            }
            if (size >= 0) {
              do_delete_nodes(ws, doccont, newpre, size);
            }
            extend_unprotect(ws, doccont);
          }

          # figure out the number of holes on the page on which the new
          # element is to be inserted (we use the PRE_KIND table for this,
          # holes are indicated with NIL)
          var holeatend := 0;   # size of hole at end of page
          var holeatstart := 0; # size of hole at start of next page
          if (int(docinsertbefore_newpre) < docsize) {
            var rid_kind_page;
            # the start of the page on which the new element is to be inserted (the page exists)
            var pageno := oid(lng(docinsertbefore_newpre) >> REMAP_PAGE_BITS);
            # the physical page id
            var pageid := map_pid_update.reverse().find(pageno);
            # the rid of the first element on the page
            var pagebase := oid(lng(pageid) << REMAP_PAGE_BITS);
            var pagelast := oid(lng(pagebase) + REMAP_PAGE_MASK);
            var isoldpage := false;
            if (map_pid.exist(pageid)) {
              isoldpage := not(isnil(map_pid.find(pageid)));
            }

            if (isoldpage) {
              rid_kind_page := pre_kind.reverse().select(swizzle(pagebase, map_pid), swizzle(pagelast, map_pid)).reverse().seqbase(pagebase);
              var rid_kind_page_update := rid_kind_update.reverse().select(pagebase, pagelast).reverse();
              rid_kind_page := rid_kind_page.copy().access(BAT_WRITE).key(true).myupdate(rid_kind_page_update);
              # record that this page gets changed (new pages do not need to be recorded)
              ws.fetch(MODIFIED_PAGE).insert(doccont, pageid);
            } else {
              rid_kind_page := rid_kind.reverse().select(pagebase, pagelast).reverse();
            }
            {
              var rid_kind_page_used := rid_kind_page.tmark(0@0).uselect(chr_nil, chr_nil);
              if (rid_kind_page_used.count() > 0) {
                holeatend := int(REMAP_PAGE_MASK - lng(max(reverse(rid_kind_page_used))));
              } else {
                holeatend := int(REMAP_PAGE_SIZE);
              }
            }
            # if we are inserting at the position of the hole at the end
            # of the page and there is not enough space, also look at any
            # hole at the start of the next
            if ((((int(docinsertbefore_rid) + holeatend) and REMAP_PAGE_BITS) = 0) and (insertsize > holeatend)) {
              # not enough space on the current page, see if there is space at the
              # start of the next
              var nxtpgno := oid(lng(pageno) + 1);
              if ((int(nxtpgno) << REMAP_PAGE_BITS) < docsize) {
                var nxtpgid := map_pid_update.reverse().find(nxtpgno);
                var nxtisoldpg := false;
                if (map_pid.exist(nxtpgid)) {
                  nxtisoldpg := not(isnil(map_pid.find(nxtpgid)));
                }
                var sz;
                if (nxtisoldpg) {
                  if (rid_size_update.exist(oid(lng(nxtpgid) << REMAP_PAGE_BITS))) {
                    sz := rid_size_update.find(oid(lng(nxtpgid) << REMAP_PAGE_BITS));
                  } else {
                    sz := pre_size.find(oid(lng(nxtpgno) << REMAP_PAGE_BITS));
                  }
                } else {
                  sz := rid_size.find(oid(lng(nxtpgid) << REMAP_PAGE_BITS));
                }
                if (isnil(niland(sz, int_nil))) {
                  holeatstart := niland(sz, INT_MAX) + 1;
                }
              }
            }
          } else if (int(docinsertbefore_newpre) = docsize) {
            # special case: insert at end of document, so the hole is the rest of the page
            holeatend := int((REMAP_PAGE_SIZE - (lng(docsize) and REMAP_PAGE_MASK)) and REMAP_PAGE_MASK);
          }
          if (insertsize > holeatend + holeatstart) {
            # not enough space on the current page and the start of the next page

            # datasize is the size of the data after the insertpoint and before the hole at the end
            var datasize := int((REMAP_PAGE_SIZE - (lng(docinsertbefore_newpre) and REMAP_PAGE_MASK)) - holeatend);

            # number of new pages needed (we need to insert insertsize, we
            # have holeatend available; round up to whole number of pages)
            var npages := ((insertsize - holeatend) + int(REMAP_PAGE_MASK)) >> REMAP_PAGE_BITS;
            # the size of the hole we are going to insert
            # we move the bit after the insert point to the last inserted page
            var shiftsize;

            var newpages := new(void, oid, npages);
            var lastpage;
            var i := 0;
            while (i < npages) {
              lastpage := ws_newpage(ws, doccont);
              newpages.append(lastpage);
              i :+= 1;
            }
            new_page.insert(newpages.reverse().project(doccont).reverse());
            var cpstart_rid, cpsize, cpwhere_rid, newholeatend;
            # insert new pages before current if there is no hole at
            # the end, we need to insert into the first half of the
            # page, and we are not inserting on the first page
            var pageno := oid(lng(docinsertbefore_newpre) >> REMAP_PAGE_BITS);
            var isoldpage := false;
            # the physical page id
            var pageid;
            if (int(docinsertbefore_newpre) < docsize) {
              # inserting into existing page, figure out whether it is an old one
              pageid := map_pid_update.reverse().find(pageno);
              if (map_pid.exist(pageid)) {
                isoldpage := not(isnil(map_pid.find(pageid)));
              }
            }
            if ((holeatend = 0) and (datasize > int(REMAP_PAGE_SIZE / 2)) and (pageno > 0@0)) {
              # insert new pages before current
              newpages := newpages.seqbase(pageno);
              map_pid_update.replace([oid]([+]([int](map_pid_update.select(pageno, oid_nil, true, true)), npages)), true);
              map_pid_update.myupdate(newpages.reverse());
              docinsertafter_rid := antiswizzle(docinsertafter_newpre, map_pid_update);
              docinsertbefore_rid := antiswizzle(docinsertbefore_newpre, map_pid_update);
              # we need to copy the initial part of the current page to the first new page
              if (int(docinsertbefore_newpre) < docsize) {
                cpstart_rid := oid(lng(pageid) << REMAP_PAGE_BITS);
                cpsize := int(REMAP_PAGE_SIZE) - datasize;
              } else {
                # we are adding to the end of the document: nothing to copy
                cpstart_rid := 0@0;
                cpsize := 0;
              }
              cpwhere_rid := oid(lng(newpages.fetch(0)) << REMAP_PAGE_BITS);
              newholeatend := holeatend;
              shiftsize := npages << REMAP_PAGE_BITS;
            } else {
              # insert new pages after current
              newpages := newpages.seqbase(oid(lng(pageno) + 1));
              map_pid_update.replace([oid]([+]([int](map_pid_update.select(pageno, oid_nil, false, true)), npages)), true);
              map_pid_update.myupdate(newpages.reverse());
              # we need to copy the data at the end to the last new page
              cpstart_rid := docinsertbefore_rid;
              cpsize := datasize;
              # start with shifting insertsize
              var cpwhere_pre := oid(int(docinsertbefore_newpre) + insertsize);
              var laststart_pre := swizzle(oid(lng(lastpage) << REMAP_PAGE_BITS), map_pid_update);
              # if this ends up on last-but-one page, move up to start of last page
              if (cpwhere_pre < laststart_pre) {
                cpwhere_pre := laststart_pre;
              }
              # calculate how much we shift the data
              shiftsize := int(cpwhere_pre) - int(docinsertbefore_newpre);
              # convert to RID
              cpwhere_rid := antiswizzle(cpwhere_pre, map_pid_update);

              var pgstart := oid((int(cpwhere_rid) >> REMAP_PAGE_BITS) << REMAP_PAGE_BITS);
              if (cpwhere_rid > pgstart) {
                # fix up hole at start of last page since it does not
                # extend to the end of the page anymore (this will be
                # overwritten again)
                extend_protect(ws, doccont);
                var update_hole := rid_size.reverse().select(pgstart, cpwhere_rid, true, false).reverse();
                update_hole := [nilor]([-](update_hole.project(int(cpwhere_rid) - 1), [int](update_hole.mirror())), int_nil);
                rid_size.replace(update_hole, true); 
                extend_unprotect(ws, doccont);
              }
              newholeatend := int(REMAP_PAGE_SIZE) - ((int(cpwhere_rid) + cpsize) and int(REMAP_PAGE_MASK));
            }

            # move data to inserted page
            if (cpsize > 0) {
              var rid_size_data;
              var rid_level_data;
              var rid_kind_data;
              var rid_prop_data;
              var rid_nid_data;
              if (isoldpage) {
                var cpend_rid := oid((lng(cpstart_rid) + cpsize) - 1);
                var cpstart_pre := swizzle(cpstart_rid, map_pid);
                var cpend_pre := oid((lng(cpstart_pre) + cpsize) - 1);
                rid_size_data := pre_size.reverse().select(cpstart_pre, cpend_pre).reverse().seqbase(cpstart_rid);
                rid_level_data := pre_level.reverse().select(cpstart_pre, cpend_pre).reverse().seqbase(cpstart_rid);
                rid_kind_data := pre_kind.reverse().select(cpstart_pre, cpend_pre).reverse().seqbase(cpstart_rid);
                rid_prop_data := pre_prop.reverse().select(cpstart_pre, cpend_pre).reverse().seqbase(cpstart_rid);
                rid_nid_data := pre_nid.reverse().select(cpstart_pre, cpend_pre).reverse().seqbase(cpstart_rid);
                # update slice with replacement values
                var rid_level_update_data := rid_level_update.reverse().select(cpstart_rid, cpend_rid).reverse();
                var rid_kind_update_data := rid_kind_update.reverse().select(cpstart_rid, cpend_rid).reverse();
                var rid_prop_update_data := rid_prop_update.reverse().select(cpstart_rid, cpend_rid).reverse();
                var rid_nid_update_data := rid_nid_update.reverse().select(cpstart_rid, cpend_rid).reverse();
                var rid_size_update_data := rid_size_update.reverse().select(cpstart_rid, cpend_rid).reverse();
                rid_level_data := rid_level_data.copy().access(BAT_WRITE).key(true).myupdate(rid_level_update_data);
                rid_kind_data := rid_kind_data.copy().access(BAT_WRITE).key(true).myupdate(rid_kind_update_data);
                rid_prop_data := rid_prop_data.copy().access(BAT_WRITE).key(true).myupdate(rid_prop_update_data);
                rid_nid_data := rid_nid_data.copy().access(BAT_WRITE).key(true).myupdate(rid_nid_update_data);
                rid_size_data := rid_size_data.copy().access(BAT_WRITE).key(true).myupdate(rid_size_update_data);
                # all these moved nodes are modified
                modified_nid.reverse().accbuild("hash");
                modified_nid.insert([lng](rid_nid_data.select(oid_nil, oid_nil).reverse().project(doccont).reverse()).[>>](OID_PAGE_BITS));
              } else {
                var cpend_rid := oid((lng(cpstart_rid) + cpsize) - 1);
                extend_protect(ws, doccont);
                rid_size_data := rid_size.reverse().select(cpstart_rid, cpend_rid).reverse().copy();
                rid_level_data := rid_level.reverse().select(cpstart_rid, cpend_rid).reverse().copy();
                rid_kind_data := rid_kind.reverse().select(cpstart_rid, cpend_rid).reverse().copy();
                rid_prop_data := rid_prop.reverse().select(cpstart_rid, cpend_rid).reverse().copy();
                rid_nid_data := rid_nid.reverse().select(cpstart_rid, cpend_rid).reverse().copy();
                extend_unprotect(ws, doccont);
                # nodes on a new page have already been added to modified_nid table
              }
              if (holeatend != newholeatend) {
                # Adjust sizes of moved data that point beyond the end of it.
                var rid_end_data := [+]([int](rid_size_data.mirror()), [niland](rid_size_data, INT_MAX));
                var past_end := rid_end_data.uselect(int(cpstart_rid) + cpsize, int_nil);
                rid_size_data.replace([nilplus](past_end.mirror().join(rid_size_data), newholeatend - holeatend), true);
              }
              extend_protect(ws, doccont);
              rid_size.replace(rid_size_data.tmark(cpwhere_rid), true);
              rid_level.replace(rid_level_data.tmark(cpwhere_rid), true);
              rid_kind.replace(rid_kind_data.tmark(cpwhere_rid), true);
              rid_prop.replace(rid_prop_data.tmark(cpwhere_rid), true);
              rid_nid.replace(rid_nid_data.tmark(cpwhere_rid), true);
              extend_unprotect(ws, doccont);

              var nid_rid_data := rid_nid_data.tmark(cpwhere_rid).select(oid_nil, oid_nil).reverse();
              nid_rid_update.myupdate(nid_rid_data);

              # overwrite the just moved data with a hole
              rid_size_data := [nilor]([-](rid_size_data.project((int(cpstart_rid) + cpsize) - 1), [int](rid_size_data.mirror())), int_nil);
              rid_level_data := rid_level_data.project(cast(nil, rid_level.ttype()));
              rid_kind_data := rid_kind_data.project(cast(nil, rid_kind.ttype()));
              rid_prop_data := rid_prop_data.project(oid_nil);
              rid_nid_data := rid_nid_data.project(oid_nil);
              if (isoldpage) {
                rid_size_update.myupdate(rid_size_data);
                rid_level_update.myupdate(rid_level_data);
                rid_kind_update.myupdate(rid_kind_data);
                rid_prop_update.myupdate(rid_prop_data);
                rid_nid_update.myupdate(rid_nid_data);
              } else {
                extend_protect(ws, doccont);
                rid_size.replace(rid_size_data, true);
                rid_level.replace(rid_level_data, true);
                rid_kind.replace(rid_kind_data, true);
                rid_prop.replace(rid_prop_data, true);
                rid_nid.replace(rid_nid_data, true);
                extend_unprotect(ws, doccont);
              }
            }

            # We have inserted new pages and copied the data on the rest
            # of the page to the last inserted page.  Now increase the
            # sizes of all ancestors.  Note that no holes can cross the
            # insert point (docinsertafter_newpre points to a non-hole
            # element)
            # Note, that even though the sizes of the moved data are not
            # yet consistent with the new situation, that is not a
            # problem, since we are only looking at sizes of data before
            # the insertion point.
            var ancestors_oldpre := new(void,oid).seqbase(0@0).append(0@0).append(ll_ancestor_or_self(new(void,oid).seqbase(0@0).append(0@0), new(void,oid).seqbase(0@0).append(docinsertafter_oldpre), pre_size, pre_level));
            var ancestors_nid := ancestors_oldpre.join(pre_nid);
            var ancestors_newrid := ancestors_nid.join(nid_rid).access(BAT_WRITE).myupdate(ancestors_nid.join(nid_rid_update));
            var ancestors_newpre := [swizzle](ancestors_newrid, [oid](map_pid_update));
            var ancestors_size := ancestors_oldpre.join(pre_size).access(BAT_WRITE).myupdate(ancestors_newrid.join(rid_size_update));
            # now figure out which ancestors *end* at the hole
            var ancestors_end := [oid]([+]([lng](ancestors_newpre), ancestors_size));

            var new_size := new(oid, int); # collect new sizes until we apply
            # select those that start before and end inside the moved data
            var indata := ancestors_end.uselect(docinsertbefore_newpre, oid(lng(docinsertbefore_newpre) + datasize), true, false);
            if (indata.count() > 0) {
              new_size.insert([+](indata.mirror().join(ancestors_size), shiftsize));
            }
            # select those that end inside the hole at the end
            var inhole := ancestors_end.uselect(oid(lng(docinsertbefore_newpre) + datasize), oid(lng(docinsertbefore_newpre) + datasize + holeatend), true, false);
            if (inhole.count() > 0) {
              new_size.insert([int]([-]((((lng(docinsertbefore_newpre) + datasize) - 1) + shiftsize), [lng](inhole.mirror().join(ancestors_newpre)))));
            }
            # select those that end after the moved data
            var behind := ancestors_end.uselect(oid(lng(docinsertbefore_newpre) + datasize + holeatend), oid_nil);
            if (behind.count() > 0) {
              new_size.insert([+](behind.mirror().join(ancestors_size), npages << REMAP_PAGE_BITS));
            }
            # figure out new and old pages since they need to be updated differently and apply
            var upd_pages := [oid]([>>]([lng](new_size.mirror().join(ancestors_newrid)), REMAP_PAGE_BITS));
            var is_new_page := [isnil](upd_pages.outerjoin(map_pid));
            var old_page := is_new_page.uselect(false);
            if (old_page.count() > 0) {
              rid_size_update.myupdate(ancestors_newrid.reverse().join(old_page.mirror().join(new_size)));
            }
            var new_page := is_new_page.uselect(true);
            if (new_page.count() > 0) {
              var tmp := ancestors_newrid.reverse().join(new_page.mirror().join(new_size));
              extend_protect(ws, doccont);
              rid_size.myupdate(tmp);
              extend_unprotect(ws, doccont);
            }
            # register which ancestors are being changed
            ancestor_nid.insert(new_size.project(doccont).reverse().join(ancestors_nid));
          } else {
            # the inserted data fits on the current page but we may have to
            # move data around to make a hole big ineough in the right place

            # holesize is the size of the hole after the node where we have to insert
            var holesize := 0;
            {
              var rid := docinsertbefore_rid;
              var pre := docinsertbefore_newpre;
              var pagelast := 0@0;
              var isoldpage := false;
              var loop := true;
              if (int(pre) >= docsize) {
                # note that the document cannot end at a page boundary
                # (there would not be a hole to insert into, so we
                # would not get here)
                holesize := int(REMAP_PAGE_SIZE - (lng(docsize) and REMAP_PAGE_MASK));
                loop := false;
              } else {
                var pageno := oid(lng(docinsertbefore_newpre) >> REMAP_PAGE_BITS);
                # the physical page id
                var pageid := map_pid_update.reverse().find(pageno);
                # the rid of the first element on the page
                var pagebase := oid(lng(pageid) << REMAP_PAGE_BITS);
                pagelast := oid(lng(pagebase) + REMAP_PAGE_MASK);
                if (map_pid.exist(pageid)) {
                  isoldpage := not(isnil(map_pid.find(pageid)));
                }
              }
              while (loop and (int(pre) < docsize) and (rid <= pagelast)) {
                var s := 0;
                if (isoldpage) {
                  if (rid_size_update.exist(rid)) {
                    s := rid_size_update.find(rid);
                  } else {
                    s := pre_size.find(swizzle(rid, map_pid));
                  }
                } else {
                  extend_protect(ws, doccont);
                  if (rid_size.exist(rid)) 
                      s := rid_size.find(rid);
                  extend_unprotect(ws, doccont);
                }
                if (isnil(niland(s, int_nil))) {
                  # we are looking at a hole
                  var h := niland(s, INT_MAX) + 1;
                  holesize :+= h;
                  pre := oid(lng(pre) + h);
                  rid := oid(lng(rid) + h);
                } else {
                  # not a hole, terminate loop
                  loop := false;
                }
              }
              if ((int(pre) >= docsize) and (rid <= pagelast)) {
                holesize :+= (int(pagelast) - int(rid)) + 1;
              }
            }
            # note: holeatstart = 0 if the insert point is not at the start of the hole at the end of the page
            if ((holesize + holeatstart) < insertsize) {
              # not enough space in the right place
              movedata(ws, doccont, oid(lng(docinsertbefore_newpre) + holesize), int(REMAP_PAGE_SIZE - ((lng(docinsertbefore_newpre) and REMAP_PAGE_MASK) + holesize + holeatend)), insertsize - holesize);
            }
          }

          # we now have a hole that is big enough below docinsertafter_newpre

          # We must adjust the sizes of all ancestors of the new element;
          # this means all elements that currently end at the newly created
          # hole and whose level is less than the level of the new element

          {
            # the document collection node (0@0) is ancestor to everybody but not returned by ll_ancestor
            var ancestors_oldpre := new(void,oid).seqbase(0@0).append(0@0).append(ll_ancestor_or_self(new(void,oid).seqbase(0@0).append(0@0), new(void,oid).seqbase(0@0).append(docinsertafter_oldpre), pre_size, pre_level));
            var ancestors_nid := ancestors_oldpre.join(pre_nid);
            var ancestors_newrid := ancestors_nid.join(nid_rid).access(BAT_WRITE).myupdate(ancestors_nid.join(nid_rid_update));
            var ancestors_newpre := [swizzle](ancestors_newrid, [oid](map_pid_update));
            var ancestors_size := ancestors_oldpre.join(pre_size).access(BAT_WRITE).myupdate(ancestors_newrid.join(rid_size_update));
            # now figure out which ancestors *end* at the hole
            var ancestors_end := [oid]([+]([lng](ancestors_newpre), ancestors_size)).uselect(docinsertafter_newpre).mirror();
            # figure out the level and select those with level < docinsertlevel
            var upd_ancestors := ancestors_end.join(ancestors_oldpre).join(pre_level).access(BAT_WRITE).myupdate(ancestors_end.join(ancestors_newrid).join(rid_level_update)).uselect(chr_nil, docinsertlevel, true, false);
            # now update the sizes of these ancestors (if any)
            if (upd_ancestors.count() > 0) {
              # calculate new size
              var new_size := [+](upd_ancestors.mirror().join(ancestors_size), insertsize);
              # figure out new and old pages since they need to be updated differently
              var upd_pages := [oid]([>>]([lng](upd_ancestors.mirror().join(ancestors_newrid)), REMAP_PAGE_BITS));
              var is_new_page := [isnil](upd_pages.outerjoin(map_pid));
              var old_page := is_new_page.uselect(false);
              if (old_page.count() > 0) {
                rid_size_update.myupdate(ancestors_newrid.reverse().join(old_page.mirror().join(new_size)));
              }
              var new_page := is_new_page.uselect(true);
              if (new_page.count() > 0) {
                var tmp := ancestors_newrid.reverse().join(new_page.mirror().join(new_size));
                extend_protect(ws, doccont);
                rid_size.myupdate(tmp);
                extend_unprotect(ws, doccont);
              }
            }
          }

          # copy the new element to the hole and do all the other work
          # only look at unedited version of to-be-inserted document
          # (required semantics)

          # get NID values for new nodes
          var newnids := ws_newnids(ws, doccont, insertsize).hmark(0@0); # [idx,NID]
          var newnididx := 0@0;

          var insert_pre_size := ws.fetch(PRE_SIZE).find(insertcont);
          var insert_pre_level := ws.fetch(PRE_LEVEL).find(insertcont);
          var insert_pre_prop := ws.fetch(PRE_PROP).find(insertcont);
          var insert_pre_kind := ws.fetch(PRE_KIND).find(insertcont);
          var insert_pre_nid := ws.fetch(PRE_NID).find(insertcont);
          var insert_pre_cont := ws.fetch(PRE_CONT).find(insertcont);
          var samedoc := false;     # perhaps we can optimize
          if (is_constant(insert_pre_cont)) {
            insert_pre_cont := bat2constant(insert_pre_cont);
            if (insert_pre_cont = doccont) {
              samedoc := true;
            }
          }
          var insert_attr_qn := ws.fetch(ATTR_QN).find(insertcont);
          var insert_attr_cont := ws.fetch(ATTR_CONT).find(insertcont);
          var sameattrdoc := false;
          if (is_constant(insert_attr_cont)) {
            insert_attr_cont := bat2constant(insert_attr_cont);
            if (insert_attr_cont = doccont) {
              sameattrdoc := true;
            }
          }
          var insert_attr_prop := ws.fetch(ATTR_PROP).find(insertcont);

          var attr_own_update := ws.fetch(ATTR_OWN_UPDATE).find(doccont);
          var attr_qn_update := ws.fetch(ATTR_QN_UPDATE).find(doccont);
          var attr_prop_update := ws.fetch(ATTR_PROP_UPDATE).find(doccont);
          var nid_qn_ins_update := ws.fetch(NID_QN_INS_UPDATE).find(doccont);

          # the difference in level for inserted elements between original
          # document and updated document
          var leveldiff := int(docinsertlevel) - int(insert_pre_level.find(insertitem));
  #         if (debug) printf("leveldiff %d\n", leveldiff);

          if (insert_pre_kind.find(insertitem) = TEXT) {
            # if we are inserting a text node, we may have to merge
            # consecutive text nodes
            # we remember the position in a hacky place
            # note, insertsize == 1 in this case
            var texts := ws.fetch(UPDATED_TEXT);
            texts.insert(doccont, newnids.find(newnididx));
          }

          # do the actual insert: copy the data from the source document
          # into the prepared hole
          # docinsertpoint is the point at which the next batch is going to be inserted
          var docinsertpoint := oid(lng(docinsertafter_newpre) + 1);
          while (insertsize > 0) {
            # docinsertpoint_rid is the RID for the new data
            var docinsertpoint_rid := antiswizzle(docinsertpoint, map_pid_update);
            # recalculate isoldpage for each iteration
            var pageid := oid(lng(docinsertpoint_rid) >> REMAP_PAGE_BITS);
            var isoldpage := false;
            if (map_pid.exist(pageid)) {
              isoldpage := not(isnil(map_pid.find(pageid)));
            }
            # one page at a time
            var batchsize := REMAP_PAGE_SIZE - (lng(docinsertpoint) and REMAP_PAGE_MASK);
            if (batchsize > lng(insertsize)) {
              batchsize := lng(insertsize);
            }
            var insprop := insert_pre_prop.reverse().select(insertitem, oid(lng(insertitem) + batchsize), true, false).reverse();
            var inskind := insert_pre_kind.reverse().select(insertitem, oid(lng(insertitem) + batchsize), true, false).reverse();
            var inssize := insert_pre_size.reverse().select(insertitem, oid(lng(insertitem) + batchsize), true, false).reverse();
            var inslevel := insert_pre_level.reverse().select(insertitem, oid(lng(insertitem) + batchsize), true, false).reverse();
            var insnid := insert_pre_nid.reverse().select(insertitem, oid(lng(insertitem) + batchsize), true, false).reverse();
            inslevel := [chr]([+]([int](inslevel), leveldiff));
            var texts, comments, pis, elems;
            var lastdata := insertitem;
            {
              var kindbats := inskind.splitkind();
              texts := kindbats.fetch(int(TEXT));
              comments := kindbats.fetch(int(COMMENT));
              pis := kindbats.fetch(int(PI));
              elems := kindbats.fetch(int(ELEMENT));
              var docs := kindbats.fetch(int(DOCUMENT));
              if (docs.count() > 0) {
                lastdata := docs.reverse().max();
              }
            }
            var insnnids := inskind.uselect(chr_nil, chr_nil).mark(newnididx).join(newnids); # PRE-New NID
            newnididx := oid(wrd(newnididx) + insnnids.count());
            if (not(samedoc)) {
              insprop := insprop.copy().access(BAT_WRITE);
              if (texts.count() > 0) {
                lastdata := max(lastdata, texts.reverse().max());
                var t := texts.hmark(0@0);
                var textval := mposjoin(t.leftjoin(insert_pre_prop), t.leftjoin(insert_pre_cont), ws.fetch(PROP_TEXT));
                var textids := add_string_bulk(ws, doccont, _PROP_TEXT, PROP_TEXT_UPDATE, textval, true);
                insprop.replace(t.reverse().join(textids));
              }
              if (comments.count() > 0) {
                lastdata := max(lastdata, comments.reverse().max());
                comments := comments.mirror().tmark(0@0);
                var commentval := mposjoin(comments.leftjoin(insert_pre_prop), comments.leftjoin(insert_pre_cont), ws.fetch(PROP_COM));
                var commentids := add_string_bulk(ws, doccont, _PROP_COM, PROP_COM_UPDATE, commentval, true);
                insprop.replace(comments.reverse().join(commentids));
              }
              if (pis.count() > 0) {
                lastdata := max(lastdata, pis.reverse().max());
                pis := pis.mirror().tmark(0@0);
                var insval := mposjoin(pis.leftjoin(insert_pre_prop), pis.leftjoin(insert_pre_cont), ws.fetch(PROP_INS));
                var tgtval := mposjoin(pis.leftjoin(insert_pre_prop), pis.leftjoin(insert_pre_cont), ws.fetch(PROP_TGT));
                var piids := add_pi_bulk(ws, doccont, tgtval, insval);
                insprop.replace(pis.reverse().join(pis.reverse().join(piids)));
              }
            }
            if (texts.count() > 0) {
                var contnid := texts.project(doccont).reverse().leftjoin(insnnids);
                added_text_nid.insert(contnid);
            }
            if (elems.count() > 0) {
              lastdata := max(lastdata, elems.reverse().max());
              elems := elems.mirror(); # PRE-PRE subset of the elements
              var prenids := elems.leftjoin(insert_pre_nid); # PRE-NID subset of the elements
              var preattrs := get_attr_own(ws, prenids, insertcont); # PRE_ATID of all attrs on the element subset, ATID is key
              var attrattrs := preattrs.reverse().mirror(); # ATID-ATID
              var attrnqnids; # ATID-New QNID
              if (sameattrdoc) {
                attrnqnids := attrattrs.leftjoin(insert_attr_qn);
              } else {
                var a := attrattrs.tmark(0@0);
                var prefurilocval := mposjoin(a.leftjoin(insert_attr_qn), a.leftjoin(insert_attr_cont), ws.fetch(QN_PREFIX_URI_LOC));
                attrnqnids := a.reverse().leftjoin(find_qn_bulk(ws, doccont, prefurilocval, true));
              }
              var attrnprops; # ATID-New PROPID
              if (sameattrdoc) {
                attrnprops := attrattrs.leftjoin(insert_attr_prop);
              } else {
                var a := attrattrs.tmark(0@0);
                var propval := mposjoin(a.leftjoin(insert_attr_prop), a.leftjoin(insert_attr_cont), ws.fetch(PROP_VAL));
                attrnprops := a.reverse().leftjoin(add_string_bulk(ws, doccont, _PROP_VAL, PROP_VAL_UPDATE, propval, true));
              }
              if (attrattrs.count() > 0) {
                # first claim enough entries in the ATTR table with the lock set
                var attr_id := add_attr_bulk(ws, doccont, attrattrs.project(oid_nil));
                var nattrattrs := attrattrs.tmark(attr_id); # New ATID-ATID
                var nattrnqnids := nattrattrs.join(attrnqnids); # New ATID-New QNID
                var nattrnids := nattrattrs.join(preattrs.reverse()).join(insnnids); # New ATID-New NID
                var nattrnprops := nattrattrs.join(attrnprops); # New ATID-New PROPID
                attr_own_update.insert(nattrnids);
                attr_qn_update.insert(nattrnqnids);
                attr_prop_update.insert(nattrnprops);
              }
              if (not(samedoc)) {
                var e := elems.tmark(0@0);
                var prefurilocval := mposjoin(e.leftjoin(insprop), e.leftjoin(insert_pre_cont), ws.fetch(QN_PREFIX_URI_LOC));
                var elemids := e.reverse().leftjoin(find_qn_bulk(ws, doccont, prefurilocval, true));
                insprop.replace(elemids);
              }
              var newelemnids := elems.leftjoin(insnnids).reverse(); # [newNID,i]
              var newnidqn := newelemnids.join(insprop); # [newNID,newQNID]
              nid_qn_ins_update.insert(newnidqn);
              added_nid.insert(newelemnids.project(doccont).reverse());
            }
            var lastsize := inssize.find(oid((lng(insertitem) + batchsize) - 1));
            if (isnil(niland(lastsize, int_nil))) {
              if (niland(lastsize, INT_MAX) > 0) {
                # last to-be-inserted node is in the middle of a hole, fix the hole to end at this node
                var hole := inssize.reverse().select(lastdata, oid_nil, false, true).reverse();
                inssize := inssize.copy().access(BAT_WRITE).replace([nilor]([-](hole.project(hole.count()), [int](hole.mark(1@0))), int_nil));
              }
            }
            insnid := insnid.copy().access(BAT_WRITE).replace(insnnids);
            insnid := insnid.tmark(docinsertpoint_rid);
            nid_rid_update.insert(insnid.select(oid_nil, oid_nil).reverse());
            insprop := insprop.tmark(docinsertpoint_rid);
            inssize := inssize.tmark(docinsertpoint_rid);
            inslevel := inslevel.tmark(docinsertpoint_rid);
            inskind := inskind.tmark(docinsertpoint_rid);
            if (isoldpage) {
              rid_size_update.myupdate(inssize);
              rid_level_update.myupdate(inslevel);
              rid_kind_update.myupdate(inskind);
              rid_prop_update.myupdate(insprop);
              rid_nid_update.myupdate(insnid);
            } else {
              extend_protect(ws, doccont);
              rid_size.replace(inssize, true);
              rid_level.replace(inslevel, true);
              rid_kind.replace(inskind, true);
              rid_prop.replace(insprop, true);
              rid_nid.replace(insnid, true);
              extend_unprotect(ws, doccont);
            }

            insertsize :-= int(batchsize);
            docinsertpoint := oid(lng(docinsertpoint) + batchsize);
            insertitem := oid(lng(insertitem) + batchsize);
          }
        }
      }
    }
  }
}

# note: extend read-lock must be taken while calling this function!!
PROC do_delete_nodes(bat[void, bat] ws, oid cont, oid newpre, int delsize) : void
{
  var map_pid := ws.fetch(MAP_PID).find(cont);
  var map_pid_update := ws.fetch(MAP_PID_UPDATE).find(cont);
  var pre_size := ws.fetch(PRE_SIZE).find(cont);
  var rid_size := ws.fetch(_RID_SIZE).find(cont);
  var rid_size_update := ws.fetch(RID_SIZE_UPDATE).find(cont);
  var nid_rid := ws.fetch(NID_RID).find(cont);
  var nid_rid_update := ws.fetch(NID_RID_UPDATE).find(cont);
  var pre_kind := ws.fetch(PRE_KIND).find(cont);
  var rid_kind := ws.fetch(_RID_KIND).find(cont);
  var rid_kind_update := ws.fetch(RID_KIND_UPDATE).find(cont);
  var pre_level := ws.fetch(PRE_LEVEL).find(cont);
  var rid_level := ws.fetch(_RID_LEVEL).find(cont);
  var rid_level_update := ws.fetch(RID_LEVEL_UPDATE).find(cont);
  var pre_prop := ws.fetch(PRE_PROP).find(cont);
  var rid_prop := ws.fetch(_RID_PROP).find(cont);
  var rid_prop_update := ws.fetch(RID_PROP_UPDATE).find(cont);
  var pre_nid := ws.fetch(PRE_NID).find(cont);
  var rid_nid := ws.fetch(_RID_NID).find(cont);
  var rid_nid_update := ws.fetch(RID_NID_UPDATE).find(cont);
  var nid_qn_ins_update := ws.fetch(NID_QN_INS_UPDATE).find(cont);
  var nid_qn_del_update := ws.fetch(NID_QN_DEL_UPDATE).find(cont);
  var modified_page := ws.fetch(MODIFIED_PAGE);
  var modified_nid := ws.fetch(MODIFIED_NID);
  var deleted_nid := ws.fetch(DELETED_NID);
  var added_nid := ws.fetch(ADDED_NID);
  var deleted_text_nid := ws.fetch(DELETED_TEXT_NID);
  var added_text_nid := ws.fetch(ADDED_TEXT_NID);

  var pageno := oid(lng(newpre) >> REMAP_PAGE_BITS);
  var pageid := map_pid_update.reverse().find(pageno);
  var next_pagebase := oid((lng(pageno) + 1) << REMAP_PAGE_BITS);
  var isoldpage := false;
  if (map_pid.exist(pageid)) {
    isoldpage := not(isnil(map_pid.find(pageid)));
  }
  var rid := oid((lng(newpre) and REMAP_PAGE_MASK) or (lng(pageid) << REMAP_PAGE_BITS));
  var ancestors_newpre;
  var nid;
  if (isoldpage) {
    if (rid_nid_update.exist(rid)) {
      nid := rid_nid_update.find(rid);
    } else {
      nid := pre_nid.find(swizzle(rid, map_pid));
    }
  } else {
    nid := rid_nid.find(rid);
  }
  if (not(isnil(nid))) {
    if (nid_rid.exist(nid)) {
      var oldrid := nid_rid.find(nid);
      if (not(isnil(oldrid))) {
        var oldpre := swizzle(oldrid, map_pid);
        var ancestors_oldpre := new(void,oid).seqbase(0@0).append(0@0).append(ll_ancestor(new(void,oid).seqbase(0@0).append(0@0), new(void,oid).seqbase(0@0).append(oldpre), pre_size, pre_level));
        var ancestors_nid := ancestors_oldpre.join(pre_nid);
        var ancestors_newrid := ancestors_nid.join(nid_rid).access(BAT_WRITE).myupdate(ancestors_nid.join(nid_rid_update));
        ancestors_newpre := [swizzle](ancestors_newrid, map_pid_update);
      }
    }
  }
  if (isnil(ancestors_newpre)) {
    # if we could not use the fast ll_ancestor, use the slow(er) mil_ancestor
    ancestors_newpre := mil_ancestor(ws, cont, newpre);
  }
  while (delsize >= 0) {
    var rid := oid((lng(newpre) and REMAP_PAGE_MASK) or (lng(pageid) << REMAP_PAGE_BITS));
    var nsize; # new size we are going to write (may join with consecutive hole)
    var pgsize; # size of hole we are dealing with this iteration (does not cross page boundary)
    if (oid(lng(newpre) + delsize) >= next_pagebase) {
      # new hole extends into next page
      nsize := (int(next_pagebase) - int(newpre)) - 1;
      pgsize := nsize;
    } else {
      # new hole wholly contained on this page
      pgsize := delsize;
      if (oid((lng(newpre) + delsize) + 1) >= next_pagebase) {
        # new hole ends at page boundary
        nsize := delsize;
      } else {
        # room to spare after new hole, maybe coalesce with consecutive hole
        var next_rid := oid((lng(rid) + delsize) + 1);
        if (isoldpage) {
          if (rid_size_update.exist(next_rid)) {
            nsize := rid_size_update.find(next_rid);
          } else {
            nsize := pre_size.find(swizzle(next_rid, map_pid));
          }
        } else {
          nsize := rid_size.find(next_rid);
        }
        if (isnil(niland(nsize, int_nil))) { # result of niland(nsize, int_nil) is either 0 or int_nil
          # there is a following hole
          nsize := (delsize + niland(nsize, INT_MAX)) + 1;
        } else {
          # no following hole
          nsize := delsize;
        }
      }
    }

    if (isoldpage) {
      modified_page.insert(cont, pageid);
    }

    var update_data;
    var oldpre := oid_nil;
    if (isoldpage) {
      if (rid_nid_update.exist(rid)) {
        var nid := rid_nid_update.find(rid);
        if (nid_rid.exist(nid)) {
          var oldrid := nid_rid.find(nid);
          oldpre := swizzle(oldrid, map_pid);
        }
      } else {
        oldpre := swizzle(rid, map_pid);
      }

      var pid_map := ws.fetch(PID_MAP).find(cont);
      if (not(isnil(oldpre))) {
        update_data := [swizzle](pre_nid.reverse().select(oldpre, oid(lng(oldpre) + pgsize)), pid_map).hmark(rid);
        update_data.access(BAT_WRITE).myupdate(rid_nid_update.reverse().select(rid, oid(lng(rid) + pgsize)).reverse());
      } else {
        # we are deleting a new node, so there is no data about it (or its descendants) in pre_nid
        update_data := rid_nid_update.reverse().select(rid, oid(lng(rid) + pgsize)).reverse().sort();
      }
    } else {
      update_data := rid_nid.reverse().select(rid, oid(lng(rid) + pgsize)).reverse();
    }
    {
      # Calculate updates for the QN_NID index.  In the end we need to
      # provide two tables: QN_NID_INS for all new QN/NID combinations, and
      # QN_NID_DEL for  all  deleted such  combinations.  We  calculate them
      # into NID_QN_{INS,DEL} tables for our convenience.
      # Since previous work in this transaction may have added nodes we
      # are going to delete, some of the nodes may already appear in
      # NID_QN_INS, so we need to remove them.  That is easy, however, since
      # we can remove all entries that contain NIDs of nodes we are deleting.
      # Harder is figuring out which NID/QN combinations of original nodes
      # are being deleted.
      # Note that update_data contains RID/NID values of nodes being
      # deleted, where the RID refers to the new RID value.
      var nid_qn_del;
      var delnid;
      var deltnid;
      if (isoldpage) {
        var rk := update_data.mirror().outerjoin(rid_kind_update).access(BAT_WRITE); # [RID,KIND/nil]
        rk.replace(rk.uselect(chr_nil).mirror().[swizzle](map_pid).join(pre_kind));  # [RID,KIND]
        var r := rk.uselect(ELEMENT).mirror();                                       # [RID,RID] (elements)
        var rn := r.outerjoin(rid_nid_update).access(BAT_WRITE);                     # [RID,NID/nil]
        rn.replace(rn.uselect(oid_nil).mirror().[swizzle](map_pid).join(pre_nid));   # [RID,NID]
        var rp := rn.join(nid_rid)          # [RID,oldRID]
          .select(oid_nil, oid_nil)         # [RID,oldRID] (non-nil)
          .[swizzle](map_pid)               # [RID,oldPRE]
          .join(pre_prop);                  # [RID,oldPROP]
        nid_qn_del := rn.reverse().join(rp);
        delnid := rn.reverse().project(cont).reverse();
        var rt := rk.uselect(TEXT).mirror();                                         # [RID,RID] (text)
        var rtn := rt.outerjoin(rid_nid_update).access(BAT_WRITE);                   # [RID,NID/nil]
        rtn.replace(rtn.uselect(oid_nil).mirror().[swizzle](map_pid).join(pre_nid)); # [RID,NID]
        deltnid := rtn.reverse().project(cont).reverse();
      } else {
        var nk := update_data.mirror()      # [newRID,newRID]
          .join(rid_kind);                  # [newRID,KIND]
        var nr := nk.uselect(ELEMENT)       # [newRID,nil] (elements)
          .mirror()                         # [newRID,newRID]
          .join(rid_nid)                    # [newRID,NID]
          .reverse();                       # [NID,newRID]
        nid_qn_del := nr.mirror()           # [NID,NID]
          .join(nid_rid)                    # [NID,oldRID]
          .select(oid_nil, oid_nil)         # [NID,oldRID] (non-nil)
          .[swizzle](map_pid)               # [NID,oldPRE]
          .join(pre_prop);                  # [NID,QN]
        delnid := nr.project(cont).reverse();
        var nrt := nk.uselect(TEXT)         # [newRID,nil] (text)
          .mirror()                         # [newRID,newRID]
          .join(rid_nid)                    # [newRID,NID]
          .reverse();                       # [NID,newRID]
        deltnid := nrt.project(cont).reverse();
      }
      # remove all NIDs in update_data from the nid_qn_ins_update; no
      # need to figure out which QNs go with them: we remove them all
      nid_qn_ins_update.delete(update_data.reverse());
      # add actual deleted NID/QN combinations to nid_qn_del_update
      nid_qn_del_update.insert(nid_qn_del);
      deleted_nid.insert(delnid);
      added_nid.deleteBuns(added_nid.sintersect(delnid));
      deleted_text_nid.insert(deltnid);
      added_text_nid.deleteBuns(added_text_nid.sintersect(deltnid));
    }

    var rid_nid_page;
    if (isoldpage) {
      if (isnil(oldpre)) {
        rid_nid_page := rid_nid_update.reverse().select(rid, oid(lng(rid) + pgsize)).reverse();
      } else {
        rid_nid_page := pre_nid.reverse().uselect(oldpre, oid(lng(oldpre) + pgsize)).hmark(rid);
        var rid_nid_page_update := rid_nid_update.reverse().select(rid, oid(lng(rid) + pgsize));
        rid_nid_page := rid_nid_page.copy().access(BAT_WRITE).key(true).replace(rid_nid_page_update);
      }
    } else {
      rid_nid_page := rid_nid.reverse().select(rid, oid(lng(rid) + pgsize)).reverse();
    }

    var nid_rid_updates := rid_nid_page.select(oid_nil, oid_nil).reverse().project(oid_nil);
    nid_rid_update := myupdate(nid_rid_update, nid_rid_updates);
    modified_nid.reverse().accbuild("hash");
    modified_nid.insert([lng](nid_rid_update.project(cont).reverse()).[>>](OID_PAGE_BITS));

    if (lng(nsize) = REMAP_PAGE_MASK) {
      # deleting whole page
      var del_page := ws.fetch(DEL_PAGE);
      del_page.insert(cont, pageid);
      # use the *before* version of map_pid_update (not that this is
      # particularly important: we are dealing with ancestors which
      # necessarily come before the to-be-deleted page)
      var pid_map_update := map_pid_update.select(oid_nil, oid_nil).reverse().sort().tmark(0@0);
      # now update map_pid_update
      map_pid_update.replace([oid]([-]([int](map_pid_update.select(pageno, oid_nil, false, true)), 1)), true);
      map_pid_update.replace(pageid, oid_nil, true);
      # figure out which ancestor sizes need to adjusted
      var ancestors_newrid := [swizzle](ancestors_newpre, pid_map_update);
      var ancestors_isnewpage := [isnil](outerjoin([oid]([>>]([lng](ancestors_newrid), REMAP_PAGE_BITS)), map_pid));
      var ancestors_nid;
      {
        # distinguish the three cases:
        # rid is on new page
        var a := ancestors_isnewpage.uselect(true).mirror().join(ancestors_newrid).join(rid_nid);
        # rid is on old page and was modified
        var b := ancestors_isnewpage.uselect(false).mirror().join(ancestors_newrid).join(rid_nid_update);
        # rid is on old page and was not modified
        var c := ancestors_isnewpage.uselect(false).kdiff(b).mirror().join(ancestors_newrid).[swizzle](map_pid).join(pre_nid);
        # combine
        ancestors_nid := a.access(BAT_WRITE).insert(b).insert(c).order().tmark(0@0);
      }
      var ancestors_oldpre := ancestors_nid.join(nid_rid).[swizzle](map_pid);
      var ancestors_size := ancestors_oldpre.join(pre_size).access(BAT_WRITE).myupdate(ancestors_newrid.join(rid_size_update));
      # now figure out which ancestors get smaller
      var ancestors_end := [oid]([+]([lng](ancestors_newpre), ancestors_size));
      var inpage := ancestors_end.uselect(newpre, oid(lng(newpre) + REMAP_PAGE_SIZE), true, false);
      var afterpage := ancestors_end.uselect(oid(lng(newpre) + REMAP_PAGE_SIZE), oid_nil, true, false);
      # nodes that end in the deleted page are truncated to end at the deleted node
      var ancestors_newsize := [int]([-](lng(newpre), [lng](inpage.mirror().join(ancestors_newpre))));
      # nodes that end beyond the deleted page are just made smaller
      ancestors_newsize.access(BAT_WRITE).insert([-](afterpage.mirror().join(ancestors_size), int(REMAP_PAGE_SIZE)));
      var old_page := ancestors_isnewpage.uselect(false);
      if (old_page.count() > 0) {
         rid_size_update.myupdate(ancestors_newrid.reverse().join(old_page.mirror().join(ancestors_newsize)));
      }
      var new_page := ancestors_isnewpage.uselect(true);
      if (new_page.count() > 0) {
        rid_size.myupdate(ancestors_newrid.reverse().join(new_page.mirror().join(ancestors_newsize)));
      }
      # remember which ancestors were changed
      var ancestor_nid := ws.fetch(ANCESTOR_NID);
      ancestor_nid.insert(ancestors_newpre.project(cont).reverse().join(ancestors_nid));
      # compensate for per-page house keeping below
      newpre := oid((lng(newpre) - pgsize) - 1);
      next_pagebase := oid(lng(next_pagebase) - REMAP_PAGE_SIZE);
    } else {
      update_data := [nilor]([-](update_data.project(nsize), [int](update_data.mark(0@0))), int_nil);
      if (isoldpage) {
        rid_size_update.myupdate(update_data);
      } else {
        rid_size.replace(update_data, true);
      }

      update_data := update_data.project(cast(nil, rid_level_update.ttype()));
      if (isoldpage) {
        rid_level_update.myupdate(update_data);
      } else {    
        rid_level.replace(update_data, true);
      }

      update_data := update_data.project(cast(nil, rid_kind_update.ttype()));
      if (isoldpage) {
        rid_kind_update.myupdate(update_data);
      } else {
        rid_kind.replace(update_data, true);
      }

      update_data := update_data.project(oid_nil);
      if (isoldpage) {
        rid_prop_update.myupdate(update_data);
      } else {
        rid_prop.replace(update_data, true);
      }

      update_data := update_data.project(oid_nil);
      if (isoldpage) {
        rid_nid_update.myupdate(update_data);
      } else {
        rid_nid.replace(update_data, true);
      }
    }

    delsize :-= pgsize + 1;
    newpre := oid((lng(newpre) + pgsize) + 1);
    if (newpre >= next_pagebase) {
      # per page house keeping
      pageno := oid(lng(newpre) >> REMAP_PAGE_BITS);
      pageid := map_pid_update.reverse().find(pageno);
      next_pagebase := oid((lng(pageno) + 1) << REMAP_PAGE_BITS);
      isoldpage := false;
      if (map_pid.exist(pageid)) {
        isoldpage := not(isnil(map_pid.find(pageid)));
      }
    }
  }
}

PROC do_update_delete(bat[void, bat] ws, bat[void,oid] update_node_item, bat[void,int] update_node_kind) : void
{
  var conts := update_node_kind.get_container();
  var types := update_node_kind.get_types();
  var attrs := types.ord_uselect(ATTR);
  var elems := types.ord_uselect(ELEM);

  if (attrs.count() > 0) {
    attrs := attrs.mirror(); # [i,i]
    var aconts := attrs.leftjoin(conts); # [i,CONT]
    {
      var contattr := aconts.reverse().join(update_node_item);
      ws.fetch(MODIFIED_ATTR).insert(contattr);
      var added_attr := ws.fetch(ADDED_ATTR);
      added_attr.deleteBuns(added_attr.sintersect(contattr));
    }
    aconts.tunique()@batloop() {
      var cont := $h;
      var list := aconts.ord_uselect(cont).mirror(); # [i,i]
      var attr_prop_updates := ws.fetch(ATTR_PROP_UPDATE).find(cont);
      var attr_own_updates := ws.fetch(ATTR_OWN_UPDATE).find(cont);
      var attr_qn_updates := ws.fetch(ATTR_QN_UPDATE).find(cont);
      var attrlist := list.leftjoin(update_node_item).reverse().project(oid_nil);
      attr_prop_updates.insert(attrlist);
      attr_own_updates.insert(attrlist);
      attr_qn_updates.insert(attrlist);
    }
  }

  if (elems.count() > 0) {
    # extract the update_node_item values that refer to elements
    elems := elems.mirror();
    update_node_item := elems.leftjoin(update_node_item);

    # the "update_node_item" parameter contains PRE values
    # instead of NID values; the PRE values refer to the unmodified
    # document
    update_node_item@batloop() {
      var oldpre := $t;         # the original PRE value of the to-be-deleted node
      var cont := conts.fetch($h);
      var pre_kind := ws.fetch(PRE_KIND).find(cont);
      if (pre_kind.find(oldpre) >= DOCUMENT) {
        ERROR("document nodes cannot be deleted");
      }
      if (ws.fetch(PRE_LEVEL).find(cont).find(oldpre) = chr(0)) {
        ERROR("root nodes cannot be deleted");
      }

      # translate PRE value to NID value which is also valid in the modified document
      var pre_nid := ws.fetch(PRE_NID).find(cont);
      var nid := pre_nid.find(oldpre);
      var nid_rid := ws.fetch(NID_RID).find(cont);
      var nid_rid_update := ws.fetch(NID_RID_UPDATE).find(cont);
      var rid;
      if (nid_rid_update.exist(nid)) {
        rid := nid_rid_update.find(nid);
      } else {
        rid := nid_rid.find(nid);
      }
      # if (isnil(rid)) the element is already gone, so nothing more to do
      if (not(isnil(rid))) {
        var map_pid := ws.fetch(MAP_PID).find(cont);
        var map_pid_update := ws.fetch(MAP_PID_UPDATE).find(cont);
        var pre := swizzle(rid, map_pid_update);
        var pre_size := ws.fetch(PRE_SIZE).find(cont);
        var rid_size := ws.fetch(_RID_SIZE).find(cont);
        var rid_size_update := ws.fetch(RID_SIZE_UPDATE).find(cont);
        var pageid := oid(lng(rid) >> REMAP_PAGE_BITS);
        var isoldpage := false;
        if (map_pid.exist(pageid)) {
          isoldpage := not(isnil(map_pid.find(pageid)));
        }

        extend_protect(ws, cont);
        var size;
        if (isoldpage) {
          if (rid_size_update.exist(rid)) {
            size := rid_size_update.find(rid);
          } else {
            size := pre_size.find(oldpre);
          }
        } else {
          size := rid_size.find(rid);
        }

        if (not(isnil(niland(size, int_nil)))) { # result of niland(size, int_nil) is either 0 or int_nil
          var rid_kind := ws.fetch(_RID_KIND).find(cont);
          var rid_kind_update := ws.fetch(RID_KIND_UPDATE).find(cont);
          var rid_nid := ws.fetch(_RID_NID).find(cont);
          var rid_nid_update := ws.fetch(RID_NID_UPDATE).find(cont);

          # record where we are deleting a node
          {
            var pid_map_update := map_pid_update.select(oid_nil, oid_nil).reverse().sort().tmark(0@0);
            var docsize;                # current size of document
            # if the size of item 0 was changed, use the new size, else use the original size
            # we assume that page 0 on which item 0 is located already exists...
            {
              var root_rid := swizzle(0@0, pid_map_update);
              if (rid_size_update.exist(root_rid)) {
                docsize := rid_size_update.find(root_rid);
              } else {
                docsize := pre_size.find(0@0);
              }
            }
            var nxtpre := oid(lng(pre) + size + 1), nxtrid, nxtsiz := 0;
            if (int(nxtpre) < docsize) {
              nxtrid := swizzle(nxtpre, pid_map_update);
              var isoldpg := false;
              if (map_pid.exist(oid(lng(nxtrid) >> REMAP_PAGE_BITS))) {
                isoldpg := not(isnil(map_pid.find(oid(lng(nxtrid) >> REMAP_PAGE_BITS))));
              }
              if (isoldpg) {
                if (rid_size_update.exist(nxtrid)) {
                  nxtsiz := rid_size_update.find(nxtrid);
                } else {
                  nxtsiz := pre_size.find(swizzle(nxtrid, map_pid));
                }
              } else {
                nxtsiz := rid_size.find(nxtrid);
              }
            }
            while ((int(nxtpre) < docsize) and isnil(niland(nxtsiz, int_nil))) {
              nxtpre := oid(lng(nxtpre) + niland(nxtsiz, INT_MAX) + 1);
              if (int(nxtpre) < docsize) {
                nxtrid := swizzle(nxtpre, pid_map_update);
                var isoldpg := false;
                if (map_pid.exist(oid(lng(nxtrid) >> REMAP_PAGE_BITS))) {
                  isoldpg := not(isnil(map_pid.find(oid(lng(nxtrid) >> REMAP_PAGE_BITS))));
                }
                if (isoldpg) {
                  if (rid_size_update.exist(nxtrid)) {
                    nxtsiz := rid_size_update.find(nxtrid);
                  } else {
                    nxtsiz := pre_size.find(swizzle(nxtrid, map_pid));
                  }
                } else {
                  nxtsiz := rid_size.find(nxtrid);
                }
              }
            }
            # only record spot if next node is a TEXT since only then do
            # we have to worry about coalescing text nodes
            if (int(nxtpre) < docsize) {
              var nxtknd;
              var nxtnid;
              var isoldpg := false;
              if (map_pid.exist(oid(lng(nxtrid) >> REMAP_PAGE_BITS))) {
                isoldpg := not(isnil(map_pid.find(oid(lng(nxtrid) >> REMAP_PAGE_BITS))));
              }
              if (isoldpg) {
                if (rid_kind_update.exist(nxtrid)) {
                  nxtknd := rid_kind_update.find(nxtrid);
                } else {
                  nxtknd := pre_kind.find(swizzle(nxtrid, map_pid));
                }
                if (rid_nid_update.exist(nxtrid)) {
                  nxtnid := rid_nid_update.find(nxtrid);
                } else {
                  nxtnid := pre_nid.find(swizzle(nxtrid, map_pid));
                }
              } else {
                nxtknd := rid_kind.find(nxtrid);
                nxtnid := rid_nid.find(nxtrid);
              }
              if (nxtknd = TEXT) {
                ws.fetch(UPDATED_TEXT).insert(cont, nxtnid);
              }
            }
          }

          # do the actual deleting in a subroutine
          do_delete_nodes(ws, cont, pre, size);
        }
        extend_unprotect(ws, cont);
      }
    }
  }
}

PROC add_attr_bulk(BAT[void,bat] ws, oid cont, BAT[oid,oid] oid_list) : oid
{ 
  coll_lock_set(ws, cont, COLL_SHORTLOCK, "add_attr_bulk");
  var ws_logtime := usec();
  var attr_own := ws.fetch(_ATTR_OWN).find(cont);
  var attr_prop := ws.fetch(_ATTR_PROP).find(cont);
  var attr_qn := ws.fetch(_ATTR_QN).find(cont);
  pf_assert(CATCH({ attr_own.append(oid_list, true);
                    attr_prop.append(oid_list, true);
                    attr_qn.append(oid_list, true); }), "master update failed (add_attr)");
  var attr_id := oid(attr_prop.count() - oid_list.count());
  coll_lock_unset(ws, cont, COLL_SHORTLOCK, "add_attr_bulk", ws_logtime);
  return attr_id;
}

PROC add_string_bulk(bat[void,bat] ws, oid cont, int tableid, int updtableid, bat[oid,str] valbat, bit dolock) : bat[oid,oid]
{
  if (dolock) coll_lock_set(ws, cont, COLL_SHORTLOCK, "add_string_bulk");
  var ws_logtime := usec();
  var table := ws.fetch(tableid).find(cont);
  var oldsize := count(table);
  var missing := valbat.tdiff(table).tunique();
  if (missing.count() > 0)
    pf_assert(CATCH(table.append(missing.reverse(), true)), "master update failed (add_string)");
  var logvals := table.slice(oldsize, count(table)).copy();
  var validbat := valbat.leftjoin(table.reverse());
  if (dolock) coll_lock_unset(ws, cont, COLL_SHORTLOCK, "add_string_bulk", ws_logtime);
  if (count(logvals) > 0) {
    ws.fetch(updtableid).find(cont).insert(logvals); # log new values
  }
  return validbat;
}

# note: coll lock must be taken while calling this function!!
PROC add_pi_bulk(bat[void,bat] ws, oid cont, bat[oid,str] tgtbat, bat[oid,str] insbat) : bat[oid,oid]
{
  coll_lock_set(ws, cont, COLL_SHORTLOCK, "add_pi_bulk");
  var ws_logtime := usec();
  var instgtbat := [+]([+](insbat, NS_ACCEL_SEP), tgtbat);
  var prop_ins := ws.fetch(_PROP_INS).find(cont);
  var prop_tgt := ws.fetch(_PROP_TGT).find(cont);
  var prop_ins_tgt := [+]([+](prop_ins, NS_ACCEL_SEP), prop_tgt);
  var idbat := instgtbat.outerjoin(prop_ins_tgt.reverse());
  var newidlist := idbat.uselect(oid_nil);
  if (newidlist.count() > 0) {
    # insert missing strings
    # first figure out unique combinations
    var instgtunique := newidlist.mirror().join(instgtbat).tunique().reverse().seqbase(0@0);
    # then extract instruction and target
    var instgtuniquesplit := instgtunique.splitbat(NS_ACCEL_SEP);
    # add them to respective BATs

    pf_assert(CATCH({ prop_ins.append(instgtuniquesplit.fetch(0), true);
                      prop_tgt.append(instgtuniquesplit.fetch(1), true); }), "master update failed (add_pi)");

    # recalculate ids
    prop_ins_tgt := [+]([+](prop_ins, NS_ACCEL_SEP), prop_tgt);
    idbat := instgtbat.outerjoin(prop_ins_tgt.reverse());
    # we also need to log the new values
    ws.fetch(PROP_INS_UPDATE).find(cont).insert(prop_ins.join(instgtuniquesplit.fetch(0).reverse().mirror()));
    ws.fetch(PROP_TGT_UPDATE).find(cont).insert(prop_tgt.join(instgtuniquesplit.fetch(1).reverse().mirror()));
  }
  coll_lock_unset(ws, cont, COLL_SHORTLOCK, "add_pi_bulk", ws_logtime);
  return idbat;
}


PROC find_qn_bulk(bat[void,bat] ws, oid cont, bat[oid,str] pref_uri_loc, bit update) : bat[oid,oid]
{
  var qn_prefix_uri_loc := ws.fetch(_QN_PREFIX_URI_LOC).find(cont);
  var qn_map := pref_uri_loc.outerjoin(qn_prefix_uri_loc.reverse());
  var miss := qn_map.uselect(oid_nil);
  if (miss.count() > 0) {
    var qn_loc := ws.fetch(_QN_LOC).find(cont);
    var qn_uri := ws.fetch(_QN_URI).find(cont);
    var qn_prefix := ws.fetch(_QN_PREFIX).find(cont);
    var qn_uri_loc := ws.fetch(_QN_URI_LOC).find(cont);
    var qn_histogram := ws.fetch(_QN_HISTOGRAM).find(cont);

    # get the unique missing strings and append them
    coll_lock_set(ws, cont, COLL_SHORTLOCK, "find_qn_bulk");
    var ws_logtime := usec();
    var _base := oid(count(qn_histogram));
    var _delta := reverse(miss).join(pref_uri_loc).histogram();
    var _pref_uri_loc := _delta.hmark(_base);
    var _histogram := [lng](_delta.tmark(_base));
    var _split := splitbat(_pref_uri_loc, NS_ACCEL_SEP);
    var _pref  := _split.fetch(0);
    var _uri   := _split.fetch(1);
    var _loc   := _split.fetch(2);
    var _uri_loc := [+]([+](_uri, NS_ACCEL_SEP), _loc);
    pf_assert(CATCH({ qn_loc.append(_loc, true); 
                      qn_uri.append(_uri, true); 
                      qn_prefix.append(_pref, true);
                      qn_uri_loc.append(_uri_loc, true);
                      qn_prefix_uri_loc.append(_pref_uri_loc, true); 
                      qn_histogram.append(_histogram, true); }), "master update failed (add_qn)");

    # recalculate map now
    qn_map := pref_uri_loc.outerjoin(qn_prefix_uri_loc.reverse());
    coll_lock_unset(ws, cont, COLL_SHORTLOCK, "find_qn_bulk", ws_logtime);

    if (update) {
      ws.fetch(QN_LOC_UPDATE).find(cont).insert(_loc);
      ws.fetch(QN_URI_UPDATE).find(cont).insert(_uri);
      ws.fetch(QN_PREFIX_UPDATE).find(cont).insert(_pref);
      ws.fetch(QN_URI_LOC_UPDATE).find(cont).insert(_uri_loc);
      ws.fetch(QN_PREFIX_URI_LOC_UPDATE).find(cont).insert(_pref_uri_loc);
      ws.fetch(QN_HISTOGRAM_UPDATE).find(cont).insert(_histogram);
    }
  }
  return qn_map;
}

PROC fix_consecutive_texts(bat[void, bat] ws, bat[oid,oid] contnid) : void
{
  # for now ignore position information
  contnid.reverse().tunique()@batloop() {
    var cont := $h;

    extend_protect(ws, cont);

    var map_pid := ws.fetch(MAP_PID).find(cont);
    var map_pid_update := ws.fetch(MAP_PID_UPDATE).find(cont);

    var pre_level := ws.fetch(PRE_LEVEL).find(cont);
    var rid_level := ws.fetch(_RID_LEVEL).find(cont);
    var rid_level_update := ws.fetch(RID_LEVEL_UPDATE).find(cont);
    var pre_kind := ws.fetch(PRE_KIND).find(cont);
    var rid_kind := ws.fetch(_RID_KIND).find(cont);
    var rid_kind_update := ws.fetch(RID_KIND_UPDATE).find(cont);
    var pid_map_update := map_pid_update.select(oid_nil, oid_nil).reverse().sort().tmark(0@0);

    var lvkd := new(oid, sht, rid_level.count()); # [RID,(level<<8)|kind]

    # go through virtual (updated) pages in logical order
    pid_map_update@batloop() {
      var pageno := $h;         # logical (PRE)
      var pageid := $t;         # physical (RID)

      var isoldpage := false;
      if (map_pid.exist(pageid)) {
        isoldpage := not(isnil(map_pid.find(pageid)));
      }

      var pagestart := oid(lng(pageid) << REMAP_PAGE_BITS); # RID value of start of page
      var pagelast := oid(lng(pagestart) + REMAP_PAGE_MASK); # RID value of last of page

      var lvpg;
      var kdpg;
      if (isoldpage) {
        var pgstart := swizzle(pagestart, map_pid); # PRE value of start of page
        var pglast := swizzle(pagelast, map_pid); # PRE value of last of page
        lvpg := pre_level.reverse().select(pgstart, pglast).reverse().seqbase(pagestart); # [RID,LEVEL]
        var lvpgup := rid_level_update.reverse().select(pagestart, pagelast).reverse(); # [RID,LEVEL]
        lvpg := lvpg.copy().access(BAT_WRITE).key(true).replace(lvpgup); # [RID,LEVEL]
        kdpg := pre_kind.reverse().select(pgstart, pglast).reverse().seqbase(pagestart); # [RID,KIND]
        var kdpgup := rid_kind_update.reverse().select(pagestart, pagelast).reverse(); # [RID,KIND]
        kdpg := kdpg.copy().access(BAT_WRITE).key(true).replace(kdpgup); # [RID,KIND]
      } else {
        lvpg := rid_level.reverse().select(pagestart, pagelast).reverse(); # [RID,LEVEL]
        kdpg := rid_kind.reverse().select(pagestart, pagelast).reverse(); # [RID,KIND]
      }

      # select all non-hole entries for both level and kind tables
      var l := lvpg.ord_select(chr_nil,chr_nil); # [RID,LEVEL]
      var k := l.mirror().leftfetchjoin(kdpg); # [RID,KIND]
      # combine them into a single table
      var x := [+]([<<]([sht](l),8),[sht](k)); # [RID,(LEVEL<<8)|KIND]
      lvkd.insert(x);
    }

    lvkd := [sht]([swizzle]([oid](lvkd).reverse(), map_pid_update).reverse()); # [newPRE,(level<<8)|kind]] 

    # use an undocumented feature of CTrefine: it does not check whether
    # the lhs is sorted
    # we now get unique OIDs for each stretch of consecutive elements
    # with the same value
    var r := CTrefine(lvkd, lvkd.project(nil)); # [newPRE,GRP] tsorted
    # s is a list of nodes where there are more than one consecutive
    # nodes of the same type at the same level, the rhs is the per-group OID
    var s := r.leftjoin(r.reverse().{count}().ord_uselect(2, int_nil).mirror()); # [newPRE,GRP] tsorted subselection of r
    # and now select the text elements from there
    var t := [and](lvkd, sht(255)).ord_uselect(sht(TEXT)).mirror().leftjoin(s).chk_order(); # [newPRE,GRP] hsorted,tsorted subselection of s
    # at this point we must do the real merging and deleting of text
    # nodes, but at least we know where they are
    if (t.count() > 0) {
      var rid_size := ws.fetch(_RID_SIZE).find(cont);
      var rid_size_update := ws.fetch(RID_SIZE_UPDATE).find(cont);
      var pre_prop := ws.fetch(PRE_PROP).find(cont);
      var rid_prop := ws.fetch(_RID_PROP).find(cont);
      var rid_prop_update := ws.fetch(RID_PROP_UPDATE).find(cont);
      var pre_nid := ws.fetch(PRE_NID).find(cont);
      var rid_nid := ws.fetch(_RID_NID).find(cont);
      var rid_nid_update := ws.fetch(RID_NID_UPDATE).find(cont);
      var t1 := [swizzle](t.reverse(), pid_map_update).reverse(); # [RID,GRP]

      var t1p := [oid]([>>]([lng](t1.mirror()), REMAP_PAGE_BITS)); # [RID,PGID]
      # [RID,RID] combos from t1.mirror() that refer to old pages
      var t1mold := t1p.leftjoin(map_pid).ord_uselect(oid_nil, oid_nil).mirror();
      # [RID,RID] combos from t1.mirror() that refer to new pages
      var t1mnew := t1p.reverse().kdiff(map_pid).reverse().mirror();

      var t2 := [swizzle](t1mold, map_pid); # [RID,oldPRE]
      # [RID,PROP] combination from unmodified nodes, modified nodes, and new nodes
      var rp := t2.join(pre_prop).access(BAT_WRITE).key(true).myupdate(t1mold.join(rid_prop_update)).myupdate(t1mnew.join(rid_prop)); # [RID,PROP]

      var gp := t1.reverse().leftjoin(rp); # [GRP,PROP]
      coll_lock_set(ws, cont, COLL_SHORTLOCK, "consec-texts");
      var ws_logtime := usec();
      var c := gp.leftfetchjoin(ws.fetch(_PROP_TEXT).find(cont)); # [GRP,content] hsorted
      var v := string_join(c, c.kunique().project("")); # [GRP,content] combined content with unique GRP values
      var i := add_string_bulk(ws, cont, _PROP_TEXT, PROP_TEXT_UPDATE, v, false); # [GRP,SID] OID into _PROP_TEXT table
      coll_lock_unset(ws, cont, COLL_SHORTLOCK, "consec-texts", ws_logtime);
      var p := t.reverse().kunique().reverse(); # [PRE,GRP] one representative from t per group
      var p1 := [swizzle](p.reverse(), pid_map_update).reverse(); # [RID,GRP]
      var h1 := kdiff(t1, p1).project(oid_nil); # [RID,nil] the other RIDs

      p1.leftjoin(i)@batloop() {
        var pageid := oid(lng($h) >> REMAP_PAGE_BITS);
        var isoldpage := false;
        if (map_pid.exist(pageid)) {
          isoldpage := not(isnil(map_pid.find(pageid)));
        }
        if (isoldpage) {
          rid_prop_update.myupdate($h, $t);
        } else {
          rid_prop.replace($h, $t, true);
        }
      }
      var nid_rid_update := ws.fetch(NID_RID_UPDATE).find(cont);
      h1@batloop() {
        var pageid := oid(lng($h) >> REMAP_PAGE_BITS);
        var isoldpage := false;
        if (map_pid.exist(pageid)) {
          isoldpage := not(isnil(map_pid.find(pageid)));
        }
        var nid;
        if (isoldpage) {
          if (rid_nid_update.exist($h)) {
            nid := rid_nid_update.find($h);
          } else {
            nid := pre_nid.find(swizzle($h, map_pid));
          }
          rid_prop_update.myupdate($h, $t);
          rid_size_update.myupdate($h, int_nil);
          rid_nid_update.myupdate($h, $t);
          rid_kind_update.myupdate($h, chr_nil);
          rid_level_update.myupdate($h, chr_nil);
        } else {
          nid := rid_nid.find($h);
          rid_prop.replace($h, $t, true);
          rid_size.replace($h, int_nil, true);
          rid_nid.replace($h, $t, true);
          rid_kind.replace($h, chr_nil, true);
          rid_level.replace($h, chr_nil, true);
        }
        nid_rid_update.myupdate(nid, oid_nil);
      }
    }
    extend_unprotect(ws, cont);
  }
}
@* Module Implementation
@h
#ifndef PF_SUPPORT_H
#define PF_SUPPORT_H

#include <monet.h>
#include "pathfinder.h"
#ifdef LIBPF
#define LIBPF_SUPPORT LIBPF
#endif
#include "pf_support.proto.h"
#include <monet_interpreter.h>
#include <monettime.h>

#endif
@c

#include "pf_config.h"
#include "pf_support.h"
#include <gdk_scanselect.h> /* for type-specific HT_bunfastins_nocheck_noinc(), until they're moved to gdk.mx */
#include <plain/algebra.h> /* needed for result size estimation in CMDenumerate */
#include <math.h> /* needed for round_up */

#if SIZEOF_OID == SIZEOF_INT
#define oidoid_bunfastins(b,h,t) intint_bunfastins(b,h,t);
#else
#define oidoid_bunfastins(b,h,t) lnglng_bunfastins(b,h,t);
#endif
@c
/* delete a file from the local tmp/ directory in the dbfarm; protect against abuse */
int CMDcleantmpdir(lng *lim) {
        DIR *dirp = opendir("tmp");
        char path[PATHLENGTH];
        struct dirent *dent;

        if (dirp == NULL)
  	        return GDK_SUCCEED;
        while ((dent = readdir(dirp)) != NULL) {
		struct stat st;
                if ((dent->d_name[0] == '.') && ((dent->d_name[1] == 0) || (dent->d_name[1] == '.' && dent->d_name[2] == 0))) {
                        continue;
                }
                GDKfilepath(path, "tmp/", dent->d_name, NULL);
		if (stat(path, &st) == 0 && st.st_mtime < *lim) {
                    int ret = unlink(path);
                    IODEBUG THRprintf(GDKout, "#unlink %s = %d\n", path, ret);
                }
        }
        closedir(dirp);
  	return GDK_SUCCEED;
}

@- hash fuctions

For general comparisons, we need to ensure that all strings hashing
on a valid boolean hash on the same double value. The same for all
numeric values. For strings, we also base the hash only on the
first keyword (ie exccluding spaces). This allows us to also support
selections on the fn:data value of nodes. As fn:data() value of a node
is the concatenation of all descendant text values with a space in 
between, we will hit (among others) on the first non-space-only text node.

examples:

XQUERY_HASH(" true")           = 1.0
XQUERY_HASH(" FALSE")          = 0.0 
XQUERY_HASH(" true love")      = 1.0
XQUERY_HASH(" false feelings") = 0.0
XQUERY_HASH("  4.21e10 ")      = 42.0
XQUERY_HASH("42")              = 42.0
XQUERY_HASH(" foo bar ")       = XQUERY_HASH("foo")
@c
#define XQUERY_STRHASH(_key,_hash) {\
     int _i;\
     for (_i = _hash = 0; _key[_i] && _key[_i] != ' '; _i++) {\
         _hash += _key[_i];\
         _hash += (_hash << 10);\
         _hash ^= (_hash >> 6);\
     }\
     _hash += (_hash << 3);\
     _hash ^= (_hash >> 11);\
     _hash += (_hash << 15);\
 }
#define XQUERY_HASH(_dst,_val) {                     \
    char *_s = _val;                                 \
    int _len = 0, _dummy = sizeof(dbl);              \
    dbl _tmp = 0.0, *_ref = &_tmp;                   \
    while(*_s && (isspace(*_s))) _s++;               \
    if (((_s[0] == 't') | (_s[0] == 'T')) &&         \
        ((_s[1] == 'r') | (_s[1] == 'R')) &&         \
        ((_s[2] == 'u') | (_s[2] == 'U')) &&         \
        ((_s[3] == 'e') | (_s[3] == 'E')) &&         \
        ((_s[4] ==  0 ) | isspace(_s[4])))           \
    {                                                \
       _tmp = 1.0; _len = 1;                         \
    } else if (((_s[0] == 'f') | (_s[0] == 'F')) &&  \
               ((_s[1] == 'a') | (_s[1] == 'A')) &&  \
               ((_s[2] == 'l') | (_s[2] == 'L')) &&  \
               ((_s[3] == 's') | (_s[3] == 'S')) &&  \
               ((_s[4] == 'e') | (_s[4] == 'E')) &&  \
               ((_s[5] ==  0 ) | isspace(_s[5])))    \
    {                                                \
       _tmp = 0.0; _len = 1;                         \
    } else if (isdigit(_s[0]) | (_s[0] == '.') |     \
               (_s[0] == '+') | (_s[0] == '-'))      \
    {                                                \
       _len = dblFromStr(_val, &_dummy, &_ref);      \
    }                                                \
    if (_len == 0) {                                 \
       hash_t _hsh;                                  \
       XQUERY_STRHASH(_val, _hsh);                   \
       _tmp = (dbl) _hsh;                            \
    } *_dst = ((((int*) (void*)&_tmp)[0] ^ ((int*) (void*)&_tmp)[1]) & 0x7fffffff); \
}

int CMDxquery_hash(int *dst, char* val) {
    XQUERY_HASH(dst, val);
    return GDK_SUCCEED;
}
int CMDxquery_hash_bat(BAT **res, BAT* b) {
    size_t cnt = BATcount(b);
    BAT *bn = *res = BATnew(TYPE_void, TYPE_int, cnt);
    int *dst = (int*) Tloc(bn, BUNfirst(bn));
    BUN p, q;
    BATiter bi;

    if (bn == NULL) return GDK_FAIL;
    bi = bat_iterator(b);
    BATloop(b,p,q) {
        str val = (str) BUNtvar(bi,p);
        XQUERY_HASH(dst, val);
        dst++;
    }
    ALIGNsetH(bn, b);
    BATsetcount(bn, cnt);
    bn->tsorted = 0;
    return GDK_SUCCEED;
}


@- constant handling
@c
int 
CMDisFakeProject(bit *r, ptr v, int tpe) {
	*r = (tpe == TYPE_bat)?is_fake_project((BAT*) v):1;
	return GDK_SUCCEED;
}

int
CMDfakeProject(BAT **res, ptr val, int tpe) 
{
	assert(val);
	if (tpe == TYPE_bat) {
		*res = (BAT*) val;
		BBPfix((*res)->batCacheid);
		return GDK_SUCCEED;
	}
	*res = BATnew(TYPE_void, tpe, 1);
	if (*res) {
		if (BUNappend(*res, val, FALSE)) return GDK_SUCCEED;
		BBPreclaim(*res);
	}
	return GDK_FAIL;
}

#include <constant.proto.h> /* for CMDconstCopy */

int
CMDdeFakeProject(ptr ret, int *tpe, ptr val, int t) 
{
	if (t == TYPE_bat) {
		BAT *b = (BAT*) val;
		BATiter bi;
		if (!is_fake_project(b)) {
			*tpe = TYPE_bat;
			*(BAT**) ret = b;
			BBPfix(b->batCacheid);
			return GDK_SUCCEED;
		}
		bi = bat_iterator(b);
		val = BUNtail(bi, BUNfirst(b));
		t = ATOMtype(b->ttype);
	} 
	return CMDconstCopy(ret, val, *tpe = t);
}

@= fetchconvert
int
CMDfetchConvert_@1(ptr res, int *tpe, BAT* b, @1* pos) 
{
	int ret = GDK_FAIL;
	if (@3 ((size_t) *pos) < BATcount(b)) {
		BATiter bi = bat_iterator(b);
		bat bid = *(bat*) BUNtail(bi, BUNfirst(b)+*pos);
		int fix = BBPfix(bid);
		BAT *bn = NULL;
		if (fix == 0 || (bn=BBPdescriptor(bid)) == NULL) {
			if (fix) BBPunfix(bid);
			GDKerror("fetch(%s) illegal BAT at position %d.\n", BBP_logical(b->batCacheid), *pos); 
		} else if (is_fake_project(bn)) {
			BATiter bni = bat_iterator(bn);
			ret = CMDconstCopy(res, BUNtail(bni, BUNfirst(bn)), *tpe = ATOMtype(bn->ttype));
			BBPunfix(bid);
		} else {
			ret = GDK_SUCCEED;
			*(BAT**)res = bn;
			*tpe = TYPE_bat;
		}
	} else {
		GDKerror("fetch(%s) illegal position @2.\n", BBP_logical(b->batCacheid), *pos); 
	}
	return ret;
}
@c
@:fetchconvert(int,%d,*pos >= 0 &&)@
@:fetchconvert(oid,"OIDFMT",)@

int
CMDinsertConvert(BAT** res, BAT *b, ptr h, ptr t, int tpe) 
{
	int ret = GDK_FAIL;
	BAT *bn = NULL;

	if (!tpe)
		return GDK_FAIL;
	assert(h);
	assert(t);
	if (CMDfakeProject(&bn, t, tpe) == GDK_SUCCEED) {
		if (BUNins(*res = b, BAThtype(b) == TYPE_bat ? (ptr) &((BAT *) h)->batCacheid : h, &bn->batCacheid, FALSE)) {
			BBPfix(b->batCacheid);
			ret = GDK_SUCCEED;
		}
		BBPunfix(bn->batCacheid);
	}
	return ret;
}

int
CMDappendConvert(BAT** res, BAT *b, ptr t, int tpe) 
{
	int ret = GDK_FAIL;
	BAT *bn = NULL;

	if (!tpe)
		return GDK_FAIL;
	assert(t);
	if (CMDfakeProject(&bn, t, tpe) == GDK_SUCCEED) {
		if (BUNappend(*res = b, &bn->batCacheid, FALSE)) {
			BBPfix(b->batCacheid);
			ret = GDK_SUCCEED;
		}
		BBPunfix(bn->batCacheid);
	}
	return ret;
}

static int 
merged_union( BAT** res, int ntabs, int ncols, BAT ***b) 
{
	BAT *bn[ncols], *BN, *temp, *tmp[2], *tgt;
	BUN cur[ntabs][ncols], dst[ncols], DST;
	bte bs[ntabs][ncols];
	bit mat[ntabs];
	int ttpe[ncols];	/* sht / bte ? */
	oid hsqb[ntabs];
	size_t Bcnt[ntabs], inc[2];
	int t, c, var, fix, non_empty[ntabs+1];
	bit concat01 = FALSE, concat10 = FALSE, void_fix = FALSE;
	bte *w = NULL, *ww[2], *wt, *ws[2];
	size_t res_count = 0, h = 0;

	BAT *src[2];
	BATiter srci[2];
	BUN csr[2];
	size_t idx[2], cnt[2], hs[2], ht;
	
	assert(ncols > 0);
	assert(ntabs > 1);
	assert(ntabs <= GDK_bte_max); /* allows to use type bte for w[] */

	*res = NULL;

	/* check arguments */
	for (t=0; t<ntabs; t++) {
		if (BATcount(b[t][0])>1 && !(BATtordered(b[t][0])&1)) {
			GDKerror("merged_union: tail of first BAT/column of table %d must be sorted.\n", t+1);
			return GDK_FAIL;
		}
	}

	for (t=0; t<ntabs; t++) {
		mat[t]  = FALSE;
		hsqb[t] = oid_nil;
		Bcnt[t] = (size_t)-1;
	}
	for (c=0; c<ncols; c++) {
		ttpe[c] = int_nil;
	}

	non_empty[ntabs] = 0;
	for (t=0; t<ntabs; t++) {
	    for (c=0; c<ncols; c++) {
		if (ttpe[c] == int_nil || ( ttpe[c] == TYPE_oid && b[t][c]->ttype == TYPE_void ) ) {
			/* get column type and record whether any OID column was non-materialized (VoID) */
			ttpe[c] = b[t][c]->ttype;
		} else {
			/* check column type (VoID matches with OID) */
			if (ATOMtype(ttpe[c]) != ATOMtype(b[t][c]->ttype)) {
				GDKerror("merged_union: BAT/column %d of table %d (ttype=%d) must have the same tail type as BAT/column %d of all other tables (ttype=%d).\n", 
						c+1, t+1, b[t][c]->ttype, c+1, ATOMtype(ttpe[c]));
				return GDK_FAIL;
			}
		}
		if (!is_fake_project(b[t][c])) {
			mat[t] |= TRUE;
			if (!BAThdense(b[t][c])) {
				GDKerror("merged_union: BAT/column %d of table %d must have a dense head.\n", c+1, t+1);
				return GDK_FAIL;
			}
			if (hsqb[t] == oid_nil) {
				hsqb[t] = b[t][c]->hseqbase;
			} else {
				if (hsqb[t] != b[t][c]->hseqbase) {
					GDKerror("merged_union: BAT/column %d of table %d (hseqbase="OIDFMT") must have the same hseqbase as all other BATs/columns of table %d (hseqbase="OIDFMT").\n", 
							c+1, t+1, b[t][c]->hseqbase, t+1, hsqb[t]);
					return GDK_FAIL;
				}
			}
			if (Bcnt[t] == (size_t)-1) {
				Bcnt[t] = BATcount(b[t][c]);
			} else {
				if (Bcnt[t] != BATcount(b[t][c])) {
					GDKerror("merged_union: BAT/column %d of table %d ("SZFMT" BUNs) must have the same size as all other BATs/columns of table %d ("SZFMT" BUNs).\n", 
							c+1, t+1, BATcount(b[t][c]), t+1, Bcnt[t]);
					return GDK_FAIL;
				}
			}
		}
	    }
	    if (!mat[t]) {
			GDKerror("merged_union: at least one BAT/column of table %d must be materialized, i.e., no 'fake_project'.\n", t+1);
			return GDK_FAIL;
	    }
	    res_count += Bcnt[t];
	    non_empty[non_empty[ntabs]] = t;
	    non_empty[ntabs] += (Bcnt[t] > 0);
	}

	/* create result BATs */

	BN = BATnew(TYPE_void, TYPE_bat, ncols);
	if (BN == NULL) {
		GDKerror("merged_union: BATnew(TYPE_void, TYPE_bat, %d) failed.\n", ncols);
		return GDK_FAIL;
	}
	temp = BATnew(TYPE_void, ATOMtype(ttpe[0]), res_count);
	if (temp == NULL) {
		GDKerror("merged_union: BATnew(TYPE_void, %s, " SZFMT ") failed.\n", ATOMname(ATOMtype(ttpe[0])), res_count);
		BBPreclaim(BN);
		return GDK_FAIL;
	}
	for (c=0; c<ncols; c++) {
		bn[c] = BATnew(TYPE_void, ATOMtype(ttpe[c]), res_count);
		if (bn[c] == NULL) {
			GDKerror("merged_union: BATnew(TYPE_void, %s, " SZFMT ") failed.\n", ATOMname(ATOMtype(ttpe[c])), res_count);
			while (c>0) {
				BBPreclaim(bn[--c]);
			}
			BBPreclaim(temp);
			BBPreclaim(BN);
			return GDK_FAIL;
		}
	}
	w = (bte*)GDKmalloc(2*res_count + 1);
	if (w == NULL) {
		GDKerror("merged_union: GDKmalloc(" SZFMT ") failed.\n", 2*res_count + 1);
		goto cleanup;
	}

	/* do the merged_union */

	for (c=0; c<ncols; c++) {
		dst[c] = BUNlast(bn[c]);
	}
	for (t=0; t<ntabs; t++) {
	    for (c=0; c<ncols; c++) {
		if (is_fake_project(b[t][c])) {
			bs[t][c] = (bte)0;
		} else {
			bs[t][c] = (bte)1;
		}
		cur[t][c] = BUNfirst(b[t][c]);
	    }
	}
@
@= merged_union_0
	/*  @1: ATOMstorage(ttpe[0]) (chr, bte, sht, int, flt, lng, dbl, var/fix=ATOMstorage(ttpe[0]))
	 *  @2: tloc, tvar, tail
	 *  @3: 0 1
	 *  @4: tail value comparison,
		e.g.,	simple_LE(BUN@2(src[0],csr[0]), BUN@2(src[1],csr[1]), @1)
		or	  atom_GT(BUN@2(src[0],csr[0]), BUN@2(src[1],csr[1]), @1)
	 */
	/* copy tails from BAT @3 to the results; 
	   for each BUN, remember in w, whether it came from BAT 0 or BAT 1 */
	while ((idx[@3] < cnt[@3]) && (@4)) {
		void@1_bunfastins_nocheck_noinc(tgt,dst[0],0,BUN@2(srci[@3],csr[@3]));
		wt[ht] = ws[@3][hs[@3]];
		idx[@3]++;
		csr[@3]++;
		dst[0]++;
		hs[@3] += inc[@3];
		ht++;
	}

	/* sucessively pair-wise merge-union the first BAT/column of all tables */
@
@= merged_union_1
	/*  @1: ATOMstorage(ttpe[0]) (chr, bte, sht, int, flt, lng, dbl, var/fix=ATOMstorage(ttpe[0]))
	 *  @2: tloc, tvar, tail (for BAT 0)
	 *  @3: tloc, tvar, tail (for BAT 1)
	 *  @4: simple, atom
	 */
	/* merge-union the first BATs/columns; regard and preserve tail-order */
	concat01 = (cnt[0]==0 || cnt[1]==0);
	concat10 = FALSE;
	if (BATtordered(src[0])&BATtordered(src[1])&1 || (BATtordered(src[0])&1 && cnt[1]==1) || (cnt[0]==1 && BATtordered(src[1])&1) ) {
		if (!concat01) {
			concat01 = @4_LE(BUN@2(srci[0],BUNlast(src[0])-1),BUN@3(srci[1],BUNfirst(src[1])),@1);
		}
		if (!concat01) {
			concat10 = @4_LT(BUN@3(srci[1],BUNlast(src[1])-1),BUN@2(srci[0],BUNfirst(src[0])),@1);
		}
	}
	if (!concat01 && !concat10) {
		while ((idx[0] < cnt[0]) && (idx[1] < cnt[1])) {
			@:merged_union_0(@1,@2,0,@4_LE(BUN@2(srci[0],csr[0]),BUN@3(srci[1],csr[1]),@1))@
			if (idx[0] < cnt[0]) {
				@:merged_union_0(@1,@3,1,@4_GT(BUN@2(srci[0],csr[0]),BUN@3(srci[1],csr[1]),@1))@
			}
		}
	}
	/* get remaining BUNs */
	if (!concat10) {
		@:merged_union_0(@1,@2,0,TRUE)@
	}
	@:merged_union_0(@1,@3,1,TRUE)@
	@:merged_union_0(@1,@2,0,TRUE)@
@
@= merged_union_2
	/*  @1: ATOMstorage(ttpe[0]) (chr, bte, sht, int, flt, lng, dbl, var, fix)
	 *  @2: tloc, tvar, tail (for BAT 0)
	 *  @3: simple, atom
	 */
	@:merged_union_1(@1,@2,@2,@3)@
	break;
@
@= merged_union_3
		//if ((void_fix = (src[0] == src[1] && src[0]->ttype==TYPE_void && src[1]->ttype==TYPE_void))) {
			/* create an artifical view (i.e., new BATdescriptor) to avoid concurrent void access problem */
			//src[0] = VIEWcreate(src[0], src[0]);
		//}
		csr[0] = BUNfirst(src[0]);
		csr[1] = BUNfirst(src[1]);
		cnt[0] = BATcount(src[0]);
		cnt[1] = BATcount(src[1]);
		dst[0] = BUNfirst(tgt);
		idx[0] = idx[1] = 0;
		hs[0] = hs[1] = ht = 0;
		
/* HACK(?): compare [v]oid (unsigned) as int/lng (signed) to get nil's first... */
#if SIZEOF_OID == SIZEOF_INT
		if (src[0]->ttype==TYPE_void && src[1]->ttype==TYPE_void) {
			@:merged_union_1(int,tvar,tvar,simple)@
		} else if (src[0]->ttype==TYPE_void && src[1]->ttype==TYPE_oid) {
			@:merged_union_1(int,tvar,tloc,simple)@
		} else if (src[0]->ttype==TYPE_oid && src[1]->ttype==TYPE_void) {
			@:merged_union_1(int,tloc,tvar,simple)@
		} else if (src[0]->ttype==TYPE_oid && src[1]->ttype==TYPE_oid) {
			@:merged_union_1(int,tloc,tloc,simple)@
#else
		if (src[0]->ttype==TYPE_void && src[1]->ttype==TYPE_void) {
			@:merged_union_1(lng,tvar,tvar,simple)@
		} else if (src[0]->ttype==TYPE_void && src[1]->ttype==TYPE_oid) {
			@:merged_union_1(lng,tvar,tloc,simple)@
		} else if (src[0]->ttype==TYPE_oid && src[1]->ttype==TYPE_void) {
			@:merged_union_1(lng,tloc,tvar,simple)@
		} else if (src[0]->ttype==TYPE_oid && src[1]->ttype==TYPE_oid) {
			@:merged_union_1(lng,tloc,tloc,simple)@
#endif
		} else {
			var = fix = ATOMstorage(src[0]->ttype);
			switch(var) {
			case TYPE_chr:	@:merged_union_2(chr,tloc,simple)@
			case TYPE_bte:	@:merged_union_2(bte,tloc,simple)@
			case TYPE_sht:	@:merged_union_2(sht,tloc,simple)@
			case TYPE_int:	@:merged_union_2(int,tloc,simple)@
			case TYPE_flt:	@:merged_union_2(flt,tloc,simple)@
			case TYPE_lng:	@:merged_union_2(lng,tloc,simple)@
			case TYPE_dbl:	@:merged_union_2(dbl,tloc,simple)@
			default:
				if (src[0]->tvarsized) {
					@:merged_union_2(var,tvar,atom)@
				} else {
					@:merged_union_2(fix,tloc,atom)@
				}
			}
		}
		
		BATsetcount(tgt, (dst[0] - BUNfirst(tgt)));
		if (!tgt->batDirty) tgt->batDirty = TRUE;
		BATkey(BATmirror(tgt),FALSE);
		tgt->tsorted = GDK_SORTED;
		tgt->tdense = FALSE;
		if (void_fix) {
			BBPunfix(src[0]->batCacheid);
		}
@
@c
	tgt = bn[0];
	wt = w;
	ws[0] = w + res_count;
	ws[1] = w + res_count + 1;
	inc[0] = inc [1] = 0;

	if (non_empty[ntabs] == 1) {
		srci[0].b = src[0] = b[non_empty[0]][0];
		srci[1].b = src[1] = b[non_empty[0] == 0][0];
		ws[0][0] = non_empty[0];
		ws[1][0] = (non_empty[0] == 0);

		@:merged_union_3@
	} else
	  if (non_empty[ntabs] == 2) {
		srci[0].b = src[0] = b[non_empty[0]][0];
		srci[1].b = src[1] = b[non_empty[1]][0];
		ws[0][0] = non_empty[0];
		ws[1][0] = non_empty[1];

		@:merged_union_3@
	} else {
		t = ntabs - 1;
		tmp[0] = bn[0];
		tmp[1] = temp;
		ww[0] = w;
		ww[1] = ws[1];
		wt = ww[t&1];
		wt[0] = t;
		tgt = b[t--][0];
		for (; t >= 0; t--) {
			srci[0].b = src[0] = b[t][0];
			srci[1].b = src[1] = tgt;
			tgt = tmp[t&1];
			ws[0][0] = t;
			ws[1] = wt;
			wt = ww[t&1];

			@:merged_union_3@

			inc[1] = 1;
		}
	}

	/* merge-union the remaining BATs/columns of all tables */
@
@= merged_union_4
	/*  @1: ATOMstorage(ttpe[c]) (chr, bte, sht, int, flt, lng, dbl, var/fix)
	 *  @2: tloc, tvar, tail
	 */
	/* merge-union the remaining BATs/columns of all tables:
	   w tell us, from which table we need to get the next BUN */
    {
	BATiter bi;
	for (h=0; h<res_count; h++) {
		t = w[h];
		bi.b = b[t][c];
		void@1_bunfastins_nocheck_noinc(bn[c],dst[c],0,BUN@2(bi,cur[t][c]));
		cur[t][c] += bs[t][c];
		dst[c]++;
	}
    }
@
@= merged_union_5
	/*  @1: ATOMstorage(ttpe[c]) (chr, bte, sht, int, flt, lng, dbl, var/fix)
	 *  @2: tloc, tvar, tail
	 */
	@:merged_union_4(@1,@2)@
	break;
@
@c
	for (c=1; c<ncols; c++) {
		if (ttpe[c]==TYPE_void) {
			/* at least one OID column is non-materialized (VoID) */
			@:merged_union_4(oid,tail)@
		} else if (ttpe[c]==TYPE_oid) {
			/* all OID columns are materialized */
			@:merged_union_4(oid,tloc)@
		} else {
			switch(ATOMstorage(ttpe[c])) {
			case TYPE_chr:	@:merged_union_5(chr,tloc)@
			case TYPE_bte:	@:merged_union_5(bte,tloc)@
			case TYPE_sht:	@:merged_union_5(sht,tloc)@
			case TYPE_int:	@:merged_union_5(int,tloc)@
			case TYPE_flt:	@:merged_union_5(flt,tloc)@
			case TYPE_lng:	@:merged_union_5(lng,tloc)@
			case TYPE_dbl:	@:merged_union_5(dbl,tloc)@
			default:
				if (b[0][c]->tvarsized) {
					@:merged_union_5(var,tvar)@
				} else {
					@:merged_union_5(fix,tloc)@
				}
			}
		}
	}

	/* set BAT properties */
	for (c=0; c<ncols; c++) {
		BATseqbase(bn[c], (oid)0);
		BATsetcount(bn[c], dst[c]-BUNfirst(bn[c]));
		assert(BATcount(bn[c]) == res_count);
		if (!bn[c]->batDirty) bn[c]->batDirty = TRUE;
		BATkey(bn[c],TRUE);
		BATkey(BATmirror(bn[c]),(res_count < 2));
		bn[c]->hsorted = GDK_SORTED;
		if (c == 0 || res_count < 2) {
			bn[c]->tsorted = GDK_SORTED;
		} else
		  if (non_empty[ntabs] == 1) {
			bn[c]->tsorted = (BATtordered(b[non_empty[0]][c])&1 ? GDK_SORTED : FALSE);
		} else
		  if (non_empty[ntabs] == 2) {
			BATiter b0i, b1i;
			BAT *b0 = b[non_empty[0]][c];
			BAT *b1 = b[non_empty[1]][c];
			b0i.b = b0;
			b1i.b = b1;
			if ( ( BATtordered(b0)&BATtordered(b1)&1 || (BATtordered(b0)&1 && BATcount(b1)==1) || (BATcount(b0)==1 && BATtordered(b1)&1) ) && \
			     ( ( concat01 && atom_LE(BUNtail(b0i,BUNlast(b0)-1),BUNtail(b1i,BUNfirst(b1)),ATOMstorage(ATOMtype(ttpe[c]))) ) || \
			       ( concat10 && atom_LE(BUNtail(b1i,BUNlast(b1)-1),BUNtail(b0i,BUNfirst(b0)),ATOMstorage(ATOMtype(ttpe[c]))) ) ) ) {
				bn[c]->tsorted = GDK_SORTED;
			} else {
				bn[c]->tsorted = FALSE;
			}
		} else {
			bn[c]->tsorted = FALSE;
		}
		bn[c]->hdense = TRUE;
		bn[c]->tdense = FALSE;
	}

	/* insert bn[] BATs in BN BAT */

	DST = BUNlast(BN);
	BATseqbase(BN, (oid)0);
	for (c=0; c<ncols; c++, DST++) {
		voidfix_bunfastins_nocheck_noinc(BN,DST,0,(void*)&bn[c]->batCacheid);
		BBPunfix(bn[c]->batCacheid);
	}
	BATsetcount(BN, ncols); 
	if (!BN->batDirty) BN->batDirty = TRUE;
	BATkey(BN,TRUE);
	BATkey(BATmirror(BN),TRUE);
	BN->hsorted = GDK_SORTED;
	BN->tsorted = FALSE;
	BN->hdense = TRUE;
	BN->tdense = FALSE;

	*res = BN;

	GDKfree(w);
	BBPreclaim(temp);

	return GDK_SUCCEED;
bunins_failed:
	GDKerror("merged_union: bunins failed.\n");
cleanup:
	BBPreclaim(BN);
	BBPreclaim(temp);
	for (c=0; c<ncols; c++) {
		BBPreclaim(bn[c]);
	}
	return GDK_FAIL;
}

int CMDmerged_union( BAT** res, ptr L, int ltpe, ptr R, int rtpe, ... )
{
	int tpe, nbats, t, ntabs = 2, ncols, ret = GDK_SUCCEED;
	BAT ***b;
	va_list ap;
        ptr p;

	nbats = 2;
	va_start(ap,rtpe);
	while((p = va_arg(ap, ptr)) != NULL) {
                tpe = va_arg(ap, int);
		nbats++;
	}
	va_end(ap);
	if (nbats&1) {
		GDKerror("merged_union: uneven number of BATs: %d.\n", nbats);
		return GDK_FAIL;
	}
	ncols = nbats>>1;

	b = (BAT***)GDKmalloc(ntabs*sizeof(BAT**));
	if (b == NULL) {
		GDKerror("merged_union: GDKmalloc(" SZFMT ") failed.\n", ntabs*sizeof(BAT**));
		return GDK_FAIL;
	}
	for (t=0; t<ntabs; t++) {
		b[t] = (BAT**)GDKmalloc(ncols*sizeof(BAT*));
		if (b[t] == NULL) {
			GDKerror("merged_union: GDKmalloc(" SZFMT ") failed.\n", ncols*sizeof(BAT*));
			while (t>0) {
				GDKfree(b[--t]);
			}
			GDKfree(b);
			return GDK_FAIL;
		}
	}

	nbats = 0;
	/* first convert any constant parameters to fake projects */
	if (CMDfakeProject(&b[nbats&1][nbats>>1], L, ltpe) == GDK_SUCCEED) {
		nbats++;
		if (CMDfakeProject(&b[nbats&1][nbats>>1], R, rtpe) == GDK_SUCCEED) {
			nbats++;
			va_start(ap,rtpe);
			while((p = va_arg(ap, ptr)) != NULL) {
        		        tpe = va_arg(ap, int);
				if (CMDfakeProject(&b[nbats&1][nbats>>1], p, tpe) == GDK_SUCCEED) {
					nbats++;
				} else {
					ret = GDK_FAIL;
					break;
				}
			}
			va_end(ap);
		}
	}
	if (ret == GDK_SUCCEED) {
		ret = merged_union(res, ntabs, ncols, b);
	}

	/* unfix all bats; this destroys any created fake projects */
	while (--nbats >= 0) {
		BBPunfix(b[nbats&1][nbats>>1]->batCacheid);
	}
	for(t=0; t<ntabs; t++) {
		GDKfree(b[t]);
	}
	GDKfree(b);
	return ret;
}

int CMDmulti_merged_union( BAT** res, BAT *Bt)
{
	int t, ntabs, c, ncols = 0, ret = GDK_SUCCEED;
	BAT **Bc, ***b;

	BATcheck(Bt, "multi_merged_union");

	if (Bt->ttype != TYPE_bat) {
		GDKerror("multi_merged_union: tail-type must be BAT, not %s.\n", ATOMname(Bt->ttype));
		return GDK_FAIL;
	}

	if ((ntabs = BATcount(Bt)) < 2) {
		GDKerror("multi_merged_union: input must contain at least 2 tables/BUNs.\n");
		return GDK_FAIL;
	}

	Bc = (BAT**)GDKmalloc(ntabs*sizeof(BAT*));
	if (Bc == NULL) {
		GDKerror("multi_merged_union: GDKmalloc(" SZFMT ") failed.\n", ntabs*sizeof(BAT*));
		return GDK_FAIL;
	}

	for (t=0; t<ntabs; t++) {
		Bc[t] = BATdescriptor(*(bat*)Tloc(Bt, t));
		if (Bc[t]->ttype != TYPE_bat) {
			GDKerror("multi_merged_union: tail-type of BAT holding table %d must be BAT, not %s.\n", t+1, ATOMname(Bc[t]->ttype));
			ret = GDK_FAIL;
		} else if (t == 0) {
			assert(BATcount(Bc[t]) <= (size_t)GDK_int_max);
			if ((ncols = (int)BATcount(Bc[t])) < 1) {
				GDKerror("multi_merged_union: first table must consist of at least 1 column.\n");
				ret = GDK_FAIL;
			}
		} else if (ncols != (int)BATcount(Bc[t])) {
			GDKerror("multi_merged_union: table %d (%d columns) must have as many columns as all other tables (%d columns).\n",
			          t+1, (int)BATcount(Bc[t]), ncols);
			ret = GDK_FAIL;
		}
		if (ret == GDK_FAIL) {
			while (t >= 0) {
				BBPunfix(Bc[t--]->batCacheid);
			}
			GDKfree(Bc);
			return GDK_FAIL;
		}
	}

	b = (BAT***)GDKmalloc(ntabs*sizeof(BAT**));
	if (b == NULL) {
		GDKerror("multi_merged_union: GDKmalloc(" SZFMT ") failed.\n", ntabs*sizeof(BAT**));
		for (t=0; t<ntabs; t++) BBPunfix(Bc[t]->batCacheid);
		GDKfree(Bc);
		return GDK_FAIL;
	}
	for (t=0; t<ntabs; t++) {
		b[t] = (BAT**)GDKmalloc(ncols*sizeof(BAT*));
		if (b[t] == NULL) {
			GDKerror("multi_merged_union: GDKmalloc(" SZFMT ") failed.\n", ncols*sizeof(BAT*));
			while (t>0) {
				GDKfree(b[--t]);
			}
			GDKfree(b);
			for (t=0; t<ntabs; t++) BBPunfix(Bc[t]->batCacheid);
			GDKfree(Bc);
			return GDK_FAIL;
		}
	}

	for (t=0; t<ntabs; t++) {
		for (c=0; c<ncols; c++) {
			b[t][c] = BATdescriptor(*(bat*)Tloc(Bc[t], c));
		}
		BBPunfix(Bc[t]->batCacheid);
	}
	GDKfree(Bc);

	ret = merged_union(res, ntabs, ncols, b);

	for (t=0; t<ntabs; t++) {
		for (c=0; c<ncols; c++) {
			BBPunfix(b[t][c]->batCacheid);
		}
	}
	return ret;
}

int
CMDll_strSplit(BAT **res, BAT *strs, BAT *seps)
{
	BATiter strsi = bat_iterator(strs), sepsi = bat_iterator(seps);
        BAT *bn;
        BUN p_str, p_sep, last_str_row;
        size_t cnt, seplen;
        oid base;
        char *actual_str;
        bit triv_prop;

        /* check arguments */

        BATcheck(strs, "ll_strSplit");
        BATcheck(seps, "ll_strSplit");

        cnt = BATcount(strs);
        base = strs->hseqbase;
        ERRORcheck(!BAThdense(strs) || !BAThdense(seps), 
                "ll_strSplit: input BATs (strs, seps) must be void-headed with non-nil seqbase.\n");
        ERRORcheck(BATcount(seps)!=cnt || seps->hseqbase!=base,
                "ll_strSplit: input BATs (strs, seps) must be head-aligned.\n");

        /* create result BAT */

        bn = BATnew(TYPE_oid, TYPE_str, 2*cnt);
        if (bn == NULL) {
                GDKerror("ll_strSplit: BATnew(TYPE_oid, TYPE_str, %d) failed.\n", 2*cnt);
                return GDK_FAIL;
        }

        /* do the ll_strSplit */

        p_str = BUNfirst(strs);
        p_sep = BUNfirst(seps);

        for (last_str_row = BUNlast(strs) ;
             p_str < last_str_row ;
             p_str++, p_sep++) {

                str sep = (str)BUNtvar(sepsi, p_sep);
                seplen = strlen(sep);
                actual_str = GDKstrdup((str)BUNtvar(strsi, p_str));

                if (!seplen)
                {
                        bunfastins(bn, &base, actual_str);
                        base++;
                        continue;
                }
  
                while (actual_str) {
                        char *e = strstr(actual_str, sep);
  
                        if (!e)
                                break;
  
                        *e = 0;
                        bunfastins(bn, &base, actual_str);
                        actual_str = e + seplen;
                }
 
                if (actual_str && *actual_str)
                        bunfastins(bn, &base, actual_str);
                base++;
        }

        /* set result properties */

        if (!bn->batDirty) bn->batDirty = TRUE;
        triv_prop = (BATcount(bn) < 2);
        BATkey(bn,triv_prop);
        BATkey(BATmirror(bn),triv_prop);
        bn->hsorted = GDK_SORTED;
        bn->tsorted = triv_prop;
        bn->hdense = triv_prop;
        bn->tdense = FALSE;
        if (BATcount(bn) == 0) {
                BATseqbase(bn, (oid)0);
        } else if (BATcount(bn) == 1) {
                BATseqbase(bn, *(oid*)Hloc(bn,BUNfirst(bn)));
        }
        *res = bn;
        return GDK_SUCCEED;
bunins_failed:
        GDKerror("ll_strSplit: bunins failed.\n");
        return GDK_FAIL;
}


int
CMDnormSpace(str *res, str string)
{
        char *ws, *cur, *pointer;
        size_t n;

        pointer = GDKstrdup(string);
        if (pointer == NULL) {
                GDKerror("normSpace: GDKmalloc(" SZFMT ") failed.\n",
                         strlen (string) + 1);
                return GDK_FAIL;
        }

        *res = pointer;

        cur = string;
        ws = " \f\n\r\t\v";
        n = strspn (cur, ws);
        if (n)
        {
            cur += n;
        }

        while ((n = strcspn (cur, ws))) {
            while (n > 0)
            {
                *pointer = *cur;
                pointer++;
                cur++;
                n--;
            }
            n = strspn (cur, ws);
            if (n)
            {
                *pointer = ' ';
                pointer++;
                cur += n;
            }
        }
        if (pointer > *res && pointer[-1] == ' ') {
            pointer--;
        }
        *pointer = '\0';

        return GDK_SUCCEED;
}

int
math_unary_up_ROUND(dbl *res, dbl *x)
{
        if (*x == dbl_nil) {
                *res = dbl_nil;
        } else {
                double integral;
                double tmp = modf(*x, &integral);

                tmp = floor(tmp + 0.5);
                tmp += integral;

                *res = tmp;
        }

        return (GDK_SUCCEED);
}




#define KIND_NODE (oid)0
#define KIND_TEXT (oid)1
#define KIND_STR  (oid)2

int CMDcombine_text_string( BAT** res, BAT *iter, BAT *kind, BAT *str_value, int *result_size )
{
	BATiter iteri = bat_iterator(iter), kindi = bat_iterator(kind), str_valuei = bat_iterator(str_value);
	BAT *bn;
	BUN qi, pi, pk, ps;
	size_t cnt, len, strsize = 1024;
	oid base0, base, i0, k0;
	str actual_str = NULL;
	bit triv_prop;
	
	*res = NULL;

	/* check arguments */

	BATcheck(iter, "combine_text_string");
	BATcheck(kind, "combine_text_string");
	BATcheck(str_value, "combine_text_string");

	cnt = BATcount(iter);
	base = iter->hseqbase;
	ERRORcheck(!BAThdense(iter) || !BAThdense(kind) || !BAThdense(str_value), 
		"combine_text_string: all input BATs (iter, kind, str_value) must be void-headed with non-nil seqbase.\n");
	ERRORcheck(BATcount(kind)!=cnt || BATcount(str_value)!=cnt || kind->hseqbase!=base || str_value->hseqbase!=base,
		"combine_text_string: all input BATs (iter, kind, str_value) must be head-aligned.\n");
	ERRORcheck(!(BATtordered(iter)&1),
		"combine_text_string: input BAT iter must be sorted on tail.\n");

	/* create result BAT */

	bn = BATnew(TYPE_oid, TYPE_str, *result_size);
	if (bn == NULL) {
		GDKerror("combine_text_string: BATnew(TYPE_oid, TYPE_str, %d) failed.\n", *result_size);
		return GDK_FAIL;
	}

	/* do the combine_text_string */

	pi = BUNfirst(iter);
	pk = BUNfirst(kind);
	ps = BUNfirst(str_value);

	/* allocate str buffer */
	actual_str = (str)GDKmalloc(strsize);
	if (actual_str == NULL) {
		GDKerror("combine_text_string: GDKmalloc(" SZFMT ") failed.\n", strsize);
		goto cleanup;
	}
	len = 0;
	actual_str[0] = '\0';
	k0 = oid_nil;
	i0 = *(oid*)BUNtail(iteri,pi); /* FIXME: "tvar" (void) vs. "tloc" (oid) */
	base0 = oid_nil;

	for (qi = BUNlast(iter) ; pi < qi ; pi++, pk++, ps++) {
		oid i = *(oid*)BUNtail(iteri,pi); /* FIXME: "tvar" (void) vs. "tloc" (oid) */
		oid k = *(oid*)BUNtail(kindi,pk); /* FIXME: "tvar" (void) vs. "tloc" (oid) */
		str s = (str)BUNtvar(str_valuei,ps);
		size_t l = strlen(s) + 1;
		if ( i0 < i || k == KIND_NODE ) {
			/* new iter or new node */
			if (len > 0) {
				/* actual_str of (previous) iter is not empty => insert it into result */
				bunfastins(bn, &base0, actual_str);
				len = 0;
				actual_str[0] = '\0';
			}
			k0 = oid_nil;
		}
		if (len+l >= strsize) {
			/* extend str buffer */
			do {
				strsize *= 2;
			} while (len+l >= strsize);
			actual_str = GDKrealloc(actual_str, strsize);
			if (actual_str == NULL) {
				GDKerror("combine_text_string: GDKrealloc(" SZFMT ") failed.\n", strsize);
				goto cleanup;
			}
		}
		if (k0 == KIND_STR && k == KIND_STR) {
			/* insert ' '-separator between adjacent STRs */
			actual_str[len++] = ' ';
			actual_str[len] = '\0';
		}
        if (k != KIND_NODE)
        {
		    strcpy(actual_str + len, s); /* we know it fits */
		    len += l - 1;	/* compensate for +1 earlier on */
        }
		i0 = i;
		k0 = k;
		base0 = base++;
	}

	if (len > 0) {
		bunfastins(bn, &base0, actual_str);
	}

	/* set result properties */

	if (!bn->batDirty) bn->batDirty = TRUE;
	triv_prop = (BATcount(bn) < 2);
	BATkey(bn,TRUE);
	BATkey(BATmirror(bn),triv_prop);
	bn->hsorted = GDK_SORTED;
	bn->tsorted = triv_prop;
	bn->hdense = triv_prop;
	bn->tdense = FALSE;
	if (BATcount(bn) == 0) {
		BATseqbase(bn, (oid)0);
	} else if (BATcount(bn) == 1) {
		BATseqbase(bn, *(oid*)Hloc(bn,BUNfirst(bn)));
	}
	*res = bn;

	return GDK_SUCCEED;
bunins_failed:
	GDKerror("combine_text_string: bunins failed.\n");
cleanup:
	if (actual_str) GDKfree(actual_str);
	BBPreclaim(bn);
	return GDK_FAIL;
}

int CMDstring_join ( BAT** res, BAT *iter_str, BAT *separator  )
{
	BATiter iter_stri = bat_iterator(iter_str), separatori = bat_iterator(separator);
	BAT *bn;
	BUN bun_iter_str, bun_sep, last_iter_str, last_sep;
	size_t len, sep_len, strsize = 1024;
	oid i, i0, sep_oid, sep0;
	str s, sep, actual_str = NULL;
	bit triv_prop, first_string;
	
	*res = NULL;

	/* check arguments */

	BATcheck(iter_str, "string_join");
	BATcheck(separator, "string_join");

	ERRORcheck(!(BAThordered(iter_str)&1),
		"string_join: input BAT iter_str must be sorted on head.\n");
	ERRORcheck(!(BAThordered(separator)&1),
		"string_join: input BAT separator must be sorted on head.\n");

	/* create result BAT */

	bn = BATnew(TYPE_oid, TYPE_str, BATcount(separator));
	if (bn == NULL) {
		GDKerror("string_join: BATnew(TYPE_oid, TYPE_str, %d) failed.\n", BATcount(separator));
		return GDK_FAIL;
	}

	/* do the string_join */

	bun_iter_str = BUNfirst(iter_str);
	last_iter_str = BUNlast(iter_str);
	bun_sep = BUNfirst(separator);
	last_sep = BUNlast(separator);

	/* handling the empty cases */

	if (bun_iter_str == last_iter_str && bun_sep == last_sep) {
		/* ... head */
		BATkey (bn, TRUE);
                bn->hsorted = GDK_SORTED;
		bn->hdense = TRUE;
		BATseqbase (bn, (oid)0); /* does not really matter */
		/* ... tail */
		BATkey (BATmirror(bn), TRUE);
                bn->tsorted = GDK_SORTED;
		bn->tdense = FALSE;
		*res = bn;
 	       	return GDK_SUCCEED;
	}
	else if (bun_sep == last_sep) {
		GDKerror("string_join: expected oid %i@0 in iter_str "
                         "is missing in separator (0 rows).",
                         *(oid*)BUNhead(iter_stri,bun_iter_str)); /* FIXME: "hvar" (void) vs. "hloc" (oid) */
		goto cleanup;
	}
	else if (bun_iter_str == last_iter_str) {
		sep0 = oid_nil;
		sep_oid = *(oid*)BUNhead(separatori,bun_sep); /* FIXME: "hvar" (void) vs. "hloc" (oid) */
		@:append_sep_and_set_props@
 	       	return GDK_SUCCEED;
	}

	/* allocate str buffer */
	actual_str = (str)GDKmalloc(strsize);
	if (actual_str == NULL) {
		GDKerror("string_join: GDKmalloc(" SZFMT ") failed.\n", strsize);
		goto cleanup;
	}
	len = 0;
	actual_str[0] = '\0';

	i0 = *(oid*)BUNhead(iter_stri,bun_iter_str); /* FIXME: "hvar" (void) vs. "hloc" (oid) */
	first_string = 1;
	sep0 = oid_nil;

@= get_next_separator
	/* - goes to the next row in the separator bat
	     to get the separator for the next iter value
	   - produces an error if oids are not aligned */
	sep_oid = *(oid*)BUNhead(separatori,bun_sep); /* FIXME: "hvar" (void) vs. "hloc" (oid) */

	while (i0 > sep_oid && bun_sep != last_sep) {
		if (sep0 == sep_oid) {
			GDKerror("string_join: the head of separator has to be keyed.");
			goto cleanup;
		}
		bunfastins(bn, &sep_oid, "");
		sep0 = sep_oid;
		bun_sep++;
	        sep_oid = *(oid*)BUNhead(separatori,bun_sep); /* FIXME: "hvar" (void) vs. "hloc" (oid) */
	}
	if (i0 == sep_oid && bun_sep != last_sep) {
		sep = (str)BUNtvar(separatori,bun_sep);
		sep_len = strlen(sep);
	}
	else {
		GDKerror("string_join: expected oid %i@0 in iter_str "
			 "is missing in separator.",
			 i0);
		goto cleanup;
	}
@c
	@:get_next_separator@

	for (; bun_iter_str < last_iter_str ; bun_iter_str++) {
		i = *(oid*)BUNhead(iter_stri,bun_iter_str); /* FIXME: "hvar" (void) vs. "hloc" (oid) */
		s = (str)BUNtvar(iter_stri,bun_iter_str);

		size_t l;

		if (i0 < i) {
			bunfastins(bn, &i0, actual_str);
			len = 0;
			actual_str[0] = '\0';
			i0 = i;
			first_string = 1;

			sep0 = sep_oid;
			bun_sep++;
			@:get_next_separator@
		}

		l = strlen(s) + sep_len + 1;

		if (len+l >= strsize) {
			/* extend str buffer */
			do {
				strsize *= 2;
			} while (len+l >= strsize);
			actual_str = GDKrealloc(actual_str, strsize);
			if (actual_str == NULL) {
				GDKerror("string_join: GDKrealloc(" SZFMT ") failed.\n", strsize);
				goto cleanup;
			}
		}

		/* adds the separator (starting before the second string) */

		if (first_string) {
			first_string = 0;
		}
		else {
			strcpy(actual_str+len, sep); /* we know it fits */
			len += sep_len;
		}

		strcpy(actual_str + len, s); /* we know it fits */
		len += l - sep_len - 1;	/* just strlen(s) */
	}

	/* inserts last row */

	if (len > 0) {
		bunfastins(bn, &i0, actual_str);
		sep0 = sep_oid;
		bun_sep++;
	}

@= append_sep_and_set_props
	while (bun_sep != last_sep) {
	        sep_oid = *(oid*)BUNhead(separatori,bun_sep); /* FIXME: "hvar" (void) vs. "hloc" (oid) */
		if (sep0 == sep_oid) {
			GDKerror("string_join: the head of separator has to be keyed.");
			goto cleanup;
		}
		bunfastins(bn, &sep_oid, "");
		sep0 = sep_oid;
		bun_sep++;
	}

	GDKfree(actual_str);

	/* set result properties */

	if (!bn->batDirty) bn->batDirty = TRUE;
	triv_prop = (BATcount(bn) < 2);
	BATkey(bn,TRUE);
	BATkey(BATmirror(bn),triv_prop);
	bn->hsorted = GDK_SORTED;
	bn->tsorted = triv_prop;
	bn->hdense = triv_prop;
	bn->tdense = FALSE;
	if (BATcount(bn) == 0) {
		BATseqbase(bn, (oid)0);
	} else if (BATcount(bn) == 1) {
		BATseqbase(bn, *(oid*)Hloc(bn,BUNfirst(bn)));
	}
	*res = bn;
@c
	@:append_sep_and_set_props@

	return GDK_SUCCEED;
bunins_failed:
	GDKerror("string_join: bunins failed.\n");
cleanup:
	if (actual_str) GDKfree(actual_str);
	BBPreclaim(bn);
	return GDK_FAIL;
}

int CMDenumerate ( BAT** res, BAT *startval, BAT *length )
{
        BAT *bn;
        BUN cur_bun, length_bun, bun_last;
	lng number, counter, result_size;
        size_t cnt;
        oid base, head_oid;
        bit triv_prop;
	BATiter startvali = bat_iterator(startval);

        /* check arguments */

        BATcheck(startval, "enumerate");
        BATcheck(length, "enumerate");

        cnt = BATcount(startval);
        base = startval->hseqbase;
        ERRORcheck(!BAThdense(startval) || !BAThdense(length), 
                "enumerate: input BATs (startval, length) must be void-headed with non-nil seqbase.\n");
        ERRORcheck(BATcount(length)!=cnt || length->hseqbase!=base,
                "enumerate: input BATs (startval, length) must be head-aligned.\n");

        /* create result BAT */
        if (CMDsum_lng_lng(&result_size, length) == GDK_FAIL) {
                GDKerror("enumerate: summing up 'length' failed.\n");
                return GDK_FAIL;
        }

        bn = BATnew(TYPE_oid, TYPE_lng, result_size);
        if (bn == NULL) {
                GDKerror("enumerate: BATnew(TYPE_oid, TYPE_lng, %d) failed.\n", result_size);
                return GDK_FAIL;
        }

        /* do the enumerate */

        for (cur_bun = BUNfirst(startval), length_bun = BUNfirst(length),
             bun_last = BUNlast(startval);
             cur_bun < bun_last;
             cur_bun++, length_bun++)
        {
	        head_oid = *(oid*)BUNhead(startvali,cur_bun); /* FIXME: "hvar" (void) vs. "hloc" (oid) */
                number = *(lng*) Tloc (startval, cur_bun);
                counter = *(lng*) Tloc (length, length_bun);
                while (counter)
                {
		        bunfastins(bn, &head_oid, &number);
                        number++;
                        counter--;
                }
        }

	if (!bn->batDirty) bn->batDirty = TRUE;
	triv_prop = (BATcount(bn) < 2);
	BATkey(bn,triv_prop);
	BATkey(BATmirror(bn),triv_prop);
	bn->hsorted = GDK_SORTED;
	bn->tsorted = triv_prop;
	bn->hdense = triv_prop;
	bn->tdense = FALSE;
	if (BATcount(bn) == 0) {
		BATseqbase(bn, (oid)0);
	} else if (BATcount(bn) == 1) {
		BATseqbase(bn, *(oid*)Hloc(bn,BUNfirst(bn)));
	}
	*res = bn;

	return GDK_SUCCEED;
bunins_failed:
	GDKerror("enumerate: bunins failed.\n");
	BBPreclaim(bn);
	return GDK_FAIL;
}

int CMDmposjoin ( BAT** res, BAT* pre, BAT* cont, BAT* ws_item )
{
	BATiter prei = bat_iterator(pre);
	BATiter conti = bat_iterator(cont), ws_itemi = bat_iterator(ws_item);
	BAT *bn, **batlist = NULL, *the_cont_bat = NULL;
	BUN q, pp, pf, pw, dst;
	int bsf = 0, ii = 0, tt = 0;
	size_t cnt, len;
	oid base, wl, wh;
	bit triv_prop;
	bit fake_cont = FALSE;
	bit fake_ws_item = FALSE;
	
	*res = NULL;

	/* check arguments */

	BATcheck(pre, "mposjoin");
	BATcheck(cont, "mposjoin");
	BATcheck(ws_item, "mposjoin");

	len = BATcount(ws_item);
	cnt = BATcount(pre);
	base = pre->hseqbase;
	fake_cont = is_fake_project(cont);
	
	if (fake_cont) {
		ERRORcheck(!BAThdense(pre), 
			"mposjoin: input BAT pre must have a dense head.\n");
		ERRORcheck(!BAThdense(ws_item), 
			"mposjoin: input BAT ws_item must have a dense head.\n");
	} else {
		ERRORcheck(!BAThdense(pre) || !BAThdense(cont) || !BAThdense(ws_item), 
			"mposjoin: all input BATs (pre, cont, ws_item) must have a dense head.\n");
		ERRORcheck(BATcount(cont)!=cnt || cont->hseqbase!=base,
			"mposjoin: first two input BATs (pre & cont) must be head-aligned.\n");
	}
	ERRORcheck(len==0,
		"mposjoin: third input BAT (ws_item) must not be empty.\n");

	the_cont_bat = BATdescriptor(*(bat*)Tloc(ws_item, BUNfirst(ws_item)));
	tt = the_cont_bat->ttype;
	BBPunfix(the_cont_bat->batCacheid);
	the_cont_bat = NULL;

	if (cnt==0) {
		@:mpos_res_empty@
	}

	ii = 0;
	
	if (fake_cont) {
		oid the_cont_id = *(oid*)BUNtail(conti, BUNfirst(cont));
		len = 1;
		batlist = &the_cont_bat;
		BUNfndVOID(pw, ws_itemi, &the_cont_id);
		if (pw==BUN_NONE) {
			@:mpos_res_empty@
		}

		@:mpos_init_batlist@
		if (!fake_ws_item) {
			*res = bn = BATleftfetchjoin(pre, batlist[0], oid_nil);
			@:mpos_free_batlist(0)@
			if (bn == NULL) {
				GDKerror("mposjoin: BATleftfetchjoin(pre, ws_item["SZFMT"]) failed.\n",
					 the_cont_id);
				return GDK_FAIL;
			}
			return GDK_SUCCEED;
		}
	
	} else {
		batlist = (BAT**)GDKmalloc(len * sizeof(BAT*));
		if (batlist == NULL) {
			GDKerror("mposjoin: GDKmalloc(" SZFMT ") failed.\n", len * sizeof(BAT*));
			return GDK_FAIL;
		}
		BATloop(ws_item, pw, q) {
			@:mpos_init_batlist@
		}
	}

@= mpos_init_batlist
{
		bit fake_item = FALSE;
		bat bid = *(bat*)Tloc(ws_item, pw);
		batlist[ii] = BATdescriptor(bid);
		fake_item = is_fake_project(batlist[ii]);
		fake_ws_item |= fake_item;
		if (!(fake_item || BAThdense(batlist[ii]))) {
			GDKerror("mposjoin: all BATs in the tail of the third input BAT (ws_item) must have a dense head.\n");
			ii++;
			@:mpos_free_batlist(0)@
			return GDK_FAIL;
		}
		if (ATOMtype(batlist[0]->ttype) != ATOMtype(batlist[ii]->ttype)) {
			GDKerror("mposjoin: all BATs in the tail of the third input BAT (ws_item) must have the same tail type (%d: %d != %d).\n",
				ii, batlist[ii]->ttype, batlist[0]->ttype);
			ii++;
			@:mpos_free_batlist(0)@
			assert(0);
			return GDK_FAIL;
		}
		ii++;
}	
@
@= mpos_res_empty
	cnt = 0;
	@:mpos_res_create@
	@:mpos_res_prop@

	return GDK_SUCCEED;
@
@= mpos_res_create
	/* create result BAT */

	bn = BATnew(TYPE_void, ATOMtype(tt), cnt);
	if (bn == NULL) {
		GDKerror("mposjoin: BATnew(TYPE_void, %s, " SZFMT ") failed.\n", ATOMname(ATOMtype(tt)), cnt);
		@:mpos_free_batlist(0)@
		return GDK_FAIL;
	}
	BATseqbase(bn, base);
@c
	@:mpos_res_create@
	dst = BUNlast(bn);

	/* do the mposjoin */

	wl = ws_item->hseqbase;
	wh = wl + len - 1;

	if (fake_cont) {
		bsf = 0;
	} else {
		bsf = 1;
	}

	pp = BUNfirst(pre);
	pf = BUNfirst(cont);

@= mposjoin
	/*  @1: ATOMstorage(ATOMtype(batlist[0]->ttype)) (chr, sht, int, flt, lng, dbl, var, fix)
	 *  @2: tloc, tvar, tail
	 */
	 if (fake_cont) {
	 	/* !fake_ws_item handled above */
	 	@:mposjoin_fake_fake(@1,@2)@
	 } else {
		if (fake_ws_item) {
			@:mposjoin_(@1,@2,is_fake_project(b))@
		} else {
			@:mposjoin_(@1,@2,FALSE)@
		}
	}
	break;
@
@= mposjoin_fake_fake
	/*  @1: ATOMstorage(ATOMtype(batlist[0]->ttype)) (chr, sht, int, flt, lng, dbl, any)
	 *  @2: tloc, tvar, tail
	 */
	BAT *b = batlist[0];
	BATiter bi = bat_iterator(b);
	ptr  w = BUN@2(bi, BUNfirst(b));
	for (q = BUNlast(pre) ; pp < q ; pp++, dst++) {
		void@1_bunfastins_nocheck_noinc(bn,dst,0,w);
	}
@
@= mposjoin_
	/*  @1: ATOMstorage(ATOMtype(batlist[0]->ttype)) (chr, sht, int, flt, lng, dbl, any)
	 *  @2: tloc, tvar, tail
	 *  @3: FALSE / is_fake_project(b)
	 */
	for (q = BUNlast(pre) ; pp < q ; pp++, pf += bsf) {
		oid p = *(oid*)BUNtail(prei,pp);  /* FIXME: "tvar" (void) vs. "tloc" (oid) */
		oid f = *(oid*)BUNtail(conti,pf); /* FIXME: "tvar" (void) vs. "tloc" (oid) */
		if (f >= wl && f <= wh) {
			BAT *b = batlist[f - wl];
			BATiter bi = bat_iterator(b);
			if (@3) {
				void@1_bunfastins_nocheck_noinc(bn,dst,0,BUN@2(bi,BUNfirst(b)));
				dst++;
			} else {
				oid l = b->hseqbase;
				oid h = l + BATcount(b) - 1;
				if (p >= l && p <= h) {
					oid b0 = BUNfirst(b);
					void@1_bunfastins_nocheck_noinc(bn,dst,0,BUN@2(bi,p-l+b0));
					dst++;
				} 
			}
		}
	}
@c

	switch(ATOMstorage(ATOMtype(batlist[0]->ttype))) {
	case TYPE_chr:	@:mposjoin(chr,tloc,simple)@
	case TYPE_sht:	@:mposjoin(sht,tloc,simple)@
#if SIZEOF_OID == SIZEOF_INT
	/* cannot use tloc on oid(void) */
	case TYPE_int:	@:mposjoin(int,tail,simple)@
#else
	case TYPE_int:	@:mposjoin(int,tloc,simple)@
#endif
	case TYPE_flt:	@:mposjoin(flt,tloc,simple)@
#if SIZEOF_OID == SIZEOF_LNG
	/* cannot use tloc on oid(void) */
	case TYPE_lng:	@:mposjoin(lng,tail,simple)@
#else
	case TYPE_lng:	@:mposjoin(lng,tloc,simple)@
#endif
	case TYPE_dbl:	@:mposjoin(dbl,tloc,simple)@
	default:
		if (batlist[0]->tvarsized) {
			@:mposjoin(var,tvar,atom)@
		} else {
			@:mposjoin(fix,tloc,atom)@
		}
	}
	BATsetcount(bn, dst - BUNfirst(bn));

@= mpos_res_prop
    /* check result size */
    if (BATcount(pre) > BATcount(bn)) {
		GDKerror("mposjoin: missing matches. The result requires the "
                 "same number of tuples as the first two input arguments: "SZFMT" < "SZFMT".", BATcount(bn), BATcount(pre));
		@:mpos_free_batlist(bn)@
		return GDK_FAIL;
	}
    if (BATcount(pre) < BATcount(bn)) {
		GDKerror("mposjoin: more than one match per input tuple. "
                 "seqbase does not match anymore.");
		@:mpos_free_batlist(bn)@
		return GDK_FAIL;
	}

	/* set result properties */

	if (!bn->batDirty) bn->batDirty = TRUE;
	triv_prop = (BATcount(bn) < 2);
	BATkey(bn,TRUE);
	BATkey(BATmirror(bn),triv_prop);
	bn->hsorted = GDK_SORTED;
	bn->tsorted = triv_prop;
	bn->hdense = TRUE;
	bn->tdense = FALSE;

	*res = bn;
@c
	@:mpos_res_prop@
	@:mpos_free_batlist(0)@

	return GDK_SUCCEED;
bunins_failed:
	GDKerror("mposjoin: bunins failed.\n");
@= mpos_free_batlist
	if (@1) BBPreclaim(bn);
	while (ii > 0) {
		BBPunfix(batlist[--ii]->batCacheid);
	}
	if (batlist && !fake_cont) {
		GDKfree(batlist);
	}
@c
	@:mpos_free_batlist(bn)@
	return GDK_FAIL;
}


int CMDmvaljoin ( BAT** res, BAT* pre, BAT* cont, BAT* ws_item )
{
	BATiter prei = bat_iterator(pre);
	BATiter conti = bat_iterator(cont), ws_itemi = bat_iterator(ws_item);
	BAT *bn, **batlist = NULL, *the_cont_bat = NULL;
	BUN q, pp, pf, pw;
	int ii = 0;
	size_t cnt, len, sze;
	oid base, wl, wh;
	bit triv_prop, all_hash, all_sort, some_hash, some_sort, all_key, fake_cont = FALSE;
	
	*res = NULL;

	/* check arguments */

	BATcheck(pre, "mvaljoin");
	BATcheck(cont, "mvaljoin");
	BATcheck(ws_item, "mvaljoin");

	len = BATcount(ws_item);
	cnt = BATcount(pre);
	base = pre->hseqbase;
	fake_cont = is_fake_project(cont);
	
	if (fake_cont) {
		ERRORcheck(!BAThdense(pre) || !BAThdense(ws_item), 
			"mvaljoin: input BATs pre & ws_item must have a dense head.\n");
	} else {
		ERRORcheck(!BAThdense(pre) || !BAThdense(cont) || !BAThdense(ws_item), 
			"mvaljoin: all input BATs (pre, cont, ws_item) must have a dense head.\n");
		ERRORcheck(BATcount(cont)!=cnt || cont->hseqbase!=base,
			"mvaljoin: first two input BATs (pre & cont) must be head-aligned.\n");
	}
	ERRORcheck(len==0,
		"mvaljoin: third input BAT (ws_item) must not be empty.\n");

	sze = cnt;
	ii = 0;
	all_hash = all_sort = all_key = TRUE;
	some_hash = some_sort = FALSE;
	
	if (sze == 0) {
		@:mval_res_empty@
	}
	if (fake_cont) {
		oid the_cont_id = *(oid*)BUNtail(conti, BUNfirst(cont));
		len = 1;
		batlist = &the_cont_bat;
		BUNfndVOID(pw, ws_itemi, &the_cont_id);
		if (pw==BUN_NONE) {
			@:mval_res_empty@
		}

		@:mval_init_batlist@
		*res = bn = BATleftjoin(pre, BATmirror(batlist[0]), oid_nil);
		@:mval_free_batlist@
		if (bn == NULL) {
			GDKerror("mvaljoin: BATleftjoin(pre, BATmirror(ws_item["SZFMT"])) failed.\n",
				 the_cont_id);
			return GDK_FAIL;
		}
		return GDK_SUCCEED;

	} else {
		batlist = (BAT**)GDKmalloc(len * sizeof(BAT*));
		if (batlist == NULL) {
			GDKerror("mvaljoin: GDKmalloc(" SZFMT ") failed.\n", len * sizeof(BAT*));
			return GDK_FAIL;
		}
		BATloop(ws_item, pw, q) {
			@:mval_init_batlist@
		}
	}
	if (all_key) {
		sze = cnt;
	}

@= mval_init_batlist
{
	bit hsh=0, srt;
	bat bid = *(bat*)Tloc(ws_item, pw);
	batlist[ii] = BATdescriptor(bid);
	srt = (BATtordered(batlist[ii])&1);
        hsh = (BAThash(BATmirror(batlist[ii]), 0) != NULL);
        if (batlist[ii]->ttype != TYPE_oid) {
                GDKerror("mvaljoin: all BATs in the tail of the third input BAT (ws_item) must have tail type OID.\n");
                ii++;
                @:mval_free_batlist@
                return GDK_FAIL;
        }
	all_hash &= hsh;
	all_sort &= srt;
	some_hash |= hsh;
	some_sort |= srt;
	all_key &= (batlist[ii]->tkey!=0);
	sze = MAX(sze, BATcount(batlist[ii]));
	ii++;
}	
@
@= mval_res_empty
	sze = 0;
	@:mval_res_create@
	@:mval_res_prop@

	return GDK_SUCCEED;
@
@= mval_res_create
	/* create result BAT */

	bn = BATnew(TYPE_oid, TYPE_oid, sze);
	if (bn == NULL) {
		GDKerror("mvaljoin: BATnew(TYPE_oid, TYPE_oid, " SZFMT ") failed.\n", sze);
		@:mval_free_batlist@
		return GDK_FAIL;
	}
@c
	@:mval_res_create@

	/* do the mvaljoin */

	wl = ws_item->hseqbase;
	wh = wl + len - 1;

	pp = BUNfirst(pre);
	pf = BUNfirst(cont);

@= hash_only
	hash_t h;
	BATiter bi;
	bi.b = b = BATmirror(b);
	HASHloop_oid(bi, b->H->hash, h, (ptr)(&p)) {
		oidoid_bunfastins(bn,&base,BUNtail(bi,h)); /* FIXME: "tvar" (void) vs. "tloc" (oid) */
	}
@= sort_only
	BATiter bi = bat_iterator(b);
	BUN rp, rq;
	SORTloop_oid(b, rp, rq, (ptr)(&p), (ptr)(&p)) {
		oidoid_bunfastins(bn,&base,BUNhead(bi,rp)); /* FIXME: "hvar" (void) vs. "hloc" (oid) */
	}
@= scan_only
	BATiter bi = bat_iterator(b);
	BUN rp, rq;
	BATloop(b, rp, rq) {
		if (p == *(oid*)BUNtail(bi,rp)) { /* FIXME: "tvar" (void) vs. "tloc" (oid) */
			oidoid_bunfastins(bn,&base,BUNhead(bi,rp)); /* FIXME: "hvar" (void) vs. "hloc" (oid) */
		}
	}
@= hash_scan
	if (b->T->hash) {
		@:hash_only@
	} else {
		@:scan_only@
	}
@= sort_scan
	if (BATtordered(b)&1) {
		@:sort_only@
	} else {
		@:scan_only@
	}
@= hash_sort_scan
	if (b->T->hash) {
		@:hash_only@
	} else if (BATtordered(b)&1) {
		@:sort_only@
	} else {
		@:scan_only@
	}
@= mvaljoin
	for (q = BUNlast(pre) ; pp < q ; pp++, pf++) {
		oid p = *(oid*)BUNtail(prei,pp); /* FIXME: "tvar" (void) vs. "tloc" (oid) */
		oid f = *(oid*)BUNtail(conti,pf); /* FIXME: "tvar" (void) vs. "tloc" (oid) */
		if (f >= wl && f <= wh) {
			BAT *b = batlist[f - wl];
			@1
		}
		base++;
	}
@c

	if (all_hash) {
		@:mvaljoin(@:hash_only@)@
	} else if (all_sort) {
		@:mvaljoin(@:sort_only@)@
	} else if (some_hash && some_sort) {
		@:mvaljoin(@:hash_sort_scan@)@
	} else if (some_hash) {
		@:mvaljoin(@:hash_scan@)@
	} else if (some_sort) {
		@:mvaljoin(@:sort_scan@)@
	} else {
		@:mvaljoin(@:scan_only@)@
	}

@= mval_res_prop
	/* set result properties */

	if (!bn->batDirty) bn->batDirty = TRUE;
	triv_prop = (BATcount(bn) < 2);
	BATkey(bn,triv_prop||all_key);
	BATkey(BATmirror(bn),triv_prop);
	bn->hsorted = GDK_SORTED;
	bn->tsorted = triv_prop;
	bn->hdense = triv_prop;
	bn->tdense = triv_prop;
	if (BATcount(bn) == 0) {
		BATseqbase(bn, (oid)0);
		BATseqbase(BATmirror(bn), (oid)0);
	} else if (BATcount(bn) == 1) {
		BATseqbase(bn, *(oid*)Hloc(bn,BUNfirst(bn)));
		BATseqbase(BATmirror(bn), *(oid*)Tloc(bn,BUNfirst(bn)));
	}

	*res = bn;
@c
	@:mval_res_prop@
	@:mval_free_batlist@

	return GDK_SUCCEED;
bunins_failed:
	GDKerror("mvaljoin: bunins failed.\n");
	BBPreclaim(bn);
@= mval_free_batlist
	while (ii > 0) {
		BBPunfix(batlist[--ii]->batCacheid);
	}
	if (batlist && !fake_cont) {
		GDKfree(batlist);
	}
@c
	@:mval_free_batlist@
	return GDK_FAIL;
}


int CMDdblFromBytes(dbl* dst, int *b0, int *b1, int *b2, int *b3, int *b4, int *b5, int *b6, int *b7) {
    char *b = (char*) dst;
    b[0] = *b0; b[1] = *b1; b[2] = *b2; b[3] = *b3; b[4] = *b4; b[5] = *b5; b[6] = *b6; b[7] = *b7;
    return GDK_SUCCEED;
}


/*
 * Worker for the ebv() function.
 */
BAT *
BATebv (BAT *b)
{
    BAT *ret = NULL;    /* return value */
    BUN p = 0, q = 0;   /* BUN variables for iteration */
    oid old;            /* Last head value we had seen */
    bit val;            /* Boolean result value that belongs to `old' */
    size_t cnt = 0;     /* "guess" of result cardinality */
    bit trivial;        /* indicator for "trivial" result properties */

    /* Just in case BATebv might be called from elsewhere than CMDebv:
     * check, that b in not NULL.
     */
    BATcheck(b, "BATebv");
    /* Input must be sorted by head value, tail must be boolean */
    ERRORcheck (!(BAThordered(b)&1), "BATebv: head of BAT must be sorted.\n");
    /* Needed only, if you want to disallow TYPE_void, 
     * which also matches the oid requirement of the signature;
     * or if BATebv could be called from somewhere/one else than CMDebv.
     */
    ERRORcheck (!(b->htype == TYPE_oid),
                "BATebv: head of BAT must have oid type.\n");
@(
    /* Not needed, if BATebv is only called from CMDebv,
     * since the signature only allows bit-tailed BATs.
     */
    ERRORcheck (!(b->ttype == TYPE_bit),
                "BATebv: tail of BAT must have bit type.\n");
@)

    /* Try to "guess" the result cardinality:
     * on the one hand, we don't want too allocate (far) too much memory;
     * on the other hand, we want to avoid BATextends (i.e., (large) memcpy's),
     * that occur if we initially allow too little space ... 
     * Obviously, BATcount(b) is the upper limit;
     * lower limit is just a "wild guess"...
     */
    if (BAThkey (b))
        cnt = BATcount (b);
    else
        cnt = MIN (200, BATcount (b));

    /* Create return BAT */
    ret = BATnew (b->htype, TYPE_bit, cnt);
    if (!ret)
        return ret;

    /*
     * Iterate over the input BAT.
     * Whenever we see a head value the first time, we record its tail
     * value in val. If we see the same head value a second (third,...)
     * time, we re-set val to false. When we reach the next head
     * value, (old, val) will be the correct BUN for the last group.
     */

    /* Initialize, ... */
    p = BUNfirst (b);
    old = *(oid *) Hloc (b, p);
    val = *(bit *) Tloc (b, p);
    /* ... skip first, ... */
    p++;
    /* ... and process the rest. */
    for(q = BUNlast(b); p < q; p++) {
    	oid *head = (oid *) Hloc (b, p);
        if (*head == old)
            val = FALSE;
        else {
            bunfastins (ret, &old, &val);
            old = *head;
            val = *(bit *) Tloc (b, p);
        }
    }
    /* Don't forget to produce the last BUN. */
    bunfastins (ret, &old, &val);
    
    /* Set result properties ... */
    cnt = BATcount (ret);
    trivial = (cnt < 2) ? TRUE : FALSE;
    /* ... head ... */
    BATkey (ret, TRUE);
    ret->hsorted = GDK_SORTED;
    ret->hdense = trivial;
    if (trivial == TRUE) {
        if (cnt == 0)
            BATseqbase (ret, (oid)0); /* does not really matter */
        else /* (cnt == 1) */
            BATseqbase (ret, old);
    }
    /* ... tail */
    BATkey (BATmirror(ret), trivial);
    ret->tsorted = trivial;
    ret->tdense = FALSE;

    return ret;

/* required by bunfastins macro */
bunins_failed:
    BBPreclaim(ret);
    return NULL;
}

/*
 * Implementation of ebv(). Basically just calls BATebv().
 */
int
CMDebv (BAT **result, BAT *b)
{
    return (*result = BATebv (b)) ? GDK_SUCCEED : GDK_FAIL;
}

int
CMDinvalid_qname(str *ret, BAT *b) 
{
	BATiter bi = bat_iterator(b);
        str s = ATOMnilptr(TYPE_str);
	BUN p, q;

	BATloop(b,p,q) {
            char *r = BUNtail(bi,p);
	    if ((*r >= 'a' && *r <= 'z') || (*r >= 'A' && *r <= 'Z') || *(unsigned char*) r >= 128) {
                r++;
                while((*r >= 'a' && *r <= 'z') || (*r >= 'A' && *r <= 'Z') || (*(unsigned char*) r >= 128) ||
                      (*r == '_') || (*r == '.') || (*r == '-') || (*r >= '0' && *r <= '9')) r++;
                if (*r == 0) continue; /* ok */
            }
            /* prefix --- local part delimiter ':' */
            if (*r == ':') r++;
	    if ((*r >= 'a' && *r <= 'z') || (*r >= 'A' && *r <= 'Z') || *(unsigned char*) r >= 128) {
                r++;
                while((*r >= 'a' && *r <= 'z') || (*r >= 'A' && *r <= 'Z') || (*(unsigned char*) r >= 128) ||
                      (*r == '_') || (*r == '.') || (*r == '-') || (*r >= '0' && *r <= '9')) r++;
                if (*r == 0) continue; /* ok */
            }
	    s = (str) BUNtail(bi,p); break;
	}
	*ret = GDKstrdup(s);
        return GDK_SUCCEED;

}

int
CMDlastmod_time(timestamp *ret, str filename) {
	struct stat st;
	*(lng*) ret = lng_nil;
	if (stat(filename, &st) == 0) {
		lng msecs = 1000*st.st_mtime;
		int year = 1970, one = 1, zero = 0;
		timestamp ts;
        	daytime dt;
		date d;
		tzone tz;
        	return date_create(&d, &year, &one, &one) &&
             	       daytime_create(&dt, &zero, &zero, &zero, &zero) &&
	               tzone_create(&tz, &zero) &&
                       timestamp_create(&ts, &d, &dt, &tz) &&
                       timestamp_add(ret, &ts, &msecs);
        }
	return GDK_SUCCEED;
}


typedef struct stack_item si;

struct stack_item {
	int *sze;	/* size to be updated */
	oid pre;	/* pre-order rank of subtree root (without holes) */
	oid limit;	/* pre-order rank of last node in subtree (with holes) */
};

#define PUSH(stack,s,p,l) \
	stack_top++;\
	if (stack_top >= stack_size) {\
		si* newstack = (si*) GDKrealloc(stack, (stack_size*=2)*sizeof(si));\
		if (newstack == NULL) {\
			GDKerror("correct_sizes: could not re-allocate stack of size %d.\n", stack_size*sizeof(si));\
			GDKfree(stack);\
	 		return GDK_FAIL;\
		}\
                stack = newstack;\
	}\
	stack[stack_top].sze = s;\
	stack[stack_top].pre = p;\
	stack[stack_top].limit = l;

#define POP(stack,s) \
	si s = stack[stack_top];\
	stack_top--;

/* StM:
 * Open: Does the following still work properly even if we're dealing with
 * multiple fragments in function map2NODE_interface() in
 * compiler/mil/milprint_summer.c ?
 */
int
CMDcorrect_sizes(BAT **ret, BAT *bat_iter, BAT *bat_item, BAT *bat_size) {
	BATiter bat_iteri = bat_iterator(bat_iter);
	BATiter bat_itemi = bat_iterator(bat_item);
	BATiter bat_sizei = bat_iterator(bat_size);
	BUN cur_iter, cur_item, cur_size;
	BUN fst_size, lst_size;
	int bs_iter;
	size_t cnt;
	oid base;
	oid prev_iter, prev_item, pre;
	bit fake_iter;
	si *stack = NULL;
	int stack_top = -1, stack_size = 128;
	
	/* check arguments */

	BATcheck(bat_iter, "correct_sizes");
	BATcheck(bat_item, "correct_sizes");
	BATcheck(bat_size, "correct_sizes");

	cnt = BATcount(bat_size);
	base = bat_size->hseqbase;
	fake_iter = is_fake_project(bat_iter);
	
	ERRORcheck(!((fake_iter || BAThdense(bat_iter)) && BAThdense(bat_item) && BAThdense(bat_size)), 
		"correct_sizes: all input BATs (iter, item, size) must have a dense head.\n");
	ERRORcheck(!((fake_iter || BATcount(bat_iter)==cnt) && BATcount(bat_item)==cnt),
		"correct_sizes: all input BATs (iter, item, size) must have the same size.\n");
	ERRORcheck(!((fake_iter || bat_iter->hseqbase==base) && bat_item->hseqbase==base),
		"correct_sizes: all input BATs (iter, item, size) must have the same seqbase.\n");

	stack = (si*)GDKmalloc(stack_size*sizeof(si));
	if (stack == NULL) {
		GDKerror("correct_sizes: could not allocate stack of size %d.\n", stack_size*sizeof(si));
	 	return GDK_FAIL;
	}
	stack_top = -1;

	bs_iter = (fake_iter?0:1);
	cur_iter = BUNfirst(bat_iter);
	cur_item = BUNfirst(bat_item);
	fst_size = BUNfirst(bat_size);
	lst_size = BUNlast(bat_size);

	prev_iter = oid_nil;
	prev_item = oid_nil;
	pre = oid_nil;
	for (cur_size = fst_size ; cur_size < lst_size ; cur_size++, cur_item++, cur_iter += bs_iter) {
		oid iter = *(oid*)BUNtail(bat_iteri, cur_iter);
		oid item = *(oid*)BUNtail(bat_itemi, cur_item);
		int *sze =  (int*)BUNtail(bat_sizei, cur_size);
		int size = *sze;
		    pre  = *(oid*)BUNhead(bat_sizei, cur_size);
		while (stack_top >= 0 && ( iter != prev_iter || item > stack[stack_top].limit || item < prev_item )) {
			POP(stack, s);
			if (s.limit > prev_item) {
				*s.sze = pre - s.pre;
			} else {
				*s.sze = pre - s.pre - 1;
			}
		}
		if (size > 0) {
			PUSH(stack, sze, pre, item+size);
		}
		prev_iter = iter;
		prev_item = item;
	}
	while (stack_top >= 0) {
		POP(stack, s);
		if (s.limit > prev_item) {
			*s.sze = pre - s.pre + 1;
		} else {
			*s.sze = pre - s.pre;
		}
	}
	GDKfree(stack);

	if (!bat_size->batDirty) bat_size->batDirty = TRUE;

	BBPfix(bat_size->batCacheid);
	*ret = bat_size;
	return GDK_SUCCEED;
}


int
CMDSplitBat(BAT **res, BAT *b, str sep)
{
	BATiter bi = bat_iterator(b);
	BATiter *bn;
	size_t nbats = 0;
	size_t maxbats = 16;
	BUN p, q, r;
	str s, e;
	size_t seplen = strlen(sep);
	char *buf = GDKmalloc(BUFSIZ);
	size_t buflen = BUFSIZ;
	size_t l;
	size_t i;
	size_t bs = BATcount(b);
	oid base, o;

	if (buf == NULL)
		return GDK_FAIL;

	ERRORcheck(seplen == 0, "splitbat: separator must not be empty.\n");

	o = base = b->hseqbase;
	bn = GDKmalloc(maxbats * sizeof(BATiter));
	if (bn == NULL)
		goto bunins_failed;
	memset(bn, 0, maxbats * sizeof(BATiter));
	BATloop(b, p, q) {
		s = (str) BUNtail(bi, p);
		l = strlen(s);
		if (l >= buflen) {
			while (l >= (buflen += BUFSIZ))
				;
			buf = GDKrealloc(buf, buflen);
			if (buf == NULL)
				goto bunins_failed;
		}
		strcpy(buf, s);
		s = buf;
		i = 0;
		while (s) {
			if ((e = strstr(s, sep)) != NULL)
				*e = 0;
			if (i >= nbats) {
				size_t j;

				if (nbats >= maxbats) {
					bn = GDKrealloc(bn, maxbats + 16*sizeof(BATiter));
					if (bn == NULL)
						goto bunins_failed;
					memset(&bn[maxbats], 0, 16 * sizeof(BATiter));
					maxbats += 16;
				}
				bn[i].b = BATnew(TYPE_void, TYPE_str, bs);
				if (bn[i].b == NULL)
					goto bunins_failed;
				bn[i].b = BATseqbase(bn[i].b, base);
				BATkey(BATmirror(bn[i].b), FALSE);
				bn[i].b->tsorted = 0;
				for (j = 0; j < bs; j++)
					bunfastins(bn[i].b, NULL, str_nil);
				nbats++;
			}
			BUNfndVOID(r, bn[i], &o);
			ATOMreplace(TYPE_str, bn[i].b->theap, Tloc(bn[i].b, r), s);
			i++;
			s = e ? e + seplen : NULL;
		}
		o++;
	}
	GDKfree(buf);
	*res = BATnew(TYPE_void, TYPE_bat, nbats);
	if (*res == NULL)
		goto bunins_failed;
	*res = BATseqbase(*res, 0);
	BATkey(BATmirror(*res), FALSE);
	(*res)->tsorted = 0;
	for (i = 0; i < nbats; i++) {
		bunfastins(*res, NULL, &bn[i].b->batCacheid);
		BBPunfix(bn[i].b->batCacheid);
	}
	GDKfree(bn);
	return GDK_SUCCEED;

  bunins_failed:
	if (bn) {
		for (i = 0; i < nbats; i++)
			if (bn[i].b)
				BBPreclaim(bn[i].b);
		GDKfree(bn);
	}
	if (buf)
		GDKfree(buf);
	return GDK_FAIL;
}

#define MAXKIND 5
int
CMDSplitKind(BAT **res, BAT *b)
{
	size_t i, n = BATcount(b);
	BAT *bn[MAXKIND];
	oid *cur[MAXKIND];

	for (i = 0; i < MAXKIND; i++)
		bn[i] = NULL;

	for (i = 0; i < MAXKIND; i++) {
		bn[i] = BATnew(TYPE_oid, TYPE_void, n);
		if (bn[i] == NULL) goto bunins_failed;

		/* inherit sortedness and keyness */
		bn[i]->tsorted = 0;
		bn[i]->hsorted = b->hsorted;
		BATkey(bn[i], b->hkey & 1);
		cur[i] = (oid*) Hloc(bn[i], BUNfirst(bn[i]));
	}
	if (BAThdense(b)) {
		/* fast split-select with direct oid-inserts and predication */
		chr *v = ((chr*) Tloc(b,BUNfirst(b))) - b->hseqbase;	
		for(i=b->hseqbase, n+=b->hseqbase; i<n; i++) {
			int val = v[i];
			if ((val >= 0) & (val < MAXKIND))
				*cur[val]++ = i;
		}
	} else {
		BUN p, q;
		assert(b->htype == TYPE_void);
		BATloop(b, p, q) {
			int val = *(chr*) Tloc(b, p);
			if ((val >= 0) & (val < MAXKIND))
				*cur[val]++ = *(oid*) Hloc(b,p);
		}
	}
	for(i=0; i<MAXKIND; i++) {
		BATsetcount(bn[i], cur[i] - (oid*)bn[i]->H->heap.base);
	}
	*res = BATnew(TYPE_void, TYPE_bat, MAXKIND);
	if (*res == NULL)
		goto bunins_failed;
	*res = BATseqbase(*res, 0);
	BATkey(BATmirror(*res), FALSE);
	(*res)->tsorted = 0;
	for (i = 0; i < MAXKIND; i++) {
		bunfastins(*res, NULL, &bn[i]->batCacheid);
		BBPunfix(bn[i]->batCacheid);
	}
	return GDK_SUCCEED;
bunins_failed:
	for (i = 0; i < MAXKIND; i++)
		if (bn[i]) BBPreclaim(bn[i]);
	return GDK_FAIL;
}

@= nilarithC
int
CMDnil@1(int *res, int *v1, int *v2)
{
	*res = *v1 @2 *v2;
	return GDK_SUCCEED;
}
@c
@:nilarithC(and,&)@
@:nilarithC(or,|)@
@:nilarithC(plus,+)@


@+ Global Transaction Locking

While concurrency control in MonetDB/XQuery is optimistic and fine-grained (ehh, per page
conflict detection) there is some coarse grained control as well. It serves to avoid
thrashing on conflicting update loads: the database is automatically brought into serial 
update mode. Also, querying the meta-tables leads to full blocking of document management
(shred doc/delete doc) queries.

We have three hooks inserted in the MIL transaction mgmt code:
- pflock_begin() (called from ws_create) => QUERY STARTS
- pflock_end()   (called from ws_destroy) => QUERY ENDS
- pflock_meta()  (called during execution) => QUERY READS THE META TABLES

The below mechanism ensures that:
- when updating queries abort due to concurrency conflicts, they are restarted 
  and run in exclusive mode
- after an abort we run not only that query, but a convoy of PF_CONVOY next updating 
  queries in exclusive mode. This to gracefully degrade (avoid thrashing) in updating 
  loads that are highly conflictive.

Exclusive updating means that we need a counted-lock/exclusive-barrier (PF_UPDATE_LOCK,PF_UPDATE_BARRIER)
In normal mode, updating queries use the 'counting' path, that is, the first entering update
acquires the exclusive barrier, and the last leaving releases it. In exclusive mode, all updating 
queries take the exclusive barrier.

Similarly, there is a protection against seeing an inconsistent meta-database state.
Just before a query uses the documents()/collections() functions, all new incoming
document management queries (that shred and delete XML documents) are blocked,
and we must wait until all such active queries have finished.

This is again implemented using a counted-lock/exclusive-barrier (PF_META_LOCK,PF_META_BARRIER).
The first docmgmt query acquires the exclusive-barrier, and the last finishing releases it.
A query that calls pflock_meta() also must get this barrier, if it is the first to call it.
When those queries do pflock_end() the last must free the lock, and thus we must know which queries 
have taken it (this is also needed for not asking it twice). 

We store the information in the BAT 'ws_overlaps_ws' using [wsid,nil] BUNs.

A special case is when a document management query itself issues such a meta-information function.
Then we enter in exclusive single-docmgt query execution mode (SPECIAL CASE). Such a query is never
included in ws_overlaps_ws. Instead, a single variable 'pf_special' records the wsid.
@c
#define PF_CONVOY 5
BAT* ws_overlaps_ws = NULL;
MT_Lock pf_runtime_lock[6];
MT_Sema pf_runtime_sema[3];
int pf_nreaders, pf_convoy, pf_ndocmgt;
lng pf_writer, pf_special;

#define PF_SHORT_LOCK        pf_runtime_lock[0]
#define PF_WAL_LOCK          pf_runtime_lock[1]
#define PF_FREE_LOCK         pf_runtime_lock[2]
#define PF_EXTEND_LOCK       pf_runtime_lock[3]
#define PF_META_LOCK         pf_runtime_lock[4]
#define PF_UPDATE_LOCK       pf_runtime_lock[5]
#define PF_META_BARRIER      pf_runtime_sema[0]
#define PF_UPDATE_BARRIER    pf_runtime_sema[1]
#define PF_EXTEND_BARRIER    pf_runtime_sema[2]

#define PF_MODE_RDONLY 0 /* read-only query */
#define PF_MODE_DOCMGT 1 /* document management query */ 
#define PF_MODE_UPDATE 2 /* updating query */
#define PF_MODE_RETRY  3 /* updating query run for the second time (exclusive mode) */

/* get handle to the shared locks */
int CMDpflock_get(ptr *ret, int* nr) {
    *ret = (ptr) &pf_runtime_lock[(*nr)&3];
    if (*nr > 3) *ret = (ptr) &PF_EXTEND_BARRIER; /* hack: do not feel like creating a CMDsema_get */
    return GDK_SUCCEED;
}

/* query execution starts */
int CMDpflock_begin(lng *wsid) 
{
    int mode = ((*wsid) & (3LL << 30)) >> 30;
    if (mode == PF_MODE_RDONLY) 
        return GDK_SUCCEED;

    if (mode == PF_MODE_DOCMGT) {
        /* multiple docmgts may be active in parallel. the first takes the meta-barrier */
        int take_barrier;
        MT_set_lock(PF_META_LOCK, "pflock_begin: metaLock for mgmt");
        MT_set_lock(PF_SHORT_LOCK, "pflock_begin: shortLock for mgmt");
        take_barrier = (pf_ndocmgt++ == 0);
        MT_unset_lock(PF_SHORT_LOCK, "pflock_begin: shirtLock for mgmt");
        if (take_barrier) {
            MT_down_sema(PF_META_BARRIER, "pflock_begin: metaBarrier");
        }
        MT_unset_lock(PF_META_LOCK, "pflock_begin: metaLock for mgmt");
        return GDK_SUCCEED;
    }

    /* we only have update queries below this point; first check if we are in exclusive retry mode */
    MT_set_lock(PF_SHORT_LOCK, "pflock_begin: shortLock for convoy"); /* protects pf_convoy */
    if (mode == PF_MODE_RETRY) {
        pf_convoy += PF_CONVOY; /* re-try: schedule a convoy */
    } else if (pf_convoy > 0) {
        mode = PF_MODE_RETRY; pf_convoy--; /* convert to exclusive execution due to convoy */
    } 
    MT_unset_lock(PF_SHORT_LOCK, "pflock_begin: shortLock for convoy");

    if (mode == PF_MODE_UPDATE) {
	/* update in concurrent mode */
        MT_set_lock(PF_UPDATE_LOCK, "pflock_begin: updateLock for update");
        if (pf_nreaders++ == 0) 
            MT_down_sema(PF_UPDATE_BARRIER, "pflock_begin: updateBarrier for update");
        MT_unset_lock(PF_UPDATE_LOCK, "pflock_begin: updateLock for update");
    } else {
	/* update in exclusive mode */
        MT_down_sema(PF_UPDATE_BARRIER, "pflock_begin: updateBarrier for retry");
        MT_set_lock(PF_SHORT_LOCK, "pflock_begin: shortLock for writer");
        pf_writer = *wsid;
        MT_unset_lock(PF_SHORT_LOCK, "pflock_begin: shortLock for writer");
    }
    return GDK_SUCCEED;
}

/* query execution finishes; note that we have PF_SHORT_LOCK during this */
int CMDpflock_end(lng *wsid) 
{
    int mode = ((*wsid) & (3LL << 30)) >> 30;
    lng nil = lng_nil;

    if (mode == PF_MODE_DOCMGT) {
        /* last docmgt function yields the meta barrier 
         */
        if (*wsid == pf_special) {
            pf_special = -1;
        } else {
            pf_ndocmgt--;
        }
        if (pf_ndocmgt == 0) {
            MT_up_sema(PF_META_BARRIER, "pflock_end: metaBarrier for docmgt");
        }
    } else {
        /* last active meta-information query releases meta-barrier 
         */
        BUN self_passed; /* this query uses meta-information functions? */
        BUN others_passed; /* there are multiple such queries active? */

        self_passed = BUNlocate(ws_overlaps_ws, wsid, &nil);
        BUNdelHead(ws_overlaps_ws, wsid, FALSE); /* release all active info on this query */
        others_passed = BUNfnd(BATmirror(ws_overlaps_ws), &nil);
        if (self_passed != BUN_NONE && others_passed == BUN_NONE) {
            MT_up_sema(PF_META_BARRIER, "pflock_end: metaBarrier");
        }

        /* last active update query releases update-barrier 
         */
        if (pf_writer == *wsid) {
            /* finish in exclusive mode */
            pf_writer = -1;
            MT_up_sema(PF_UPDATE_BARRIER, "pflock_end: exclusive updateBarrier");
        } else if (mode != PF_MODE_RDONLY) { 
            /* finish in concurrent mode */
            if (--pf_nreaders == 0)
                MT_up_sema(PF_UPDATE_BARRIER, "pflock_end: updateBarrier");
        }
    }
    return GDK_SUCCEED;
}

/* we are going to use a meta-information function that returns all collections/documents
 * thus we must block all new docmgt queries and wait for all active ones to finish 
 */
int CMDpflock_meta(lng *wsid) 
{
    int mode = ((*wsid) & (3LL << 30)) >> 30;
    lng nil = lng_nil;
    BUN others_passed; /* there are multiple such queries active? */

    MT_set_lock(PF_SHORT_LOCK, "pflock_meta: shortLock precheck");
    if (*wsid == pf_special || BUNlocate(ws_overlaps_ws, wsid, &nil) != BUN_NONE) {
        MT_unset_lock(PF_SHORT_LOCK, "pflock_meta: shortLock precheck");
        return GDK_SUCCEED; /* already got permission to proceed */
    }
    if (mode == PF_MODE_DOCMGT) {
        /* SPECIAL CASE: a meta function inside a docmgt function. 
         * now we must wait for potential other docmgts to finish first 
         */
        int reapply_barrier = (pf_ndocmgt > 1);
        if (reapply_barrier) pf_ndocmgt--; 
        MT_unset_lock(PF_SHORT_LOCK, "pflock_meta: metaLock precheck");
        if (reapply_barrier) {
            MT_down_sema(PF_META_BARRIER, "pflock_meta: metaBarrier for convert");
            MT_set_lock(PF_SHORT_LOCK, "pflock_meta: shortLock special");
            pf_special = *wsid;
            MT_unset_lock(PF_SHORT_LOCK, "pflock_meta: shortLock special");
        }
        return GDK_SUCCEED;
    }
    MT_unset_lock(PF_SHORT_LOCK, "pflock_meta: shortLock precheck");

    MT_set_lock(PF_META_LOCK, "pflock_meta: metaLock");
    MT_set_lock(PF_SHORT_LOCK, "pflock_meta: shortLock insert");
    others_passed = BUNfnd(BATmirror(ws_overlaps_ws), &nil);
    BUNins(ws_overlaps_ws, wsid, &nil, FALSE);
    MT_unset_lock(PF_SHORT_LOCK, "pflock_meta: shortLock insert");
    if (others_passed == BUN_NONE) {
        /* first query with meta function: get barrier */
        MT_down_sema(PF_META_BARRIER, "pflock_meta: metaBarrier");
    }
    MT_unset_lock(PF_META_LOCK, "pflock_meta: metaLock");
    return GDK_SUCCEED;
}

int CMDpflock_free(bit *res, bit* doCommit) {
    /* assumption: short lock is taken!! */
    *res = FALSE;
    if (*doCommit == TRUE) {
        /* only OK if no active query called a meta-information function */
        lng nil = lng_nil;
        *res = (BUNfnd(BATmirror(ws_overlaps_ws), &nil) == BUN_NONE);
    }
    return GDK_SUCCEED;
}

/* retrieve BAT ws that belongs to wsid; note that you must be sure that it is still alive */
int CMDws_bat(BAT **ret, lng* wsid)
{
    bat bid = *wsid & 1073741823LL;
    *ret = 0;
    if (BBPfix(bid)) {
        BAT *v = 0, *b = BBPdescriptor(bid);
        if (b) *ret = v = VIEWcreate(b,b);
        BBPunfix(bid);
        if (v) {
            /* give the view a name such that lng(bbname(v)) = lng(bbpname(b)) */
            long_str buf;
            snprintf(buf, sizeof(buf), "%s-%d", BBP_logical(bid), ABS(b->batCacheid));
            buf[sizeof(buf) - 1] = 0;
            BBPrename(v->batCacheid, buf);
    	    return GDK_SUCCEED;
        } else
        if (b) {
            GDKerror("CMDws_bat: VIEWcreate(%s(%d)) failed.\n", BATgetId(b), b->batCacheid);
        } else {
            GDKerror("CMDws_bat: BBPdescriptor(%d) failed.\n", (int)bid);
        }
    } else {
        GDKerror("CMDws_bat: BBPfix(%d) failed.\n", (int)bid);
    }
    return GDK_FAIL;
}

#define SWIZZLE(idx) ((map[idx >> REMAP_PAGE_BITS] << REMAP_PAGE_BITS) | (idx & REMAP_PAGE_MASK))

int
CMDdocsused(BAT** res, BAT* input, BAT* frag_root, BAT* nid_rid, BAT* map_pid, BAT* pre_size) {
    oid* docid = (oid*) Hloc(frag_root, BUNfirst(frag_root));
    oid* root = (oid*) Tloc(frag_root, BUNfirst(frag_root));
    int* size = (int*) Tloc(pre_size, BUNfirst(pre_size));
    BATiter inputi = bat_iterator(input);
    oid* map = (oid*) Tloc(map_pid, BUNfirst(map_pid));
    oid* rid = (oid*) Tloc(nid_rid, BUNfirst(nid_rid));
    size_t i=0,j=0, n=BATcount(input), m=BATcount(frag_root);

    *res = BATnew(TYPE_oid, TYPE_oid, 100);
    if (*res == NULL)
        return GDK_FAIL;
    BATseqbase(*res, 0);

    while (i<n) {
        oid delta, docpre, cur = * (oid *) BUNtail(inputi, i);

        /* skip all documents before cur */
        if (map_pid->ttype == TYPE_void) {
            for (delta = (m-j) >> 4; delta > 40; delta >>= 4)
                while (j+delta < m && cur > (docpre=root[j+delta]) + size[docpre])
                    j += delta;
            while (j < m && cur > (docpre=root[j]) + size[docpre])
                j++;
        } else {
            /* we need swizzling */
            for (delta = (m-j) >> 4; delta > 40; delta >>= 4)
                while (j+delta < m && cur > (docpre=SWIZZLE(rid[root[j+delta]])) + size[docpre])
                    j += delta;
            while (j < m && cur > (docpre=SWIZZLE(rid[root[j]])) + size[docpre])
                j++;
        }
        assert(j < m);

        /* insert docid */
        BUNins(*res, &docpre, &docid[j], FALSE);

        /* skip all nodes of this document */
        docpre = docpre + size[docpre];
        for (delta = (n-i) >> 4; delta > 40; delta >>= 4)
            while (i+delta < n && * (oid *) BUNtail(inputi, i+delta) <= docpre)
                i += delta;
        while (i < n && * (oid *) BUNtail(inputi, i) <= docpre)
            i++;
    }
    return GDK_SUCCEED;
}

#include "serialize.h"

int
xquery_print_result_loop (
    str  mode,
    str  moduleNS,
    str  method,
    BAT* ws,
    BAT* loop, 
    BAT* iter, 
    BAT* item, 
    BAT* kind,
    BAT* intVAL,
    BAT* dblVAL,
    BAT* strVAL)
{
    size_t niters = BATcount (loop);
    return xquery_print_result_DRIVER (
               mode,
               moduleNS,
               method,
               NULL, /* set of printing callback function */
               NULL, /* optional arguments for the callback functions */
               ws,
               niters?niters:1, /* number of iterations */
               loop, /* loop relation */
               iter, /* iter relation */
               item, /* item relation */
               kind, /* kind relation */
               intVAL,
               dblVAL,
               strVAL);
}

int
xquery_print_result_main (
    str  mode,
    BAT* ws,
    BAT* item, 
    BAT* kind,
    BAT* intVAL,
    BAT* dblVAL,
    BAT* decVAL,
    BAT* strVAL)
{
    (void) decVAL;
    
    return xquery_print_result_DRIVER (
               mode,
               NULL, /* module */
               NULL, /* method */
               NULL, /* set of printing callback function */
               NULL, /* optional arguments for the callback functions */
               ws,
               1, /* number of iterations */
               item, /* loop relation */
               item, /* iter relation */
               item, /* item relation */
               kind, /* kind relation */
               intVAL,
               dblVAL,
               strVAL);
}

int
xquery_print_result_file (
    str  file,
    str  mode,
    BAT* ws,
    oid* item, 
    int* kind,
    BAT* intVAL,
    BAT* dblVAL,
    BAT* decVAL,
    BAT* strVAL)
{
    int len = strlen(file);
    int ret = GDK_FAIL;
    (void) decVAL;
    if (DIR_SEP != '/') {
        char *s = file; /* normalize path on windows to windows DIR_SEP */
        do { if (*s == '/') *s = DIR_SEP; } while(*(++s));
    }
    if (*file == DIR_SEP) {
        GDKerror("xquery_print_result_file: %s cannot have an absolute name.\n", file);
    } else if (len < 4 || (strcmp(file+len-4, ".xml") && strcmp(file+len-4, ".XML"))) {
        GDKerror("xquery_print_result_file: %s name does not end in .xml\n", file);
    } else if (GDKcreatedir(file)) {
        FILE *fp = fopen(file, "wb");
        if (fp) {
            stream *s = file_wastream(fp, file);
            if (s) {
                BAT* i = BATnew(TYPE_void,TYPE_oid,1);
                if (i) {
                    BAT* k = BATnew(TYPE_void,TYPE_int,1);
                    if (k) {
                        BATseqbase(k, 0);
                        BATseqbase(i, 0);
                        BUNappend(k, kind, FALSE);
                        BUNappend(i, item, FALSE);
                        ret = xquery_print_result_driver (
                             s,
                             mode,
                             NULL, /* module */
                             NULL, /* method */
                             NULL, /* set of printing callback function */
                             NULL, /* optional arguments for the callback functions */
                             ws,
                             1, /* number of iterations */
                             i, /* loop relation */
                             i, /* iter relation */
                             i, /* item relation */
                             k, /* kind relation */
                             intVAL,
                             dblVAL,
                             strVAL);
                         stream_destroy(s);
                    }
                    BBPreclaim(k);
                }
                BBPreclaim(i);
            } 
            fclose(fp);
        } else {
            GDKsyserror("xquery_print_result_file: %s", file);
        }
    }
    return ret;
}

int
xquery_print_doc_main (str mode, BAT* ws, str docName)
{
    return xquery_print_doc_DRIVER(mode,NULL,NULL,ws,docName);
}

bat* pf_support_prelude() {
    MT_init_lock(pf_runtime_lock[0], "PF_SHORT_LOCK");
    MT_init_lock(pf_runtime_lock[1], "PF_WAL_LOCK");
    MT_init_lock(pf_runtime_lock[2], "PF_FREE_LOCK");
    MT_init_lock(pf_runtime_lock[3], "PF_EXTEND_LOCK");
    MT_init_lock(pf_runtime_lock[5], "PF_META_LOCK");
    MT_init_lock(pf_runtime_lock[4], "PF_UPDATE_LOCK");
    MT_init_sema(pf_runtime_sema[0],1, "PF_META_BARRIER");
    MT_init_sema(pf_runtime_sema[1],1, "PF_UPDATE_BARRIER");
    MT_init_sema(pf_runtime_sema[2],1, "PF_EXTEND_BARRIER");
    ws_overlaps_ws = BATnew(TYPE_lng, TYPE_lng, 1024);
    BBPrename(ws_overlaps_ws->batCacheid, "ws_overlaps_ws");
    (void) BATprepareHash(ws_overlaps_ws);
    (void) BATprepareHash(BATmirror(ws_overlaps_ws));
    BATset(ws_overlaps_ws, BOUND2BTRUE);
    pf_nreaders = pf_convoy = pf_ndocmgt = 0;
    pf_special = pf_writer = -1;
    return NULL;
}

void pf_support_epilogue() {
    MT_destroy_lock(pf_runtime_lock[0]);
    MT_destroy_lock(pf_runtime_lock[1]);
    MT_destroy_lock(pf_runtime_lock[2]);
    MT_destroy_lock(pf_runtime_lock[3]);
    MT_destroy_lock(pf_runtime_lock[4]);
    MT_destroy_lock(pf_runtime_lock[5]);
    MT_destroy_sema(pf_runtime_sema[0]);
    MT_destroy_sema(pf_runtime_sema[1]);
    MT_destroy_sema(pf_runtime_sema[2]);
}

@- PROCs required by the algebraic translation ###
@mil
PROC doc_tbl (BAT[void, BAT] ws, BAT[void, str] item) : BAT[void,BAT]
{
    # load all requested documents into the working set
    var r := ws_opendoc(ws, item);

    # pick the according cont value for each document requested
    var ret_cont := r.tmark(0@0);

    # pick the according root-pre value for each document requested
    var ret_item := r.hmark(0@0);

    # return result as a BAT of BATs
    return new (void, BAT).append(ws)
                          .append(ret_item)
                          .append(ret_cont)
                          .seqbase(0@0);
}

ADDHELP("doc_tbl", "teubner", "Aug 2005",
                "PARAMETERS:\n\
ws    current working set; will be modified\n\
item  list of documents to add to the working set\n\
DESCRIPTION:\n\
Implementation of the algebra operator `doc_tbl' that\n\
loads persistent documents into the working set.\n\
Input is a list of document names. Output is a BAT of\n\
BATs with the components\n\
(a) the modified working set,\n\
(b) an `item' column with the pre values of the\n\
document roots, and\n\
(c) a `cont' column that encodes document\n\
container (within the working set) according to our\n\
working set representation.",
                "pf_support");

# primitive for supporting highly specific XQuery functionality
PROC merge_adjacent_text_nodes (BAT[void,oid] iter,
                                BAT[void,oid] pre,
                                BAT[void,oid] pcont,
                                BAT[void,BAT] ws) : BAT[void,BAT]
{
    # nothing to do if our item sequences are all of length 1
    if (iter.count() = iter.tunique().count())
        return new (void, BAT).append (ws)
                              .append (pre)
                              .append (pcont)
                              .seqbase (0@0);

    var map := pre.ord_uselect(oid_nil,oid_nil).hmark(0@0);
    iter := map.leftfetchjoin(iter);
    pre := map.leftfetchjoin(pre);
    pcont := map.leftfetchjoin(pcont);

    var kind := mposjoin (pre, pcont, ws.fetch(PRE_KIND));
    var text := kind.[=](TEXT);
    var text_sel := text.uselect(true).hmark(0@0);
    var text_pre := text_sel.leftfetchjoin(pre).tmark(0@0);
    var text_cont := text_sel.leftfetchjoin(pcont).tmark(0@0);

    var text_prop := mposjoin (mposjoin (text_pre, text_cont, ws.fetch(PRE_PROP)),
                               mposjoin (text_pre, text_cont, ws.fetch(PRE_CONT)),
                               ws.fetch(PROP_TEXT));
    text_pre := nil;
    text_cont := nil;

    var pre_prop := pre.mirror()
                       .outerjoin(text_prop.reverse()
                                           .leftfetchjoin(text_sel)
                                           .reverse());
    var pre_enum := [oid](text);
    var res_size := (iter.tunique().count() 
                  + text.count() + 1)
                  - text_sel.count();

    var res_strs := combine_text_string (iter.chk_order(),
                                         pre_enum,
                                         pre_prop,
                                         res_size);
    iter := nil;
    pre_enum := nil;
    pre_prop := nil;
    res_size := nil;
    var res_texts := text_constr (res_strs.tmark(0@0), ws);
    ws := res_texts.fetch(0);
    var textnodes := res_texts.fetch(1);
    res_texts := nil;
    
    text_pre := pre.mirror()
                   .outerjoin(res_strs.mark(0@0)
                                      .leftfetchjoin(textnodes));
    text_cont := pre.mirror()
                    .outerjoin(res_strs.project(WS));
    res_strs := nil;
    textnodes := nil;
    pre := map.reverse()
              .leftfetchjoin([ifthenelse](text,text_pre,pre));
    pcont := map.reverse()
                .leftfetchjoin([ifthenelse](text,text_cont,pcont));

    # return result as a BAT of BATs
    return new (void, BAT).append (ws)
                          .append (pre)
                          .append (pcont)
                          .seqbase (0@0);
}
ADDHELP("merge_adjacent_text_nodes", "tsheyar", "Oct 2005",
        "PARAMETERS:\n\
BAT[void,oid] iter : iter references\n\
BAT[void,oid] pre  : pre values\n\
BAT[void,oid] cont : container id which the pre values refer to\n\
BAT[void,BAT] ws   : working set that stores the qname\n\
DESCRIPTION:\n\
merge_adjacent_text_nodes takes an iter|pre|cont schema and\n\
combines within each iteration all adjacent text nodes.\n\
(Note that the order is given by the input order.)\n\
New textnodes are added into the working set ws and the result is\n\
a bat of bats (ws, modified_pres, modified_conts). The heads of the\n\
pre and cont are aligned to the input relations.",
        "pf_support");
                                
# add_qnames changes the working set as side effect
# without a 'prefix:uri:local' index this could be quite expensive
#
# it basically does:
# [ifthenelse]([isnil](local),local.project(nil),[add_qname](prefix,uri,local,local.project(ws)));
PROC add_qnames (BAT[void,str] prefix,
                 BAT[void,str] uri,
                 BAT[void,str] local,
                 BAT[void,BAT] ws) : BAT[void,oid]
{
    var pref_uri_loc := prefix.[+](NS_ACCEL_SEP).[+](uri).[+](NS_ACCEL_SEP).[+](local); # [void,str]
    pref_uri_loc := [isnil](local).ord_uselect(false).mirror().leftfetchjoin(pref_uri_loc); # [oid,str] (sorted subset)
    return mirror(local).outerjoin(find_qn_bulk(ws, WS, pref_uri_loc, false)).tmark(seqbase(local)); # [void,oid]
}
ADDHELP("add_qnames", "tsheyar", "Oct 2005",
        "PARAMETERS:\n\
BAT[void,str] prefix : prefix of qname\n\
BAT[void,str] uri    : URI of the qname\n\
BAT[void,str] local  : local part of the qname\n\
BAT[void,BAT] ws     : working set that stores the qname\n\
DESCRIPTION:\n\
add_qnames adds qnames consisting of the three strings prefix,\n\
uri, and local to the working set (ws). The return value is the\n\
identifier that corresponds to the qname in the qname container\n\
of the transient nodes.\n\
NOTE: the working set 'ws' is changed as side effect!",
        "pf_support");

# add_qname changes the working set as side effect
PROC add_qname (str prefix, str uri, str local, BAT[void,BAT] ws) : oid
{
    var props := ws.fetch(QN_PREFIX_URI_LOC).fetch(WS);
    var key := prefix + NS_ACCEL_SEP + uri + NS_ACCEL_SEP + local;
    var itemID := oid(count(props));
    if (not(isnil(CATCH(itemID := reverse(props).find(key))))) {
        props.insert(itemID, key);
        ws.fetch(QN_HISTOGRAM).fetch(WS).insert(itemID, 1LL);
        ws.fetch(QN_URI_LOC).fetch(WS).insert(itemID, uri + NS_ACCEL_SEP + local);
        ws.fetch(QN_URI).fetch(WS).insert(itemID, uri);
        ws.fetch(QN_PREFIX).fetch(WS).insert(itemID, prefix);
        ws.fetch(QN_LOC).fetch(WS).insert(itemID, local);
    }
    return itemID;
}
ADDHELP("add_qname", "tsheyar", "Oct 2005",
        "PARAMETERS:\n\
str           prefix : prefix of qname\n\
str           uri    : URI of the qname\n\
str           local  : local part of the qname\n\
BAT[void,BAT] ws     : working set that stores the qname\n\
DESCRIPTION:\n\
add_qname adds a qname consisting of the three strings prefix,\n\
uri, and local to the working set ws. The return value is the\n\
identifier that corresponds to the qname in the qname container\n\
of the transient nodes.\n\
NOTE: the working set 'ws' is changed as side effect!",
        "pf_support");

PROC add_content (BAT[void, str] item, BAT[void, BAT] ws, int slot) : BAT[void,oid]
{
    # find all strings that are not already in the working set ...
    var ws_prop_string := ws.fetch(slot).fetch(WS);
    var unq_str := item.tunique().hmark(0@0);
    var str_unq := unq_str.reverse().kdiff(ws_prop_string.reverse());
    unq_str := nil;
    # ... and add them to the 'slot' container
    var seqb := oid(int(ws_prop_string.seqbase()) + ws_prop_string.count());
    unq_str := str_unq.hmark(seqb);
    str_unq := nil;
    ws_prop_string := ws_prop_string.insert(unq_str);
    unq_str := nil;

    # get the property values of the strings;
    # we invest in sorting ws_prop_string &  X_strings/item on the TYPE_str 
    # join columns as the mergejoin proved to be faster and more robust with
    # large BATs than a hashjoin
    var ws_string_prop := ws_prop_string.reverse().sort();
    var X_item := item.mark(0@0);
    var X_strings := item.tmark(0@0).tsort();
    var X_prop := X_strings.leftjoin(ws_string_prop);
    X_strings := nil;
    ws_string_prop := nil;
    var newProp := X_item.leftjoin(X_prop);
    X_item := nil;
    X_prop := nil;

    return newProp;
}


@- !!! THE FOLLOWING PROCs (*_constr) ARE DEPRECATED !!!
   They are however not removed yet to support comparison
   with older MIL scripts (that rely on the PROCs.
@mil

PROC text_constr (BAT[void, str] item, BAT[void, BAT] ws) : BAT[void,BAT]
{
    # find all strings that are not already in the working set ...
    var ws_prop_text := ws.fetch(PROP_TEXT).fetch(WS);
    var unq_str := item.tunique().hmark(0@0);
    var str_unq := unq_str.reverse().kdiff(ws_prop_text.reverse());
    unq_str := nil;
    # ... and add them to the PROP_TEXT container
    var seqb := oid(int(ws_prop_text.seqbase()) + ws_prop_text.count());
    unq_str := str_unq.hmark(seqb);
    str_unq := nil;
    ws_prop_text := ws_prop_text.insert(unq_str);
    unq_str := nil;

    # get the property values of the strings;
    # we invest in sorting ws_prop_text &  X_strings/item on the TYPE_str 
    # join columns as the mergejoin proved to be faster and more robust with
    # large BATs than a hashjoin
    var ws_text_prop := ws_prop_text.reverse().sort();
    var X_item := item.mark(0@0);
    var X_strings := item.tmark(0@0).tsort();
    var X_prop := X_strings.leftjoin(ws_text_prop);
    X_strings := nil;
    ws_text_prop := nil;
    var newProp := X_item.leftjoin(X_prop);
    X_item := nil;
    X_prop := nil;

    # add new text nodes to the working set
    var seqb := oid(count(ws.fetch(PRE_KIND).fetch(WS)) +
                    int(ws.fetch(PRE_KIND).fetch(WS).seqbase()));
    var newPre_prop := newProp.tmark(seqb).chk_order();
    newProp := nil;
    ws.fetch(PRE_PROP).fetch(WS).insert(newPre_prop);
    ws.fetch(PRE_SIZE).fetch(WS).insert(newPre_prop.project(0));
    ws.fetch(PRE_LEVEL).fetch(WS).insert(newPre_prop.project(chr(0)));
    ws.fetch(PRE_KIND).fetch(WS).insert(newPre_prop.project(TEXT));
    ws.fetch(PRE_CONT).fetch(WS).insert(newPre_prop.project(WS));
    ws.fetch(PRE_NID).fetch(WS).append(newPre_prop.mirror());
    ws.fetch(NID_RID).fetch(WS).append(newPre_prop.mirror());

    newPre_prop := nil;
    item := item.mark(seqb);
    seqb := nil;

    # return result as a BAT of BATs
    var res := new (void, BAT).append (ws)
                              .append (item)
                              .append (item.project(WS))
                              .seqbase (0@0);

    { # adding new fragments to the FRAG_ROOT bat
        ws.fetch(FRAG_ROOT).fetch(WS).insert(reverse(reverse(item).project(oid_nil)));
    }

    return res;
}
ADDHELP("text_constr", "tsheyar", "Oct 2005",
        "PARAMETERS:\n\
BAT[void,str] item : content of the text nodes\n\
BAT[void,BAT] ws   : working set that stores the document representations\n\
DESCRIPTION:\n\
text_constr is a generic text constructor that creates for each\n\
item a new textnode. These textnodes are added to the working\n\
set ws. Output is a BAT of BATs with the following components:\n\
(a) the modified working set,\n\
(b) the pre values of the textnodes, and\n\
(c) the cont values of the textnodes",
        "pf_support");

PROC attr_constr (BAT[void, oid] qn, BAT[void, str] item, BAT[void, BAT] ws) : BAT[void,BAT]
{
    # find all strings that are not already in the working set ...
    var ws_prop_val := ws.fetch(PROP_VAL).fetch(WS);
    var unq_str := item.tunique().hmark(0@0);
    var str_unq := unq_str.reverse().kdiff(ws_prop_val.reverse());
    unq_str := nil;
    # ... and add them to the PROP_VAL container
    var seqb := oid(int(ws_prop_val.seqbase()) + ws_prop_val.count());
    unq_str := str_unq.hmark(seqb);
    str_unq := nil;
    ws_prop_val := ws_prop_val.insert(unq_str);
    unq_str := nil;

    # get the property values of the strings;
    # we invest in sorting ws_prop_val &  X_strings/item on the TYPE_str 
    # join columns as the mergejoin proved to be faster and more robust with
    # large BATs than a hashjoin
    var ws_val_prop := ws_prop_val.reverse().sort();
    var X_item := item.mark(0@0);
    var X_strings := item.tmark(0@0).tsort();
    var X_prop := X_strings.leftjoin(ws_val_prop);
    X_strings := nil;
    ws_val_prop := nil;
    var newProp := X_item.leftjoin(X_prop);
    X_item := nil;
    X_prop := nil;

    # add new text nodes to the working set
    var seqb := oid(count(ws.fetch(ATTR_OWN).fetch(WS)) +
                    int(ws.fetch(ATTR_OWN).fetch(WS).seqbase()));
    var newAttr_prop := newProp.tmark(seqb);
    newProp := nil;
    ws.fetch(ATTR_PROP).fetch(WS).insert(newAttr_prop);
    ws.fetch(ATTR_OWN).fetch(WS).insert(newAttr_prop.project(oid_nil));
    ws.fetch(ATTR_QN).fetch(WS).insert(qn.tmark(seqb));
    ws.fetch(ATTR_CONT).fetch(WS).insert(newAttr_prop.project(WS));

    newAttr_prop := nil;
    item := item.mark(seqb);
    seqb := nil;

    # return result as a BAT of BATs
    var res := new (void, BAT).append (ws)
                              .append (item)
                              .append (item.project(WS))
                              .seqbase (0@0);

    return res;
}
ADDHELP("attr_constr", "tsheyar", "Oct 2005",
        "PARAMETERS:\n\
BAT[void,oid] qn   : names of the attributes\n\
BAT[void,str] item : values of the attributes\n\
BAT[void,BAT] ws   : working set that stores the document representations\n\
DESCRIPTION:\n\
attr_constr is a generic attribute constructor that creates for\n\
each aligned qn|item pair a new attribute. These attributes are\n\
added to the working set ws. Output is a BAT of BATs with the\n\
following components:\n\
(a) the modified working set,\n\
(b) the attribute ids, and\n\
(c) the cont values of the attributes",
        "pf_support");

PROC elem_constr_empty (BAT[void, oid] qn, BAT[void, BAT] ws) : BAT[void,BAT]
{
    # add new element nodes to the working set
    var seqb := oid(count(ws.fetch(PRE_KIND).fetch(WS)) +
                    int(ws.fetch(PRE_KIND).fetch(WS).seqbase()));
    var newPre_prop := qn.tmark(seqb).chk_order();
    ws.fetch(PRE_PROP).fetch(WS).insert(newPre_prop);
    ws.fetch(PRE_SIZE).fetch(WS).insert(newPre_prop.project(0));
    ws.fetch(PRE_LEVEL).fetch(WS).insert(newPre_prop.project(chr(0)));
    ws.fetch(PRE_KIND).fetch(WS).insert(newPre_prop.project(ELEMENT));
    ws.fetch(PRE_CONT).fetch(WS).insert(newPre_prop.project(WS));
    ws.fetch(PRE_NID).fetch(WS).append(newPre_prop.mirror());
    ws.fetch(NID_RID).fetch(WS).append(newPre_prop.mirror());

    newPre_prop := nil;
    qn := qn.mark(seqb);
    seqb := nil;

    # return result as a BAT of BATs
    var res := new (void, BAT).append (ws)
                              .append (qn)
                              .append (qn.project(WS))
                              .seqbase (0@0);

    { # adding new fragments to the WS_FRAG bat
        ws.fetch(FRAG_ROOT).fetch(WS).insert(reverse(reverse(qn).project(oid_nil)));
    }

    return res;
}
ADDHELP("elem_constr_empty", "tsheyar", "Oct 2005",
        "PARAMETERS:\n\
BAT[void,oid] qn   : names of the elements\n\
BAT[void,BAT] ws   : working set that stores the document representations\n\
DESCRIPTION:\n\
elem_constr_empty is a generic element constructor for empty\n\
elements that creates for each qname qn a new element. These\n\
elements are added to the working set ws. Output is a BAT of\n\
BATs with the following components:\n\
(a) the modified working set,\n\
(b) the pre values, and\n\
(c) the cont values of the elements",
        "pf_support");

PROC elem_constr (BAT[void, oid] qn_iter,
                  BAT[void, oid] qn_item,
                  BAT[void, oid] iter,
                  BAT[oid, oid] pre,
                  BAT[oid, oid] pcont,
                  BAT[void, oid] attr,
                  BAT[void, oid] acont,
                  BAT[void, BAT] ws) : BAT[void,BAT]
{
    var root_iter;
    var root_size;
    var root_prop;
    var root_kind;
    var root_cont;
    var root_level;
    # attr
        var root_pre;
        var root_pre_cont;
        
    # throw out nil values and generate iter|item|cont representation
    # for attributes
    var selected := pre.select(oid_nil,oid_nil);
    var piter := selected.hmark(0@0).leftfetchjoin(iter).tmark(0@0); # make it void
    pre := selected.tmark(0@0);
    pcont := pcont.select(oid_nil,oid_nil).tmark(0@0);
    selected := nil;

    # throw out nil values and generate iter|item|cont representation
    # for attributes
    selected := attr.select(oid_nil,oid_nil);
    var aiter := selected.hmark(0@0).leftfetchjoin(iter).tmark(0@0); # make it void
    attr := selected.tmark(0@0);
    acont := acont.select(oid_nil,oid_nil).tmark(0@0);
    selected := nil;

    if (pre.count() != 0) {

        # use head to avoid elimination of duplicates
        # (this is additionally used in the content level determination
        var iter_unq := piter.mirror();
        # get all subtree copies
        var res_scj := loop_lifted_descendant_or_self_step 
                           (iter_unq, pre, pcont, ws, 0);
        iter_unq := nil;

        # variables for the result of the scj 
        var res_iter := res_scj.fetch(0);
        var res_item := res_scj.fetch(1);
        # !be aware that res_cont is only a fake_project!
        var res_cont := res_scj.fetch(2);
        # !avoid being res_iter a fake_project!
        res_iter := materialize (res_iter, res_item);
        res_scj := nil;
            
        # create content_iter as sorting argument for the merged union
        var content_iter := res_iter.leftfetchjoin(piter).chk_order();
        # create subtree copies for all bats
        var content_size := mposjoin(res_item, res_cont, ws.fetch(PRE_SIZE));
        var content_prop := mposjoin(res_item, res_cont, ws.fetch(PRE_PROP));
        var content_kind := mposjoin(res_item, res_cont, ws.fetch(PRE_KIND));
        var content_cont := mposjoin(res_item, res_cont, ws.fetch(PRE_CONT));
        var content_level := mposjoin(res_item, res_cont, ws.fetch(PRE_LEVEL));
        # change the level of the subtree copies
        content_level := content_level.[+](chr(1));
        var contentRoot_level := mposjoin(pre, pcont, ws.fetch(PRE_LEVEL));
        # map Root_level to the result of the scj 
        #using the faked iteration values
        contentRoot_level := res_iter.leftfetchjoin(contentRoot_level);
        content_level := content_level.[-](contentRoot_level);
        content_level := content_level.tmark(0@0);
        contentRoot_level := nil;
            
        # attr
        # content_pre is needed for attribute subtree copies
        var content_pre := res_item;

        # as well as content_pre_cont
        var content_pre_cont := res_cont;
            
        root_iter := qn_iter.chk_order();
        # calculate the sizes for the root nodes
        root_size := {count}(content_iter.reverse(), qn_iter.reverse(), FALSE).tmark(seqbase(qn_iter));
        root_prop := qn_item;
        root_kind := constant2bat(ELEMENT);
        root_cont := constant2bat(WS);
        root_level := constant2bat(chr(0));

        # attr
            # root_pre is a dummy needed for merge union with content_pre 
            root_pre := constant2bat(oid_nil);
            # as well as root_cont_pre
            root_pre_cont := constant2bat(oid_nil);

        # merge union root and nodes
        {
        var merged_result := merged_union (
                                 root_iter, content_iter,
                                 root_size, content_size,
                                 root_level, content_level,
                                 root_kind, content_kind,
                                 root_prop, content_prop,
                                 root_cont, content_cont,
        # attr
                                 root_pre, content_pre,
                                 root_pre_cont, content_pre_cont);
        root_iter := nil;
        content_iter := nil;
        content_size := nil;
        content_level := nil;
        content_kind := nil;
        content_prop := nil;
        content_cont := nil;
        # attr
            content_pre := nil;
            content_pre_cont := nil;
        root_size := merged_result.fetch(1);
        root_level := merged_result.fetch(2);
        root_kind := merged_result.fetch(3);
        root_prop := merged_result.fetch(4);
        root_cont := merged_result.fetch(5);
        # attr
            root_pre := merged_result.fetch(6);
            root_pre_cont := merged_result.fetch(7);

        merged_result := nil;

        # printing output for debugging purposes
            # print("merged (root & content)");
            # print(root_size, [int](root_level), [int](root_kind), root_prop);
        }

    } else { # end of ``if (pre.count() != 0)''

        root_size := qn_iter.project(0);
        root_prop := qn_item.copy(); # !the seqbase of qn_item is later modified
        root_kind := qn_iter.project(ELEMENT);
        root_cont := qn_iter.project(WS);
        root_level := qn_iter.project(chr(0));
        # attr
            root_pre := qn_iter.project(oid_nil);
            root_pre_cont := qn_iter.project(oid_nil);

    }  # end of else in ``if (pre.count() != 0)''
        
    # set the offset for the new created trees
    {
        var seqb := oid(count(ws.fetch(PRE_SIZE).fetch(WS))
                        + int(ws.fetch(PRE_SIZE).fetch(WS).seqbase()));
        root_size := root_size.seqbase(seqb);
        root_prop := root_prop.reverse().mark(seqb).reverse();
        root_kind := root_kind.seqbase(seqb);
        root_cont := root_cont.seqbase(seqb);
        root_level := root_level.seqbase(seqb);
        # attr
            # get the new pre values
            root_pre := root_pre.seqbase(seqb);
            root_pre_cont := root_pre_cont.seqbase(seqb);
        seqb := nil;
    }

    # insert the new trees into the working set
    ws.fetch(PRE_SIZE).fetch(WS).insert(root_size);
    ws.fetch(PRE_KIND).fetch(WS).insert(root_kind);
    ws.fetch(PRE_PROP).fetch(WS).insert(root_prop);
    ws.fetch(PRE_CONT).fetch(WS).insert(root_cont);
    ws.fetch(PRE_LEVEL).fetch(WS).insert(root_level);
    ws.fetch(PRE_NID).fetch(WS).append(root_kind.mirror());
    ws.fetch(NID_RID).fetch(WS).append(root_kind.mirror());

    # save the new roots for creation of the intermediate result
    var roots := root_level.ord_uselect(chr(0));
    # (note that all operations are order preserving and ``mark''
    # aligns the key with the qn_iter input 
    roots := roots.hmark(0@0);

    # resetting the temporary variables
    root_size := nil;
    root_prop := nil;
    root_kind := nil;
    root_cont := nil;
    root_level := nil;
        
    # adding the new constructed roots to the WS_FRAG bat of the
    # working set, that a following (preceding) step can check
    # the fragment boundaries
    {
        ws.fetch(FRAG_ROOT).fetch(WS).insert(reverse(reverse(roots).project(oid_nil)));
    }

    # ----------------------------------
    # ----- ATTRIBUTE TRANSLATION ------
    # ----------------------------------
    # 1. step: add subtree copies of attributes
    if (pre.count() != 0) { # but only if there are any subtree nodes
        # lookup the affected attributes using the old pre values
        var preNew_attr := mvaljoin(root_pre, 
                                    root_pre_cont,
                                    ws.fetch(ATTR_OWN));
        # lookup the first free attr value
        var seqb := oid(ws.fetch(ATTR_QN).fetch(WS).count());
        # split up result of mvaljoin and mark them with the correct seqbase
        var attrNew_preNew := preNew_attr.mark(seqb).reverse();
        var attrNew_attrOld := preNew_attr.reverse().mark(seqb).reverse();
        preNew_attr := nil;
        var attrNew_pre_cont := attrNew_preNew.leftfetchjoin(root_pre_cont);
        # help MIL to keep head void
        attrNew_pre_cont := attrNew_pre_cont.reverse().mark(seqb).reverse();
        seqb := nil;

        # get the values of the QN/OID offsets for the reference to the
        # string values
        var attrNew_qn := mposjoin(attrNew_attrOld,
                                   attrNew_pre_cont,
                                   ws.fetch(ATTR_QN));
        var attrNew_prop := mposjoin(attrNew_attrOld,
                                     attrNew_pre_cont,
                                     ws.fetch(ATTR_PROP));
        # get container where values are stored (not where attribute is stored)
        var attrNew_cont := mposjoin(attrNew_attrOld,
                                     attrNew_pre_cont,
                                     ws.fetch(ATTR_CONT));
        attrNew_attrOld := nil;
        attrNew_pre_cont := nil;

        ws.fetch(ATTR_QN).fetch(WS).insert(attrNew_qn);
        ws.fetch(ATTR_PROP).fetch(WS).insert(attrNew_prop);
        ws.fetch(ATTR_OWN).fetch(WS).insert(attrNew_preNew);
        ws.fetch(ATTR_CONT).fetch(WS).insert(attrNew_cont);
        attrNew_qn := nil;
        attrNew_prop := nil;
        attrNew_preNew := nil;
        attrNew_cont := nil;
    }

    # 2. step: add attribute binding for new elements
    if (attr.count() != 0) { # but only if there are any top level attributes
        
        # use iter, qn and cont to find unique combinations
        var attr_qn := mposjoin(attr, acont, ws.fetch(ATTR_QN));
        var attr_cont := mposjoin(attr, acont, ws.fetch(ATTR_CONT));
        var sorting := aiter.tsort();
        sorting := sorting.CTrefine(mposjoin(attr_qn,
                                             attr_cont,
                                             ws.fetch(QN_URI_LOC)));
        var unq_attrs := sorting.tunique();
        sorting := nil;
        # test uniqueness
        if (unq_attrs.count() != aiter.count())
        {
           if (qn_item.count() > 0) {
               ERROR ("err:XQDY0025: attribute names are not unique in constructed element '%s'.",
                      qn_item.leftfetchjoin(ws.fetch(QN_LOC).fetch(WS))
                             .fetch(0));
           } else {
               ERROR ("err:XQDY0025: attribute names are not unique in constructed element.");
           }
        }
        unq_attrs := nil;

        var seqb := oid(ws.fetch(ATTR_QN).fetch(WS).count());
        attr_qn := attr_qn.seqbase(seqb);
        var attr_own := aiter.leftjoin(qn_iter.reverse())
                             .leftfetchjoin(roots)
                             .reverse().mark(seqb).reverse();
        var attr_prop := mposjoin(attr, acont, ws.fetch(ATTR_PROP));
        attr_prop := attr_prop.seqbase(seqb);
        attr_cont := attr_cont.seqbase(seqb);
        seqb := nil;

        ws.fetch(ATTR_QN).fetch(WS).insert(attr_qn);
        ws.fetch(ATTR_PROP).fetch(WS).insert(attr_prop);
        ws.fetch(ATTR_OWN).fetch(WS).insert(attr_own);
        ws.fetch(ATTR_CONT).fetch(WS).insert(attr_cont);
        attr_qn := nil;
        attr_prop := nil;
        attr_own := nil;
        attr_cont := nil;
    }

    # create result as a BAT of BATs
    var res := new (void, BAT).append (ws)
                              .append (roots)
                              .append (roots.project(WS))
                              .seqbase (0@0);
    roots := nil;

    return res;
}
ADDHELP("elem_constr", "tsheyar", "Oct 2005",
        "PARAMETERS:\n\
BAT[void, oid] qn_iter : iteration values of the qnames\n\
BAT[void, oid] qn_item : names of the elements\n\
BAT[void, oid] iter    : iteration values of the content\n\
BAT[oid, oid] pre      : pre values of the content (heads aligned with head of iter)\n\
BAT[oid, oid] pcont    : node cont values of the content (heads aligned with head of iter)\n\
BAT[void, oid] attr    : attribute ids of the content (heads aligned with head of iter)\n\
BAT[void, oid] acont   : attribute container-ids of the content (heads aligned with head of iter)\n\
BAT[void, BAT] ws      : working set that stores the document representations\n\
DESCRIPTION:\n\
elem_constr is a full featured element constructor that requires\n\
qn|iter pairs for the name part and iteration, attribute, and node\n\
information for the content. These elements are added to the working\n\
set ws. Output is a BAT of BATs with the following components:\n\
(a) the modified working set,\n\
(b) the pre values, and\n\
(c) the containder-ids of the elements",
        "pf_support");
