@' The contents of this file are subject to the MonetDB Public License
@' Version 1.1 (the "License"); you may not use this file except in
@' compliance with the License. You may obtain a copy of the License at
@' http://monetdb.cwi.nl/Legal/MonetDBLicense-1.1.html
@'
@' Software distributed under the License is distributed on an "AS IS"
@' basis, WITHOUT WARRANTY OF ANY KIND, either express or implied. See the
@' License for the specific language governing rights and limitations
@' under the License.
@'
@' The Original Code is the MonetDB Database System.
@'
@' The Initial Developer of the Original Code is CWI.
@' Portions created by CWI are Copyright (C) 1997-2007 CWI.
@' All Rights Reserved.

@f monet_multiplex
@a Peter Boncz

@* Multiplexed MIL Function Execution
This module contains the implementations of the interpretation
of the [X]() multiplex and {Y}() set-aggregate operators.

@+ Multiplex Operators
Multiplex operators provide the possibility of executing any
MIL command "CMD" (or operators or PROCs) with signature:
@
CMD(T1,..,Tn) : Tr
@
on a series of BATs with corresponding head values
by putting the command between square brackets, giving the
following corresponding signature:.
@
[CMD](bat[any::1,T1],...,bat[any::1,Tn]) : bat[any::1,Tr]
@
Implementation of the multiplex is done by performing the
natural join on all bat-valued parameters, and executing the
CMD for each combination of matching tuples. All results are
collected in a result BAT.

There are two main factors that determine how the multiplex is
executed: what kind of command is CMD (either proc, 'simple'
operator, or 'complex' operator)? and: how easy is it to
construct the natural join on the bat parameters?

In the case of simple operators *and* simple natural join
matching, significant optimizations are made. Specifically, there
are efficient implementations for 2-ary and 3-ary natural
joins that are either synced (join on keys that contain the
same sequence) or merge-able. 1-ary joins (non-joins)
default into the synced case. The optimized implementations
only hold for 'simple' operators; e.g. those [CMD] with 1,2 or 3
parameters that do not produce a result of an atom-type that
must be fixed/unfixed (like BAT). This restriction keeps the
logic in the optimized implementations simple and hence
efficient. In the synced case, some of the parameters may
actually be non-BAT, a constant (we use some trickery to divert
[+](b,1) into the fast 2-ary synced-join implementation).

If for some reason no optimizations can be made, execution is
diverted to a general-purpose multiplex implementation that in
turn depends on a general-purpose multi equi-join (BATmultijoin).
This BATmultijoin uses callback functions for registering
matching join values and matching join tuples. The matching
join value calls are used to copy tail-values into a argv[]
value array, that is used to pass parameters to CMD.  For each
join hit, the join tuple callback function is called, that
executes CMD(argv[]) and inserts the outcome into the result BAT.
@h
#ifndef _MONET_SETOP_H_
#define _MONET_SETOP_H_

@+ Exported Functions
@- {Y}() set aggregate implementation
@h
int interpret_setaggr(str s, int argc, ValPtr argv, ValPtr res, YYSTREE lt, int stk);

@- [X]() implementation
@h
int interpret_multiplex(str s, int argc, ValPtr argv, ValPtr res, YYSTREE t, Cntxt stk);

@- [X?Y:Z]() implementation functions
@h
m_export int interpret_ifthen(str *r, bit *b, str t);
m_export int interpret_ifelse(str *r, bit *b, str f);
m_export int interpret_ifthenelse(str *r, bit *b, str t, str f);

m_export int CMDifthen(int argc, ValPtr argv);
m_export int CMDifelse(int argc, ValPtr argv);
m_export int CMDifthenelse(int argc, ValPtr argv);

#endif /* _MONET_SETOP_H_ */

@- bulk error handling
When a certain predicate is executed in bulk, that is possibly millions of times, and something goes wrong, we
do not like to see millions of error messages. Therefore, both the multiplex and set aggregate implementations
use the below macros to shield off bulk error reporting. Some buffers are maintained in order to condense
all errors in a summary.
@c
#include "monetdb4_config.h"
#include "monet.h"

#define EXEC_ERROR_WHERE 80

typedef struct {
	char buf[GDKMAXERRLEN];	/* buffer where GDKerror functions log into */
	char where[EXEC_ERROR_WHERE];	/* tuples where errors happened */
	char first[GDKMAXERRLEN];	/* first error */
	char *bak;		/* backup of GDKerrbuf */
	int cnt;		/* number of errors that have occurred sofar */
} bulkerror_t;

static void
bulkerror_init(bulkerror_t *bulkerror)
{
	bulkerror->bak = GDKerrbuf;
	bulkerror->cnt = 0;
	bulkerror->buf[0] = 0;
	GDKsetbuf(bulkerror->buf);
}

static void
bulkerror_handle(bulkerror_t *bulkerror, str msg)
{
	if (bulkerror->cnt++ == 0) {
		strcpy(bulkerror->first, bulkerror->buf);
		strncpy(bulkerror->where, msg, EXEC_ERROR_WHERE - 1);
	} else if (strlen(bulkerror->where) + strlen(msg) + 2 < EXEC_ERROR_WHERE) {
		strcat(bulkerror->where, ", ");
		strcat(bulkerror->where, msg);
	}
	bulkerror->buf[0] = 0;
}

static void
bulkerror_format(bulkerror_t *bulkerror, int tpe, ptr val)
{
	char *buf = NULL;

	ATOMformat(tpe, val, &buf);
	bulkerror_handle(bulkerror, buf);
	GDKfree(buf);
}

static void
bulkerror_finish(bulkerror_t *bulkerror, str fcn_nme)
{
	GDKsetbuf(bulkerror->bak);
	if (bulkerror->cnt > 0) {
		GDKerror("%s: %d times inserted nil due to errors at tuples %s.\n", fcn_nme, bulkerror->cnt, bulkerror->where);
		GDKerror("%s: first error was:\n%s", fcn_nme, bulkerror->first);
	}
}

static int
BATunfix(ptr i)
{
	if (i && *(bat *) i)
		(void) BBPunfix(*(bat *) i);
	return 0;
}

/* call an interpret tree, and return <0 only on error */
static int
interpret_interpret(Cntxt stk, YYSTREE tt, ValPtr argv)
{
	int ret_val = interpret(stk, tt, argv);

	if (ret_val == -TOK_RETURN) {
		return 0;	/* this is what we expect normally */
	}
	VALempty(argv);		/* no return value! */
	return ret_val;
}

@:interpret_multiplex_merge(multiplex)@
@:interpret_multiplex_array(multiplex)@

/* for special [A?B:C] operators we know that the first parameter A
 * is a bat with bit-values in the tail. It is therefore no use to
 * generate varsized-tail code for these implementations.
 * The below define prevents that from happening
 */
#define FIRST_PARAM_FIXEDSIZE
@:interpret_multiplex_merge(special)@
@:interpret_multiplex_array(special)@

@c
#define REALLY_REALLY_BIG	2000000000

static int interpret_multiplex_fcn(str s, int argc, ValPtr argv, ValPtr res, int assignop, int stk, size_t size, int tpe, int ret, GDKfcn imp);

int
interpret_multiplex(str s, int argc, ValPtr argv, ValPtr res, YYSTREE t, Cntxt stk)
{
	monet_sig_t *sig = (monet_sig_t *) t->yyval.val.pval;
	int i, tpe = 0, nbats = 0;
	BAT *all_synced = NULL;
	int one_unique = FALSE;
	int all_unique = TRUE;
	int all_sorted = TRUE;
	int any_dense = FALSE;
	size_t max_size = 0;
	size_t min_size = REALLY_REALLY_BIG;
	char name[128];

	strcpy(name, s);
	if (sig->assignop) {
		BBPfix(argv[1].val.bval);
		strcat(name, "=");
	}
@- Analysis Of The BAT Parameters
Do basic check on parameters. Look at the tail types in order to derive
a function signature. Check that all head types correspond. We also want
to discover if all the input BAT parameters are sorted or in sync. This
may enable the use optimizations. Constants passed into the multiplex
operator are a special case.
@c
	for (i = 1; i < argc; i++) {
		if (argv[i].vtype == TYPE_bat && argv[i].len != int_nil) {
			BAT *b = BBPdescriptor(argv[i].val.bval);
			size_t sz = BATcount(b);

			any_dense |= BAThdense(b);
			if (min_size == REALLY_REALLY_BIG) {
				all_synced = b;
				tpe = BAThtype(b);
				if (sig->assignop && b->batRestricted) {
					GDKerror("interpret_multiplex: %s: param 1 must be writeable.\n", name);
					return -2;
				}
			}
			if (sz > max_size) {
				max_size = sz;
			}
			if (sz < min_size) {
				min_size = sz;
			}
			if (b->hkey) {
				one_unique = TRUE;
			} else {
				all_unique = FALSE;
			}
			if ((b->hsorted & 1) == 0 || b->htype == TYPE_void) {
				all_sorted = FALSE;
			}
			if (all_synced && !ALIGNsynced(all_synced, b)) {
				all_synced = NULL;
			}
			nbats++;
		} else {
			/* no merge optimization for constants */
			all_sorted = FALSE;
		}
	}
@-
Multiplex functions have multi-equijoin semantics for the matching of
head values. Double elements will cause multiplcation of the result.
Optimizations only work on 1-1 matching of oids.
@c
	if (nbats > 1) {
		if (all_synced && one_unique == 0) {
			all_synced = FALSE;
		}
		if (all_sorted && all_unique == 0) {
			all_sorted = FALSE;
		}
	}
	if (all_synced && any_dense)
		tpe = TYPE_void;
@-
Optimization: the merge multiplex operation does a merge scan. This
is very efficient in the bulk case, but if you are doing very sparse
merging (e.g. a 100 item table with a 10M table) you are better off
with the more sophisticated matching of the multijoin implementation.
That implementation has more overhead, but presumably wins that back
by doing binary search on the 10M table.
@c
	if (all_sorted && min_size * 100 < max_size) {
		all_sorted = FALSE;
	}

@- execution
@c
	if ((GDKfcn) sig->pack_fcn == sig->fast_fcn) {	/* TOK_PROC */
		return interpret_multiplex_fcn(name, argc, argv, res, sig->assignop, stk, min_size, tpe, sig->ret_tpe, (GDKfcn) t);
	} else if (sig->special) {
		if (sig->array_opt && all_synced) {
			return interpret_special_array(name, argc, argv, res, sig->fast_fcn, min_size, sig->ret_tpe, sig->assignop, sig->special);
		}
		if (sig->merge_opt && all_sorted) {
			return interpret_special_merge(name, argc, argv, res, sig->fast_fcn, min_size, sig->ret_tpe, sig->assignop, sig->special);
		}
	} else if (sig->fast_fcn != NULL) {
		if (sig->array_opt && all_synced) {
			return interpret_multiplex_array(name, argc, argv, res, sig->fast_fcn, min_size, sig->ret_tpe, sig->assignop, FALSE);
		}
		if (sig->merge_opt && all_sorted) {
			return interpret_multiplex_merge(name, argc, argv, res, sig->fast_fcn, min_size, sig->ret_tpe, sig->assignop, FALSE);
		}
	}
	{
		/* fall-through => 'durchfall' implementation */
		return interpret_multiplex_fcn(name, argc, argv, res, sig->assignop, -1, min_size, tpe, sig->ret_tpe, sig->pack_fcn);
	}
}



@- Non-Optimized Multiplex Operator Interpretation
As for the implementation, multiplex operators on strongly related BATs
with the alignment interface, are executed much faster. Also,
paralellism may be applied.

The default case - shown below - is handled using the BATmultijoin()
@c
typedef struct {
	GDKfcn fcn;		/* function to be called */
	int stk;		/* client stack */
	int argc;		/* number of arguments to call */
	ValPtr argv;		/* [MAXPARAMS] space for matching tail values */
	ValPtr c[MAXPARAMS];	/* pointer to the elements in argv */
	ColFcn f[MAXPARAMS];	/* pointer to value functions */
	int (*unfix) (ptr);	/* unfix function for produced atoms */
	int need_unfix;		/* indicates if an unfix function is needed */
	ValRecord id;		/* current head value */
	size_t cap;		/* capacity of the result */
	BAT *b;			/* result bat */
	BAT *piv[MAXPARAMS];	/* bats to be joined */
	int nbats;		/* actual number of involved bats */
	int notall;		/* ifthen or ifelse do not yield all values */
	str name;		/* operator name */
	bulkerror_t bulkerror;	/* bulk error structure */
} exec_t;


@= multiplex_copy
static void
multiplex_copy_@1(ValPtr val, @1* v)
{
	val->val.@2 = *v;
}
@c
@:multiplex_copy(chr,cval[0])@
@:multiplex_copy(bte,btval)@
@:multiplex_copy(sht,shval)@
@:multiplex_copy(int,ival)@
@:multiplex_copy(lng,lval)@

static void
multiplex_copy_var(ValPtr val, ptr v)
{
	val->val.pval = v;
	val->len = ATOMlen(val->vtype, v);
}

static void
multiplex_copy_loc(ValPtr val, ptr v)
{
	val->val.pval = v;
}

static ColFcn
multiplex_fcn(int tpe)
{
	switch (ATOMstorage(tpe)) {
	case TYPE_chr:
		return (ColFcn) multiplex_copy_chr;
	case TYPE_bte:
		return (ColFcn) multiplex_copy_bte;
	case TYPE_sht:
		return (ColFcn) multiplex_copy_sht;
#if SIZEOF_OID == SIZEOF_INT
	case TYPE_void:
#endif
	case TYPE_int:
	case TYPE_flt:
		return (ColFcn) multiplex_copy_int;
#if SIZEOF_OID == SIZEOF_LNG
	case TYPE_void:
#endif
	case TYPE_lng:
	case TYPE_dbl:
		return (ColFcn) multiplex_copy_lng;
	}
	return (ColFcn) (ATOMextern(tpe) ? multiplex_copy_var : multiplex_copy_loc);
}

#if 0
@:multiplex_exec(proc,intern,tpechk)@
@:multiplex_exec(proc,extern,tpechk)@
#endif
@:multiplex_exec(fcn,intern,intern)@
@:multiplex_exec(fcn,extern,intern)@
@:multiplex_exec(fcn,intern,extern)@
@:multiplex_exec(fcn,extern,extern)@

@:multiplex_exec(fcn,intern,tpechk)@
@:multiplex_exec(fcn,extern,tpechk)@

@:multiplex_exec_bat(proc,intern,tpechk)@
@:multiplex_exec_bat(proc,extern,tpechk)@

@:multiplex_exec_bat(fcn,intern,intern)@
@:multiplex_exec_bat(fcn,extern,intern)@
@:multiplex_exec_bat(fcn,intern,extern)@
@:multiplex_exec_bat(fcn,extern,extern)@

@:multiplex_exec_bat(fcn,intern,tpechk)@
@:multiplex_exec_bat(fcn,extern,tpechk)@

static int
interpret_multiplex_fcn(str s,		/* operator name */
			int argc,	/* #nparams */
			ValPtr argv,	/* parameter */
			ValPtr res,	/* result value (to store bat) */
			int assignop,	/* project away result head */
			int stk,	/* -1 if no proc */
			size_t size,	/* expected result size */
			int tpe,	/* head column type */
			int ret,	/* return type; <0 if unknown */
			GDKfcn imp	/* command pack address or proc YYSTREE */
	)
{
	int recursive_bats = 0;
	int r, i, proc = (stk >= 0), real_type = 0;
	int special = (imp == CMDifthen || imp == CMDifelse || imp == CMDifthenelse);
	BAT *assignbat = assignop ? BBPdescriptor(argv[1].val.bval) : NULL;
	ValPtr bak = (ValPtr) alloca(argc * (int) sizeof(ValRecord));
	exec_t desc;
	RowFcn wrap;
	oid  hseq = oid_nil;

	memset(&desc, 0, sizeof(exec_t));
	desc.name = s;
	desc.fcn = imp;
	desc.argc = argc;
	desc.argv = argv;
	desc.stk = stk;
	desc.notall = special;
	desc.cap = size;
	desc.nbats = 0;

	for (i = 1; i < argc; i++) {
		bak[i] = argv[i];
		if (argv[i].vtype == TYPE_bat && argv[i].len != int_nil) {
			BAT *b = BBPdescriptor(argv[i].val.bval);

			if (b == NULL)
				return 0;
			real_type = b->htype;
			if (BAThdense(b)) {
				if (hseq == oid_nil) {
					hseq = b->hseqbase;
				}
			}
			desc.piv[desc.nbats] = b;
			desc.c[++desc.nbats] = desc.argv + i;
		} else if (argv[i].vtype == TYPE_bat) {
			argv[i].len = 0;	/* no idea but != int_nil */
		}
	}
@-
Set up type and length info for the values that will come out of BAT params.
@c
	for (i = 0; i < desc.nbats; i++) {
		recursive_bats |= (desc.piv[i]->ttype == TYPE_bat);
		desc.f[i + 1] = multiplex_fcn(desc.piv[i]->ttype);
		desc.c[i + 1]->vtype = ATOMtype(desc.piv[i]->ttype);
		desc.c[i + 1]->len = ATOMsize(desc.c[i + 1]->vtype);
	}
	if (desc.nbats == 1 && real_type == TYPE_void) {
		desc.id.vtype = real_type;
	} else {
		desc.id.vtype = tpe;
	}
	desc.id.len = ATOMsize(tpe);

	desc.f[0] = multiplex_fcn(tpe);
	desc.c[0] = &desc.id;
	wrap = (RowFcn) (recursive_bats ?
			 (proc ?
			  (ATOMextern(desc.id.vtype) ?
			   multiplex_bat_proc_extern_tpechk :
			   multiplex_bat_proc_intern_tpechk) :
			  (ret >= 0 ?
			   (ATOMextern(ret) ?
			    (ATOMextern(desc.id.vtype) ?
			     multiplex_bat_fcn_extern_extern :
			     multiplex_bat_fcn_intern_extern) :
			    (ATOMextern(desc.id.vtype) ?
			     multiplex_bat_fcn_extern_intern :
			     multiplex_bat_fcn_intern_intern)) :
			   (ATOMextern(desc.id.vtype) ?
			    multiplex_bat_fcn_extern_tpechk :
			    multiplex_bat_fcn_intern_tpechk))) :
			 (proc ?
			  (ATOMextern(desc.id.vtype) ?
			   multiplex_bat_proc_extern_tpechk :
			   multiplex_bat_proc_intern_tpechk) :
			  (ret >= 0 ?
			   (ATOMextern(ret) ?
			    (ATOMextern(desc.id.vtype) ?
			     multiplex_fcn_extern_extern :
			     multiplex_fcn_intern_extern) :
			    (ATOMextern(desc.id.vtype) ?
			     multiplex_fcn_extern_intern :
			     multiplex_fcn_intern_intern)) :
			   (ATOMextern(desc.id. vtype) ?
			    multiplex_fcn_extern_tpechk :
			    multiplex_fcn_intern_tpechk))));
	desc.need_unfix = (!proc && !desc.notall);
	if (ret >= 0) {
		desc.b = BATnew(desc.id.vtype, ret, desc.cap + 3);
		BATseqbase(desc.b, hseq);

		desc.unfix = (desc.notall) ? NULL : BATatoms[ret].atomUnfix;
		if (desc.need_unfix && ret == TYPE_bat)
			desc.unfix = BATunfix;
	}
@-
Ok, do it! The integer return status tells us about how the matching
process was conducted.
@c
	bulkerror_init(&desc.bulkerror);
	r = BATmultijoin(desc.nbats, desc.piv, wrap, &desc, desc.f, (void **) desc.c, 0);
	bulkerror_finish(&desc.bulkerror, desc.name);
@-
New semantic of multiplexes as required by the SQL frontend:
In case there were errors, we do not accept the inserted NILs,
but rather discard the complete result and return an error.
@c
	if (desc.bulkerror.cnt > 0) {
		if (desc.b != NULL) {
			BBPreclaim(desc.b);
			desc.b = NULL;
		}
		return -1;
	}
@-
When zero matching elements where there, no execution took place.
So, what tail-type should the empty return BAT have? Dilemma!
@c
	if (desc.b == NULL && ret < 0) {
		GDKwarning("%s: empty result, hence return type unknown.\n", desc.name);
		desc.b = BATnew(desc.id.vtype, TYPE_int, 10);
		BATseqbase(desc.b, hseq);
	}
@-
Set the correct properties in the result BAT.
@c
	if (MULTIJOIN_SYNCED(r)) {
		ALIGNsetH(desc.b, desc.piv[(int) MULTIJOIN_LEAD(r)]);
	} else {
		desc.b->hsorted = MULTIJOIN_SORTED(r) ? GDK_SORTED : 0;
		if (MULTIJOIN_KEY(r))
			BATkey(desc.b, TRUE);
	}
	desc.b->tsorted = 0;
@-
Do simply an update for [+=]() kind of operations.
@c
	if (assignop) {
		BATclear(assignbat);
		BATins(assignbat, desc.b, FALSE);
		BBPreclaim(desc.b);
		res->val.ival = assignbat->batCacheid;
	} else {
		res->val.ival = desc.b->batCacheid;
	}
	for (i = 1; i < argc; i++) {
		argv[i] = bak[i];
	}
	res->vtype = TYPE_bat;
	res->len = 0;
	return 0;
}

@-
The below macros make functions that execute a proc or commands
when a matching tuple is found. They insert it into the result BAT and
take care of memory deallocation.

@= multiplex_exec
static int
multiplex_@1_@2_@3(exec_t *desc, ValPtr* argv)
{
	int retval = @:call_@1@
	(void) argv;

	if (desc->notall && retval < 0) {
		return 0;
	}
	if (!desc->b) {
		desc->b = BATnew(desc->id.vtype, desc->argv[0].vtype, desc->cap + 3);
		if (!desc->b) {
			GDKsetbuf(desc->bulkerror.bak);
			GDKerror("multiplex: bat allocation failed.\n");
			desc->bulkerror.buf[0] = 0;
			GDKsetbuf(desc->bulkerror.buf);
			return -1;
		}
		desc->unfix = desc->notall ? NULL : BATatoms[desc->argv[0].vtype].atomUnfix;
		if (desc->need_unfix && desc->argv[0].vtype == TYPE_bat)
			desc->unfix = BATunfix;
	}
	@:insert_@3(@2)@
	return 0;
bunins_failed:
	BBPreclaim(desc->b);
	desc->b = NULL;
	return -1;
}

@= multiplex_exec_bat
static int
multiplex_bat_@1_@2_@3(exec_t *desc, ValPtr* argv)
{
	int i, retval;

	/* fix all bats */
	for (i = 1; i < desc->argc; i++)
		if (desc->c[i] && desc->c[i]->vtype == TYPE_bat && argv[i]->val.bval)
			BBPfix(argv[i]->val.bval);
	retval = @:call_@1@
	/* unfix all bats */
	for (i = 1; i < desc->argc; i++)
		if (desc->c[i] && desc->c[i]->vtype == TYPE_bat && argv[i]->val.bval)
			BBPunfix(argv[i]->val.bval);

	if (desc->notall && retval < 0) {
		return 0;
	}
	if (!desc->b) {
		desc->b = BATnew(desc->id.vtype, desc->argv[0].vtype, desc->cap+3);
		if (!desc->b) {
			GDKsetbuf(desc->bulkerror.bak);
			GDKerror("multiplex: bat allocation failed.\n");
			desc->bulkerror.buf[0] = 0;
			GDKsetbuf(desc->bulkerror.buf);
			return -1;
		}
		desc->unfix = (desc->notall)?NULL:BATatoms[desc->argv[0].vtype].atomUnfix;
		if (desc->need_unfix && desc->argv[0].vtype == TYPE_bat)
			desc->unfix = BATunfix;
	}
	@:insert_@3(@2)@
	return 0;
bunins_failed:
	BBPreclaim(desc->b);
	desc->b = NULL;
	return -1;
}

@= call_proc
	interpret_interpret(desc->stk, (YYSTREE) desc->fcn, desc->argv);
@= call_fcn
	(*desc->fcn)(desc->argc,desc->argv);

@= insert_bat
	if (retval < 0) {
		VALnil(desc->argv, desc->b->ttype);
		bulkerror_format(&desc->bulkerror, desc->id.vtype, @1);
	}
        if (desc->b->htype || desc->b->ttype) {
	    bunfastins(desc->b, @1, @2);
        } else {
            BATsetcount(desc->b, desc->b->batCount+1);
        }
	if (desc->unfix)
		(desc->unfix)(@2);

@= insert_intern_intern
	@:insert_bat(&desc->id.val.ival,&desc->argv[0].val.ival)@
@= insert_extern_intern
	@:insert_bat(desc->id.val.pval,&desc->argv[0].val.ival)@
@= insert_intern_extern
	@:insert_bat(&desc->id.val.ival,desc->argv[0].val.pval)@
	GDKfree(desc->argv[0].val.pval);
@= insert_extern_extern
	@:insert_bat(desc->id.val.pval,desc->argv[0].val.pval)@
	GDKfree(desc->argv[0].val.pval);

@= insert_intern
	@:insert_@1_intern@
@= insert_extern
	@:insert_@1_extern@
@= insert_tpechk
	else if (retval >= 0 && desc->argv[0].vtype != desc->b->ttype) {
		/* fully destroy the returnvalue of the incorrect type */
		ptr _p = VALptr(desc->argv);
		if (_p) {
			ATOMunfix(desc->argv->vtype, _p);
			if (ATOMextern(desc->argv->vtype))
				GDKfree(_p);
		}
		GDKerror("%s: return type %s is different than expected (%s).\n",
			 desc->name, ATOMname(desc->argv[0].vtype), ATOMname(desc->b->ttype));
		retval = -1;
	}
	if (ATOMextern(desc->b->ttype)) {
		@:insert_@1_extern@
	} else {
		@:insert_@1_intern@
	}

@- Optimized Multiplex Operators: Synced Unique case

If the BATs fed into the multiplex are all synced and unique, we can
just shift a horizontal pointer through them and execute the function.

Speed is won by coding specific cases out. You cannot do all cases.
We selected the 1, 2 and 3-ary executions of C-implemented functions
(no procs; nor builtins) for this (only if the returntype is simple to
handle => no bats or varsized returns). BATs may also be constant values
in this implementation.

@= check_special
	if (*(ptr*) tptr == NULL) {
	    *(ptr*) tptr = tnil;
	}
	if (retval) /* only insert some tuples */

@= check_multiplex
	if (!retval) {
		char buf[32];
		sprintf(buf, PDFMT, (ptrdiff_t) (@2));
		bulkerror_handle(&bulkerror, buf);
		*(ptr*) tptr = tnil; /* insert a nil */
	}

@= BUNins_array_special
	{
		ptr _h = (buntrick==-1)?BUNhpos(bhi,ph):BUNhloc(bhi,ph);
		bunfastins_nocheck(@1, d, _h, *(ptr*)@3, hs, ts);
		d++;
		(void) dst; /* only used in "multiplex" case below */
	}

@= BUNins_array_multiplex
	(void) bhi; /* only used in "special" case above */
	(void) hs;  /* only used in "special" case above */
	(void) ts;  /* only used in "special" case above */
	switch (buntrick) {
	case 0:	  
		goto bunins_failed;
		break;
	case 1:
		*(bte*) dst = *(bte*) @3;
		break;
	case 2:
		*(sht*) dst = *(sht*) @3;
		break;
	case 4:
		*(int*) dst = *(int*) @3;
		break;
	case 8:
		*(lng*) dst = *(lng*) @3;
		break;
	default:
		memcpy(dst, @3, buntrick);
		break;
	}
	dst += buntrick;

@= arrayloop1
	while(ph < hend) {
		int retval = (*addr)(tptr,@2);
		@:check_@1(name, ph)@
		@:BUNins_array_@1(retb, ph, tptr)@
		p1 += s1; ph++;
	}

@= arrayloop2
	while(ph < hend) {
		int retval = (*addr)(tptr,@2,@3);
		@:check_@1(name, ph)@
		@:BUNins_array_@1(retb, ph, tptr)@
		p1 += s1; p2 += s2; ph++;
	}

@= arrayloop3
	while(ph < hend) {
		int retval = (*addr)(tptr,@2,@3,@4);
		@:check_@1(name, ph)@
		@:BUNins_array_@1(retb, ph, tptr)@
		p1 += s1; p2 += s2; p3 += s3; ph++;
	}

@= batinit
	b@1 = BBPdescriptor(@2);
	b@1i = bat_iterator(b@1);
	a@1 = ATOMvarsized(b@1->ttype)?b@1->theap->base:0;
	p@1 = Tloc(b@1,BUNfirst(b@1));
	s@1 = Tsize(b@1);
	if (htpe != TYPE_void)
		htpe = (special>1)?ATOMtype(b@1->htype):BAThstore(b@1);
	if (htpe == TYPE_void) {
		if (hseq == oid_nil) {
			hseq = b@1->hseqbase;
		}
	}
	ph = BUNfirst(b@1);
	hend = BUNlast(b@1);
	ah = a@1; bh = b@1; bhi = b@1i;
	hsrt |= b@1->hsorted;
	hky |= b@1->hkey;
@= arrayinit
	if (argv[@1].vtype == TYPE_bat && argv[@1].len != int_nil) {
		@:batinit(@1,argv[@1].val.bval)@
	} else {
		b@1 = 0; a@1 = 0; s@1 = 0;
		p@1 = (str) VALptr(argv+@1);
	}

@-
If we know all BAT parameters of a multiplex operation
have unique columns that are in sync, we can eliminate
matching overhead, as captured by the below macro
implementation. It is expanded to @`interpret_multiplex_array@5
and @`interpret_special_array@5.

@= interpret_multiplex_array
static int
interpret_@1_array(str name, int argc, ValPtr argv, ValPtr res, GDKfcn addr,
		   size_t size, int ret_tpe, int assignop, int special)
{
	BATiter b1i, b2i, b3i, bhi;
	str  a1, a2 = NULL, a3 = NULL, ah = NULL; /* heap bases */
	int s1 = 0, s2 = 0, s3 = 0;
	char *p1, *p2 = NULL, *p3 = NULL; /* direct pointers */
	BUN ph = 0, hend = 0, d = 0; 
	int  hky=0, hsrt=0, htpe=-1, ok = 0;
	oid  hseq = oid_nil;
	BAT  *b1, *b2, *b3=0, *retb, *bh=0;
	int ret_sze = ATOMsize(ret_tpe);
	ptr  tptr = (ptr) GDKmalloc(ret_sze+8);
	ptr  tnil = ATOMnilptr(ret_tpe);
	int replace = FALSE, hs, ts, buntrick = 0;
	bulkerror_t bulkerror;
	char *dst;

	/* keep compilers happy */
	ah = 0; (void) ah; (void)d;

	@:arrayinit(1)@
	if (argc>=3) {
		@:arrayinit(2)@
	}
	if (argc==4) {
		@:arrayinit(3)@
	}
	if (assignop && b1->batInserted == b1->batFirst) {
		replace = TRUE;
		buntrick = ATOMsize(b1->ttype);
		HASHdestroy(retb = b1);
		dst = Tloc(retb, BUNfirst(retb));
	} else {
		retb = BATnew(htpe, ret_tpe, size);
		BATseqbase(retb, hseq);
		if (htpe == TYPE_void && special == 0 && !ATOMvarsized(ret_tpe)) {
			buntrick = Tsize(retb);
			retb->H->heap.free = headsize(retb,BATcount(bh));
			retb->T->heap.free = tailsize(retb,BATcount(bh));
			BATsetcount(retb, BATcount(bh));
		} else if (special > 1 && bh->htype == TYPE_void) {
			buntrick = -1;
		}
		/* lets copy the head */
		if (special <= 1 && retb->htype) {
			buntrick = Tsize(retb);
			memcpy(retb->H->heap.base, bh->H->heap.base, headsize(bh,BATcount(bh)));
			retb->H->heap.free = headsize(retb,BATcount(bh));
			/* for var sized also copy heap */
			if (retb->hvarsized) {
				Heap hhp;

				memset(&hhp, 0, sizeof(Heap));
			    	if (bh->hheap && HEAPcopy(&hhp, bh->hheap) < 0){
					BBPreclaim(retb);
					return GDK_FAIL;
				}
				if (hhp.filename == NULL) {
					hhp.filename = retb->hheap->filename;
					retb->hheap->filename = NULL;
				}
				HEAPfree(retb->hheap);
				*retb->hheap = hhp;
			}
		}
		dst = Tloc(retb, BUNfirst(retb));
	}
	hs = Hsize(retb);
	ts = Tsize(retb);
	bulkerror_init(&bulkerror);

	if (argc == 2) {
#ifndef FIRST_PARAM_FIXEDSIZE
		if (a1) {
			@:arrayloop1(@1,a1+*(var_t*)p1)@
		} else
#endif
		{
			@:arrayloop1(@1,p1)@
		}
	} else if (argc == 3) {
#ifndef FIRST_PARAM_FIXEDSIZE
		if (a1) {
			if (a2) {
				@:arrayloop2(@1,a1+*(var_t*)p1,a2+*(var_t*)p2)@
			} else {
				@:arrayloop2(@1,a1+*(var_t*)p1,p2)@
			}
		} else
#endif
		{
			if (a2) {
				@:arrayloop2(@1,p1,a2+*(var_t*)p2)@
			} else {
				@:arrayloop2(@1,p1,p2)@
			}
		}
#ifndef FIRST_PARAM_FIXEDSIZE
	} else if (a1) {
		if (a2) {
			if (a3) {
				@:arrayloop3(@1,a1+*(var_t*)p1,a2+*(var_t*)p2,a3+*(var_t*)p3)@
			} else {
				@:arrayloop3(@1,a1+*(var_t*)p1,a2+*(var_t*)p2,p3)@
			}
		} else {
			if (a3) {
				@:arrayloop3(@1,a1+*(var_t*)p1,p2,a3+*(var_t*)p3)@
			} else {
				@:arrayloop3(@1,a1+*(var_t*)p1,p2,p3)@
			}
		}
#endif
	} else {
		if (a2) {
			if (a3) {
				@:arrayloop3(@1,p1,a2+*(var_t*)p2,a3+*(var_t*)p3)@
			} else {
				@:arrayloop3(@1,p1,a2+*(var_t*)p2,p3)@
			}
		} else {
			if (a3) {
				@:arrayloop3(@1,p1,p2,a3+*(var_t*)p3)@
			} else {
				@:arrayloop3(@1,p1,p2,p3)@
			}
		}
	}
	if (!special) 
		BATsetcount(retb,BATcount(bh));
	retb->H->heap.free = headsize(retb,BATcount(retb));
	retb->T->heap.free = tailsize(retb,BATcount(retb));
	ok = TRUE;
bunins_failed:
	bulkerror_finish(&bulkerror, name);
	GDKfree(tptr);
	if (!ok) {
		BBPreclaim(retb);
		return -1;
	}
/*
New semantic of multiplexes as required by the SQL frontend:
In case there were errors, we do not accept the inserted NILs,
but rather discard the complete result and return an error.
*/
	if (bulkerror.cnt > 0) {
		if (retb != NULL) {
			BBPreclaim(retb);
			retb = NULL;
		}
		return -1;
	}

	if (!assignop) {
		if (BATcount(retb) == BATcount(bh)) {
			ALIGNsetH(retb, bh);
		}
		BATkey(retb, hky);
		retb->hsorted = hsrt;
	} else if (replace) {
		retb->tsorted = FALSE;
		BATkey(BATmirror(retb), FALSE);
	} else {
		oid bak = b1->halign;
		BATclear(b1);
		BATins(b1, retb, FALSE);
		BBPreclaim(retb);
		retb = b1;
		b1->halign = bak;
	}
	retb->batDirty = 1;
	retb->tsorted = 0;
	res->val.bval = retb->batCacheid;
	res->vtype = TYPE_bat;
	res->len = 0;
	return 0;
}

@- Optimized Multiplex Operators: Ordered Unique case

The second case coded out are 2- and 3-ary merges of
unique sorted BATs (that are not synced). No procs or builtins
end up in this case, and the function being executed should
return a simple type.

@= BUNins_merge_special
	bunfastins(@1, @2, *(ptr*)@3);
@= BUNins_merge_multiplex
	switch (replace) {
	case 0:
		bunfastins(@1, @2, @3);
		break;
	case 1:
		*(bte*) @4 = *(bte*) @3;
		break;
	case 2:
		*(sht*) @4 = *(sht*) @3;
		break;
	case 4:
		*(int*) @4 = *(int*) @3;
		break;
	case 8:
		*(lng*) @4 = *(lng*) @3;
		break;
	default:
		memcpy(@4, @3, replace);
		break;
	}
@= mergeloop2
	while (p1 < e1 && p2 < e2) {
		oid o1 = *(oid*) BUNhloc(b1i, p1), o2 = *(oid*) BUNhloc(b2i, p2);
		if (o1 < o2) {
			p1++;
		} else if (o1 > o2) {
			p2++;
		} else {
			int retval = (*addr)(tptr,BUNt@2(b1i,p1),BUNt@3(b2i,p2));
			@:check_@1(name, p1-BUNfirst(b1))@
			@:BUNins_merge_@1(retb,BUNhloc(b1i,p1),tptr,BUNtloc(b1i,p1))@
			p1++; p2++;
		}
	}

@= mergeloop3
	if (p1 < e1 && p2 < e2 && p3 < e3) {
		oid o1 = *(oid*)BUNhloc(b1i, p1), o2 = *(oid*) BUNhloc(b2i, p2);
		for (;;) {
			if (o1 < o2) {
				p1++;
				if (p1 >= e1)
					goto xit;
			} else if (o1 > o2) {
				p2++;
				if (p2 >= e2)
					goto xit;
			} else {
				for (;;) {
					o1 = *(oid*) BUNhloc(b2i, p2);
					o2 = *(oid*) BUNhloc(b3i, p3);
					if (o1 < o2) {
						p2++;
						if (p2 >= e2)
							goto xit;
					} else if (o1 > o2) {
						p3++;
						if (p3 >= e3)
							goto xit;
					} else {
						break;
					}
				}
				o1 = *(oid*) BUNhloc(b1i, p1);
				o2 = *(oid*) BUNhloc(b2i, p2);
				if (o1 == o2) {
					int retval = (*addr)(tptr, BUNt@2(b1i,p1), BUNt@3(b2i,p2), BUNt@4(b3i,p3));
					@:check_@1(name, p1-BUNfirst(b1))@
					@:BUNins_merge_@1(retb,BUNhloc(b1i,p1),tptr,BUNtloc(b1i,p1))@
					p1++;
					if (p1 >= e1)
						goto xit;
					p2++;
					if (p2 >= e2)
						goto xit;
					p3++;
					if (p3 >= e3)
						goto xit;
				} else
					continue;
			}
			o1 = *(oid*) BUNhloc(b1i, p1);
			o2 = *(oid*) BUNhloc(b2i, p2);
		}
	}

@= mergeinit
	{
		BAT *bn = BBPdescriptor(argv[@1].val.bval);
		if (htpe == TYPE_void)
			htpe = (special>1)?ATOMtype(bn->htype):bn->htype;
		b@1 = bn;
		p@1 = BUNfirst(bn);
		e@1 = BUNlast(bn);
		b@1i = bat_iterator(bn);
	}

@= mergealign
	if ((argc > @1) && (BATcount(retb) == BATcount(b@1)))
		ALIGNsetH(retb, b@1);

@-
If we know all BAT parameters of a multipelx operation
have unique columns and are sorted, we can eliminate
matching overhead, as captured by the below macro
implementation. It is expanded to @`interpret_special_merge@5
and @`interpret_multiplex_merge@5.

@= interpret_multiplex_merge
static int
interpret_@1_merge(str name, int argc, ValPtr argv, ValPtr res, GDKfcn addr,
		   size_t size, int ret_tpe, int assignop, int special)
{
	BATiter b1i, b2i, b3i;
	BAT *b1, *b2, *b3 = NULL; /* BATs */
	BUN  p1, p2, p3 = BUN_NONE;   /* bun pointers */
	BUN  e1, e2, e3 = BUN_NONE;   /* bun pointers */
	int ret_sze = ATOMsize(ret_tpe), ok = 0;
	ptr  tptr = GDKmalloc(MAX(1,ret_sze));
	ptr  tnil = ATOMnilptr(ret_tpe);
	int replace=0, htpe = TYPE_void;
	bulkerror_t bulkerror;
	BAT* retb;

	@:mergeinit(1)@
	@:mergeinit(2)@
	if (argc==4)
		@:mergeinit(3)@

	if (assignop && b1->batInserted == b1->batFirst && special == 0) {
		replace = ATOMsize(b1->ttype);
		HASHdestroy(retb = b1);
	} else {
		retb = BATnew(htpe, ret_tpe, size);
		BATseqbase(retb, b1->hseqbase);
	}
	bulkerror_init(&bulkerror);

	if (argc < 4) {
#ifndef FIRST_PARAM_FIXEDSIZE
		if (b1->tvarsized) {
			if (b2->tvarsized) {
				@:mergeloop2(@1,var,var)@
			} else {
				@:mergeloop2(@1,var,loc)@
			}
		} else {
#endif
			if (b2->tvarsized) {
				@:mergeloop2(@1,loc,var)@
			} else {
				@:mergeloop2(@1,loc,loc)@
			}
#ifndef FIRST_PARAM_FIXEDSIZE
		}
	} else if (b1->tvarsized) {
		if (b2->tvarsized) {
			if (b3->tvarsized) {
				@:mergeloop3(@1,var,var,var)@
			} else {
				@:mergeloop3(@1,var,var,loc)@
			}
		} else {
			if (b3->tvarsized) {
				@:mergeloop3(@1,var,var,loc)@
			} else {
				@:mergeloop3(@1,var,loc,loc)@
			}
		}
#endif
	} else {
		if (b2->tvarsized) {
			if (b3->tvarsized) {
				@:mergeloop3(@1,loc,var,var)@
			} else {
				@:mergeloop3(@1,loc,var,loc)@
			}
		} else {
			if (b3->tvarsized) {
				@:mergeloop3(@1,loc,var,loc)@
			} else {
				@:mergeloop3(@1,loc,loc,loc)@
			}
		}
	}
	@:mergealign(1)@
	@:mergealign(2)@
	@:mergealign(3)@
xit:
	ok = TRUE;
bunins_failed:
	bulkerror_finish(&bulkerror, name);
	GDKfree(tptr);
	if (!ok) {
		BBPreclaim(retb);
		return -1;
	}
/*
New semantic of multiplexes as required by the SQL frontend:
In case there were errors, we do not accept the inserted NILs,
but rather discard the complete result and return an error.
*/
	if (bulkerror.cnt > 0) {
		if (retb != NULL) {
			BBPreclaim(retb);
			retb = NULL;
		}
		return -1;
	}

	if (!assignop) {
		if (BATcount(retb) == BATcount(b1)) {
			ALIGNsetH(retb, b1);
		}
		if (BATcount(retb) == BATcount(b2)) {
			ALIGNsetH(retb, b2);
		}
		if (argc == 4 && BATcount(retb) == BATcount(b3)) {
			ALIGNsetH(retb, b3);
		}
		BATkey(retb, 1);
		retb->hsorted = GDK_SORTED;
	} else if (replace) {
		retb->tsorted = FALSE;
		BATkey(BATmirror(retb), FALSE);
	} else {
		if (BATcount(retb) < BATcount(b1)) {
			BATreplace(b1, retb, 0);
		} else {
			oid bak = b1->halign;
			BATclear(b1);
			BATins(b1, retb, FALSE);
			b1->halign = bak;
		}
		BBPreclaim(retb);
		retb = b1;
	}
	retb->batDirty = 1;
	retb->tsorted = 0;
	res->val.bval = retb->batCacheid;
	res->vtype = TYPE_bat;
	res->len = 0;
	return 0;
}

@- Optimized Special Multiplex Operators
@T
The following special multiplex operators are supported:
\begin{itemize}
\item {\tt ifthen(bit, val) -> (bit==true)?val:none}
\item {\tt ifelse(bit, val) -> (bit==false)?val:none}
\item {\tt ifthenelse(bit, val1,val2) -> (bit==true)?val1:(bit==false)?val2:none}
\end{itemize}
The special thing about them is that they can return a
subset of the oids in the BATs -- the [ifthen] operation for
instance only produces the tuples for the oids which have a true
value in the first vector. These optimized implementations
also work with variable-sized atoms, and avoid all atom copying,
even in the fixed size case.
@c
int
interpret_ifelse(str *retv, bit *b, str p1)
{
	if (*b != FALSE) {
		return GDK_FAIL;
	}
	*retv = p1;
	return GDK_SUCCEED;
}

int
interpret_ifthen(str *retv, bit *b, str p1)
{
	if (*b != TRUE) {
		return GDK_FAIL;
	}
	*retv = p1;
	return GDK_SUCCEED;
}

int
interpret_ifthenelse(str *retv, bit *b, str p1, str p2)
{
	if (*b == bit_nil) {
		*retv = NULL;
	} else {
		*retv = *b ? p1 : p2;
	}
	return GDK_SUCCEED;
}

@- Non-optimized Special Multiplex Operators

These are used as a backup implementation if optimization using
some bulk technique is not possible.

@c
int
CMDifelse(int argc, ValPtr argv)
{
	int ret = argv[1].val.cval[0];

	(void) argc;

	if (ret == FALSE) {
		VALcopy(argv, argv + 2);
	} else {
		return -1;
	}
	return 0;
}

int
CMDifthen(int argc, ValPtr argv)
{
	int ret = argv[1].val.cval[0];

	(void) argc;

	if (ret == TRUE) {
		VALcopy(argv, argv + 2);
	} else {
		return -1;
	}
	return 0;
}

int
CMDifthenelse(int argc, ValPtr argv)
{
	int ret = argv[1].val.cval[0];

	(void) argc;

	if (ret == bit_nil) {
		VALnil(argv, argv[2].vtype);
	} else if (ret) {
		VALcopy(argv, argv + 2);
	} else {
		VALcopy(argv, argv + 3);
	}
	return 0;
}

@* Set Aggregate Implementation
@T
Set aggregates are the MIL language construct
	{\tt \{Y\}(bat-expr)}
that allow you to to interpret a
	{\tt bat[ht,tt]}
as 	$S$
a set of collections of {\tt tt} values, defined as
	$S = \{ set_{h} | [h,t] \in bat\}$
in which
	$set_{h} = \{ t | [h,t] \in bat\}$
are bags/sets of tail values.
\\
The {\tt Y()} is a {\bf read-only} unary BAT command
	{\tt Y(bat[tt,any]) : rt}
or
	{\tt Y(bat[any,tt]) : rt}
that ignores the contents of the {\em any} column.
\\
The {\bf set-aggregate} version of {\tt Y}, denoted in MIL\\
	{\center\tt \{Y\}(bat[ht,tt]) : bat[ht,rt]} \\
is defined as\\
	{\center$\{ [h,r] | [h,t] \in bat \wedge r = \mbox{\tt Y}(set_{h})\}$ }

The {\tt \{Y\}()} language constructor is orthogonal, and works with all
commands, builtins, procs, derefenced address variables, etc.
@
The set-aggregate implementation will not be very efficient in
its generic form, because it implies translating one command into
N calls to some operator that needs a unary BAT. When N is very large,
and the assembled BATs are small, the assembling and call-overhead
will be substantial.
@T
Power users are therefore requested to overload their much-used set-
aggregates Y by {\tt \{Y\}()} user-defined commands that have the same
semantics.

@- nested function execution
Note that the varsized result produced by the execution must be freed.
Note also that result atoms with an unfix ADT routine must be unfixed (odSet, e.g.).

@= exec_cleanup
	if (nested->H->hash || nested->T->hash) {
		HASHdestroy(nested);
	}
	nested->halign = nested->talign = 0;
	nested_rev->hkey = nested_rev->tkey = nested->hkey = nested->tkey = prop_key;
	nested->hsorted = prop_sorted;
@= exec_interpret
{
	ptr _p;

	if (packed_call) {
		ret_val = (*packed_call)(2, argv);
	} else {
		ret_val = interpret_interpret(stk, tt, argv);
	}
	if (ret_val >= 0 && argv->vtype != sig->ret_tpe) {
		/* fully destroy the return value of the illegal type */
		_p = VALptr(argv);
		if (_p) {
			ATOMunfix(argv->vtype, _p);
			if (ATOMextern(argv->vtype)) GDKfree(_p);
		}
		GDKerror("{%s}: return type %s is different than expected (%s).\n",
			 ATOMname(argv->vtype), ATOMname(sig->ret_tpe));
		ret_val = -1;
	}
	if (ret_val < 0) {
		VALnil(argv, sig->ret_tpe);
		bulkerror_format(&bulkerror, ret->htype, @1);
	}
	if (argv->vtype == TYPE_bat) {
		/* terrible+true; we must check for views (on nested) */
		BAT *_b = BBP_cache(argv->val.bval);

		if (isVIEW(_b))
			VIEWreset(_b);
		if (BUNfastins(ret, @1, _p = VALptr(argv)) == 0)
			ret_val = -1;
		BBPdecref(*(bat*) _p, !packed_call); /* TOK_COMMAND=hard unfix, proc=logical */
	} else {
		if (BUNfastins(ret, @1, _p = VALptr(argv)) == 0)
			ret_val = -1;
		ATOMunfix(argv->vtype, _p);
		if (ATOMextern(argv->vtype)) {
			GDKfree(_p);
		}
	}
	@:exec_cleanup@
}
@= exec_opt
	if (!(*direct_call)(buf, nested)) {
		memcpy(buf, ATOMnilptr(ret->ttype), ATOMsize(ret->ttype));
		bulkerror_format(&bulkerror, ret->htype, @1);
	}
	bunfastins(ret, @1, buf);
	@:exec_cleanup@
@c
int
interpret_setaggr(str name, int argc, ValPtr argv, ValPtr res, YYSTREE tt, int stk)
{
	monet_sig_t *sig = (monet_sig_t *) tt->yyval.val.pval;
	BAT *b, *nested, *nested_rev, *histo = 0, *extent = 0, *ret = 0;
	size_t minpos;
	int ret_val = -1, varsize, head_type, hidden_vartype = 0;
	int prop_key, prop_sorted = 0, restore = 1;
	GDKfcn direct_call = NULL, packed_call = NULL;
	ValRecord argv_bak;
	BUN p, q, r, s;
	size_t zz;
	hash_t hh;
	str buf = NULL;
	bulkerror_t bulkerror;
	BATstore nested_bak;
	BAT nested_rev_bak;
	BATiter extenti, bi;

	(void) name;

@- argument check
@T
The function can have two parameters. The first is obligatory, and contains
the {\tt bat[ht,tt]}. The second is the extent {\tt bat[ht,any]} and may
be omitted; in which case the operators are executed on all unique
head values only (no empty sets possible).
@c
	if (!(b = BBPdescriptor(argv[1].val.bval))) {
		return -1;
	}
	if ((argc == 3) && !(extent = BBPdescriptor(argv[2].val.bval))) {
		return -1;
	}
	minpos = p = BUNfirst(b);

	/* try to establish whether we can use the non-packed direct implementation */
	if ((GDKfcn) sig->pack_fcn != sig->fast_fcn) {	/* {Y} with Y is not TOK_PROC */
		if (sig->fast_fcn &&
		    /* fast path call available */
		    ATOMstorage(b->htype) <= TYPE_flt && ATOMsize(b->htype) <= (int) sizeof(int) &&
		    /* small int head matching */
		    BATatoms[sig->ret_tpe].atomUnfix == NULL &&
		    /* no unfix work */
		    !ATOMvarsized(sig->ret_tpe))
			/* no garbage collection needed */
		{
			direct_call = sig->fast_fcn;	/* ok, do the optimized calling */
		} else {
			packed_call = sig->pack_fcn;	/* nope, use standard method */
		}
	}

@- tricks with the nested bat
Make an immutable temporary BAT called @%nested@.  Here we will assemble
the current subset. We play a trick with its column specifiers: both
point to the same column. This makes the {Y}() work for Y that work
both on head or tail column.

The 'hidden_vartype' variable is used for hiding any varsized
types.  This saves additional cost, as we just manipulate the integer heap
offsets.  When we insert stuff, they are treated as integers. When the nested
BAT is passed to the Y function, the theap of nested points to 'b', and voila,
there are the {strings, polygons, images, etc}!
@c
	if (!extent) {
		BAT *v = VIEWhead(b);	/* just copy the heads */

		extent = histo = (BAT *) BATukunique(v);
		BBPreclaim(v);
	}
	if ((BAThordered(b) & 1) == 0 && ATOMvarsized(b->ttype)) {
		hidden_vartype = b->ttype;	/* just copy offsets */
	}
	nested = BATnew(hidden_vartype ? TYPE_var : (b->ttype?b->ttype:TYPE_oid), TYPE_void, (BATcount(b) * 3) / (BATcount(extent) + 1));
	nested_rev = BATmirror(nested);
	nested->batRestricted = BAT_READ;
	prop_key = (b->batSet || b->tkey);
 	nested_bak = *(BATstore*) nested;
 	nested_rev_bak = *(BAT*) nested_rev;

	/* TRICK: on varsized types we reuse the heaps from 'b' in 'nested' */
	if (hidden_vartype) {
		nested->htype = hidden_vartype;
		nested->hvarsized = TRUE;
		nested->hheap = (Heap*) alloca(sizeof(Heap));
		*nested->hheap = *b->theap;
	}

	/* TRICK: for sorted aggregation, we let 'nested' point into 'b' */
	if (BAThordered(b) & 1) {
		nested->hsorted = nested->tsorted = BATtordered(b);
		*nested->H = *b->T;
		*nested->T = *b->T;
		nested->H->props = nested->T->props = NULL;
		nested->halign = 0;
		nested->H->hash = nested->T->hash = NULL;
		prop_sorted = BATtordered(b) & 1;
	} else {
		nested->hsorted = nested->tsorted = 0;
	}

	/* TRICK: let the tail column of 'nested' point to the head column */
	nested->T = nested->H;

@- non-tricky? initializations
@c
	/* we will repreatedly invoke X(argv[1]), where X from {X}(b) */
	argv_bak = argv[1];
	argv[1].vtype = TYPE_bat;
	argv[1].val.bval = nested->batCacheid;
	argv[1].len = 0;

	/* create the result BAT */
	ret = BATnew(BAThtype(b), sig->ret_tpe, BATcount(extent));

	/* find out things about the head type and the result type */
	head_type = BAThvoid(b) ? TYPE_void : ATOMstorage(ATOMtype(b->htype));
	varsize = ATOMvarsized(sig->ret_tpe);
	if (!varsize)
		buf = (char *) GDKmalloc(ATOMsize(sig->ret_tpe));

	bulkerror_init(&bulkerror);

@+ sorted aggregation
The idea is to avoid copying by mapping chunks of 'b' into 'nested'.

the below is a merge-semijoin of 'extent' and 'b'. For each element in 'extent'
let 'nested' point (by manipulating its BUNheap pointers) to the matching
range in 'b' (possibly empty). We then call the execution function on 'nested'.
@c
	bi = bat_iterator(b);
	extenti = bat_iterator(extent);
	if (BAThordered(b) & 1) {
		BAT *m = BATmirror(b);
		int (*cmp) (ptr, ptr) = BATatoms[b->htype].atomCmp;
		oid minoid = b->hseqbase;
		oid maxoid = minoid + BATcount(b);
		oid sqb = 0;
		BUN pp = BUN_NONE;

		if (!(BAThordered(extent) & 1)) {
			BAT *n = BATsort(extent);

			if (histo)
				BBPreclaim(histo);
			extent = histo = n;
		}
		p = BUNfirst(b);
		r = BUNfirst(extent);
		q = BUNlast(b);
		s = BUNlast(extent);

		if (direct_call == NULL) {
@-
here we do not have an efficient execution function, so other optimizations
are quite useless. Maybe we shouldn't even distinuish this case and do
it with the generic hashimpl anyway.
@c
			if (b->ttype == TYPE_void) {
				pp = p;
				sqb = nested->hseqbase;
				@:non_opt_sort((void),nested->hseqbase=nested->tseqbase=sqb+p)@
			} else {
				@:non_opt_sort(,)@
			}
@= non_opt_sort
		if (GDKdebug&131072)
			THRprintf(GDKout, "setaggr impl: non-optimized sort @1\n");
		for (; r < s; r++) {	   /* loop on extent */
			ptr last = BUNhead(extenti,r);
			if (p < q && (*cmp)(BUNhead(bi, p), last) < 0) {
				 p = SORTfndfirst(m, last);
			}
			nested->batDeleted = nested->batInserted = nested->batFirst = 0;
			nested->H->heap.base = nested->T->heap.base = Hloc(m,p); 
			@2;
			pp = p;
			p = SORTfndlast(m, last);
			nested->H->heap.size = nested->H->heap.free = headsize(nested,p-pp);
			nested->T->heap.size = nested->T->heap.free = tailsize(nested,p-pp);
			BATsetcount(nested, p-pp);
			@:exec_interpret(last)@
			nested->hdense = 0;
		}
@-
optimized cases that use exec_opt and hence optimize_call
@c
		} else {
			switch (head_type) {
			case TYPE_int:
				@:getset_int(int)@
				break;
			case TYPE_sht:
				@:getset_int(sht)@
				break;
			case TYPE_bte:
			case TYPE_chr:
				@:getset_int(bte)@
				break;
			default:
				/* (head_type == TYPE_void) */
				if (GDKdebug & 131072)
					THRprintf(GDKout, "setaggr impl: void-optimized sort\n");
				sqb = nested->hseqbase;
				for (; r < s; r++) {
					oid last = *(oid *) BUNhead(extenti, r);

					if (last >= minoid && last < maxoid) {
						zz = minpos + (last - minoid);
						q = zz;
						p = q+1;
						nested->hseqbase = nested->tseqbase = sqb + zz;
					} else {
						q = p;
					}
					nested->batDeleted = nested->batInserted = nested->batFirst = 0;
					nested->T->heap.base = Tloc(b,q); 
					nested->T->heap.size = nested->T->heap.free = tailsize(nested,p-q);
					BATsetcount(nested, p-q);
					@:exec_opt(&last)@
				}
				break;
			}
		}
@= getset_int
	if (b->ttype == TYPE_void) {
		pp = p;
		sqb = nested->hseqbase;
		@:getset_int_(@1,-void,nested->hseqbase = nested->tseqbase = sqb + p)@
	} else {
		@:getset_int_(@1,,)@
	}
@= getset_int_
	if (GDKdebug&131072)
		THRprintf(GDKout, "setaggr impl: @1@2-optimized sort\n");
	for (; r < s; r++) {
		@1 last = *(@1*) BUNhead(extenti,r);
		if (*(@1*) BUNhloc(bi,p) < last) {
			p = SORTfndfirst_@1(m, &last);
		}
		nested->batDeleted = nested->batInserted = nested->batFirst = 0;
		nested->H->heap.base = nested->T->heap.base = Hloc(m,p);
		@3;
		pp = p;
		p = SORTfndlast_@1(m, &last);
		nested->H->heap.size = nested->H->heap.free = headsize(nested,p-pp);
		nested->T->heap.size = nested->T->heap.free = tailsize(nested,p-pp);
		BATsetcount(nested, p-pp);
		@:exec_opt(&last)@
		nested->hdense = 0;
	}

@+ hashed aggregation
Here we use a hash-table on the head of 'b' to discover all occuring
head values that make up each set. For each set, we materialize
a temporary BAT 'nested'; in which we insert all tail values.
@c
	} else {
		oid voiddelta = b->tseqbase - minpos;
		int hs = Hsize(nested);
		Heap *head = &nested->H->heap;

		nested->batDeleted = nested->batInserted = 0;

		restore = 0;
		if (direct_call == NULL) {
			if (GDKdebug & 131072)
				THRprintf(GDKout, "setaggr impl: non-optimized hash\n");
			BATloop(extent, p, q) {

				ptr last = BUNhead(extenti, p);

				/* make nested empty and writeable */
				nested->batDeleted = nested->batFirst = nested->batInserted = 0;
				BATsetcount(nested, 0);
				head->free = 0;

				if (BATprepareHash(b))
					goto bunins_failed;
				HASHloop(bi, b->H->hash, hh, last) {
					@:bunins_unary(hh)@
				}
				@:exec_interpret(last)@
				nested->hsorted = 0;
			}
		} else {

@- optimized hashed or binary index implementation
we know that the head is oid (or a small version) here..
@c
			if (BATcount(extent) > 6) {
				if (BATprepareHash(b))
					goto bunins_failed;
				@:getset_ins(hash)@
			} else {
				@:getset_ins(loop)@
			}
		}
	}

@= getset_ins
	switch (head_type) {
	case TYPE_chr:
	case TYPE_bte:
		@:getset_ins_imp(bte,@1)@
		break;
	case TYPE_sht:
		@:getset_ins_imp(sht,@1)@
		break;
	case TYPE_int:
		@:getset_ins_imp(int,@1)@
		break;
	}
	nested->hsorted = 0;
@= getset_ins_imp
	if (GDKdebug&131072)
		THRprintf(GDKout, "setaggr impl: @1-optimized @2\n");
	BATloop(extent, p, q) {
		ptr last = BUNhead(extenti,p);
		@1 cur = *(@1*) last;

		/* make 'nested' empty and writeable */
		nested->batFirst = 0;
		BATsetcount(nested, 0);
		head->free = 0;

		/* get all elements for this id and insert them in 'nested' */
		@:getset_ins_@2(@1)@

		/* execute the function, and insert the result */
		@:exec_opt(last)@
	}
@= getset_ins_hash
	HASHloop_@1(bi, b->H->hash, hh, (&cur)) {
		@:bunins_unary(hh)@
	}
@= getset_ins_loop
	BATloop(b, r, s) {
		if (cur == *(@1*) BUNhloc(bi,r)) {
			@:bunins_unary(r)@
		} 
	}

@+ epilogue
set result bat properties
@c
	ret_val = 0;
	ret->tsorted = 0;

	if (histo) {
		if (b->halign == 0) {
			b->halign = OIDnew(1);
		}
		ret->hsorted = BAThordered(b);
		ret->halign = NOID_AGGR(b->halign);
		if (BATcount(ret) == BATcount(b)) {
			ALIGNsetH(ret, b);
		}
	} else {
		ALIGNsetH(ret, extent);
	}

	BATkey(ret, TRUE);
@-
set return value
@c
	res->vtype = TYPE_bat;
	res->len = 0;
	res->val.bval = ret->batCacheid;
@-
clean up and exit
@c
   bunins_failed:
	bulkerror_finish(&bulkerror, "interpret_setaggr");
@-
New semantic of multiplexes as required by the SQL frontend:
In case there were errors, we do not accept the inserted NILs,
but rather discard the complete result and return an error.
@c
	if (bulkerror.cnt > 0 && ret_val >= 0) {
		ret_val = -1;
	}
	if (histo)
		BBPreclaim(histo);
	if (!varsize)
		GDKfree(buf);
	argv[1] = argv_bak;

	/* undo all our hacks in nested before freeing it */
	if (restore) {
		*(BATstore*) nested = nested_bak;
		*(BAT*) nested_rev = nested_rev_bak;
	} else {
		BATstore *n = (BATstore*)nested;
		nested->H = &n->H;
		nested->T = &n->T;
		nested_rev->H = &n->T;
		nested_rev->T = &n->H;
		nested->H->type = TYPE_void;
		nested->H->key = FALSE;
		nested->H->varsized = TRUE;
		nested->H->shift = 0;
		nested->H->width = 0;
		nested->H->vheap = NULL;
	}
	BBPreclaim(nested);

	if (ret_val < 0) {
		BBPreclaim(ret);
	}
	return ret_val;
}

@- quick bunins
As we do all kinds of nasty things with 'nested' (like remapping its atom
heaps to 'b') we don't want the normal @#BUNins()@ function to see it. Besides,
coding out the @#BUNins()@ gives extra performance, as we save a function
call and we have the additional knowledge (through the atom heap trick) that:
- 'nested' is unary and
- its atomic content is fixed size

@= bunins_unary
	if (head->free >= head->size) {
		int err;

		err = HEAPextend(head, head->size*2);
		if (err < 0)
			goto bunins_failed;
		nested->batDeleted = nested->batDeleted = nested->batFirst = 0;
	}
	if (hs == SIZEOF_OID) {
		if (b->ttype == TYPE_void) {
			*(oid*) (head->base + head->free) = voiddelta + @1;
		} else {
#if SIZEOF_OID == SIZEOF_INT
		        *(int*) (head->base + head->free) = *(int*) BUNtloc(bi,@1);
#else
		        *(lng*) (head->base + head->free) = *(lng*) BUNtloc(bi,@1);
#endif
		}
#if SIZEOF_OID != SIZEOF_INT
	} else if (hs == SIZEOF_INT) {
	        *(int*) (head->base + head->free) = *(int*) BUNtloc(bi,@1);
#endif
	} else if (hs > SIZEOF_INT) {
		memcpy(head->base + head->free, BUNtloc(bi,@1), hs);
	} else if (hs == 1) {
		*(bte*) (head->base + head->free) = *(bte*) BUNtloc(bi,@1);
	} else {
		*(sht*) (head->base + head->free) = *(sht*) BUNtloc(bi,@1);
	}
	head->free += hs;
	nested->batCount++;
@}
